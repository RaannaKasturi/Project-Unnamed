{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# from generate_mindmap import generate_mindmap_svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 35 key-value pairs and 147 tensors from Llama-3.2-1B-Instruct-Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 1B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  18:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-1B-Instruct-GGU...\n",
      "llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 112\n",
      "llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125\n",
      "llama_model_loader: - type  f32:   34 tensors\n",
      "llama_model_loader: - type q8_0:  113 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 16\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 512\n",
      "llm_load_print_meta: n_embd_v_gqa     = 512\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 1.24 B\n",
      "llm_load_print_meta: model size       = 1.22 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3.2 1B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors: offloading 16 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 17/17 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   266.16 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  1252.42 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_ctx      = 100000\n",
      "llama_new_context_with_model: n_batch    = 4096\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  3125.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3125.00 MiB, K (f16): 1562.50 MiB, V (f16): 1562.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =  6461.32 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   199.32 MiB\n",
      "llama_new_context_with_model: graph nodes  = 518\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'llama.embedding_length': '2048', 'llama.feed_forward_length': '8192', 'general.license': 'llama3.2', 'llama.attention.value_length': '64', 'general.size_label': '1B', 'general.type': 'model', 'quantize.imatrix.chunks_count': '125', 'llama.context_length': '131072', 'general.name': 'Llama 3.2 1B Instruct', 'tokenizer.ggml.bos_token_id': '128000', 'general.basename': 'Llama-3.2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.block_count': '16', 'llama.attention.head_count': '32', 'llama.attention.key_length': '64', 'general.finetune': 'Instruct', 'general.file_type': '7', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.vocab_size': '128256', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '128009', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\\n    {%- else %}\\n        {%- set date_string = \"26 Jul 2024\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n        {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n        {{- \\'\"parameters\": \\' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \"}\" }}\\n        {{- \"<|eot_id|>\" }}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'llama.rope.dimension_count': '64', 'quantize.imatrix.file': '/models_out/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.imatrix', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'llama.attention.head_count_kv': '8', 'quantize.imatrix.entries_count': '112'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path=\"Llama-3.2-1B-Instruct-Q8_0.gguf\",\n",
    "    n_gpu_layers = -1,\n",
    "    n_ctx=100000,\n",
    "    n_batch=4096,\n",
    "    # main_gpu=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"cr1c00107.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "final_text = \"\"\n",
    "for text in texts:\n",
    "    if text.page_content.startswith(\"REFERENCES\"):\n",
    "        break\n",
    "    else:\n",
    "        final_text = final_text + text.page_content\n",
    "research_paper = \"\"\n",
    "for text in final_text:\n",
    "    if text.startswith((\"REFERENCES\", \"REFERENCESREFERENCES\", \"REFERENCESREFERENCESREFERENCES\")):\n",
    "        break\n",
    "    else:\n",
    "        research_paper = research_paper + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Paper:  Combining Machine Learning and Computational Chemistry for\n",
      "Predictive Insights Into Chemical Systems\n",
      "John A. Keith,* Valentin Vassilev-Galindo, Bingqing Cheng, Stefan Chmiela, Michael Gastegger,\n",
      "Klaus-Robert Müller,* and Alexandre Tkatchenko*Klaus-Robert Müller,* and Alexandre Tkatchenko*\n",
      "Cite This: Chem. Rev. 2021, 121, 9816−9872 Read Online\n",
      "ACCESS Metrics & More Article Recommendations\n",
      "ABSTRACT: Machine learning models are poised to make a transformative impact onchemical sciences by dramatically accelerating computational algorithms and amplifying\n",
      "insights available from computational chemistry methods. However, achieving this requires aconﬂuence and coaction of expertise in computer science and physical sciences. This Review\n",
      "is written for new and experienced researchers working at the intersection of bothﬁelds. Weﬁrst provide concise tutorials of computational chemistry and machine learning methods,\n",
      "showing how insights involving both can be achieved. We follow with a critical review ofnoteworthy applications that demonstrate how computational chemistry and machine\n",
      "learning can be used together to provide insightful (and useful) predictions in molecular and\n",
      "materials modeling, retrosyntheses, catalysis, and drug design.\n",
      "CONTENTSCONTENTS\n",
      "1. Introduction 9817\n",
      "1.1. Background 9817\n",
      "1.2. Motivation for This Review 9817\n",
      "2. CompChem and Notable Intersections with ML 9820\n",
      "2.1. Computational Modeling, Data, and Infor-\n",
      "mation Across Many Scales 9820mation Across Many Scales 9820\n",
      "2.1.1. Models and Levels of Abstraction 9820\n",
      "2.1.2. CompChem Representations 9820\n",
      "2.1.3. Method Accuracy 9821\n",
      "2.1.4. Precision and Reproducibility 9821\n",
      "2.2. Hierarchies of Methods 98212.2. Hierarchies of Methods 9821\n",
      "2.2.1. Wavefunction Theory Methods 9823\n",
      "2.2.2. Correlated Wavefunction Methods 9824\n",
      "2.2.3. Density Functional Theory 9825\n",
      "2.2.4. Semiempirical Methods 9826\n",
      "2.2.5. Nuclear Quantum Eﬀects 98272.2.5. Nuclear Quantum Eﬀects 9827\n",
      "2.2.6. Interatomic Potentials 9827\n",
      "2.3. Response Properties 9829\n",
      "2.4. Solvation Models 9829\n",
      "2.5. Insightful Predictions for Molecular and\n",
      "Material Properties 9830\n",
      "3. Machine Learning Tutorial and Intersections3. Machine Learning Tutorial and Intersections\n",
      "with Chemistry 9830\n",
      "3.1. What is ML? 9831\n",
      "3.1.1. What Does ML Do Well? 9832\n",
      "3.1.2. What Does ML Do Poorly? 9832\n",
      "3.2. Types of Learning 9833\n",
      "3.2.1. Supervised Learning 98333.2.1. Supervised Learning 9833\n",
      "3.2.2. Unsupervised Learning 9833\n",
      "3.2.3. Reinforcement Learning 9833\n",
      "3.3. Universal Approximators 9834\n",
      "3.4. ML Workﬂow 9834\n",
      "3.4.1. Data Sets 9834\n",
      "3.4.2. Descriptors 9835\n",
      "3.4.3. Training 98353.4.2. Descriptors 9835\n",
      "3.4.3. Training 9835\n",
      "4. Applications of Machine Learning to Chemical\n",
      "Systems 9836\n",
      "4.1. Representing Chemical Systems 9837\n",
      "4.1.1. Descriptors 9837\n",
      "4.1.2. Representing Local Environments 9837\n",
      "4.1.3. Locality Approximation 98394.1.3. Locality Approximation 9839\n",
      "4.1.4. Advantages of Built-In Symmetries 9839\n",
      "4.1.5. End-to-End NN Representations 9840\n",
      "4.2. From Descriptors to Predictions 9840\n",
      "4.3. CompChem Data 9842\n",
      "4.3.1. Benchmark Data Sets 98424.3.1. Benchmark Data Sets 9842\n",
      "4.3.2. Visualization of Data Sets 9842\n",
      "4.3.3. Text and Data Mining for Chemistry 9843\n",
      "4.4. Transforming Atomistic Modeling 9844\n",
      "4.4.1. Predicting Thermodynamic Properties 9844\n",
      "4.4.2. Nuclear Quantum Eﬀects 98444.4.2. Nuclear Quantum Eﬀects 9844\n",
      "Special Issue: Machine Learning at the Atomic Scale\n",
      "Received: February 4, 2021\n",
      "Published: July 7, 2021\n",
      "Reviewpubs.acs.org/CR\n",
      "© 2021 The Authors. Published by\n",
      "American Chemical Society\n",
      "9816American Chemical Society\n",
      "9816\n",
      "https://doi.org/10.1021/acs.chemrev.1c00107\n",
      "Chem. Rev. 2021, 121, 9816−98724.5. ML for Structure Search, Sampling, and\n",
      "Generation 9844\n",
      "4.6. Multiscale Modeling 9845\n",
      "5. Selected Applications and Paths toward Insights 9845\n",
      "5.1. Molecular and Material Design 9845\n",
      "5.2. Retrosynthetic Technologies 9846\n",
      "5.3. Catalysis 98475.3. Catalysis 9847\n",
      "5.4. Drug Design 9850\n",
      "6. Conclusions and Outlook 9851\n",
      "Author Information 9853\n",
      "Corresponding Authors 9853\n",
      "Authors 9853\n",
      "Notes 9853\n",
      "Biographies 9853\n",
      "Acknowledgments 9854\n",
      "Acronyms 9854\n",
      "References 9855\n",
      "1. INTRODUCTION\n",
      "1.1. BackgroundReferences 9855\n",
      "1. INTRODUCTION\n",
      "1.1. Background\n",
      "A lasting challenge in applied physical and chemical sciences\n",
      "has been to answer the question: how can oneidentify and\n",
      "make chemical compounds or materials that have optimalproperties for a given purpose? A substantial part of research in\n",
      "physics, chemistry, and materials science concerns the\n",
      "discovery and characterization of novel compounds that can\n",
      "beneﬁt society, but most advances still are generally attributedto trial-and-error experimentation, and this requires signiﬁcant\n",
      "time and cost. Current global challenges create greater urgency\n",
      "for faster, better, and less expensive research and development\n",
      "eﬀorts. Computational chemistry (CompChem) methods havesigniﬁcantly improved over time, and they promise paradigm\n",
      "shifts in how compounds are fundamentally understood and\n",
      "designed for speciﬁc applications.\n",
      "Machine learning (ML) methods have in the past decadeswitnessed an unprecedented technological evolution enabling a\n",
      "plethora of applications, some of which have become daily\n",
      "companions in our lives.1−3 Applications of ML include\n",
      "technological ﬁelds, such as web search, translation, naturallanguage processing, self-driving vehicles, control architectures,\n",
      "and in the sciences, for example, medical diagnostics,4−8\n",
      "particle physics,9 nano sciences,10 bioinformatics,11,12 brain-computer interfaces,13 social media analysis,14 robotics,15,16\n",
      "and team, social, or board games.17−19 These methods have\n",
      "also become popular for accelerating the discovery and design\n",
      "of new materials, chemicals, and chemical processes.20 At thesame time, we have witnessed hype, criticism, and misunder-\n",
      "standing about how ML tools are to be used in chemical\n",
      "research. From this, we see a need for researchers working at\n",
      "the intersection of CompChem+ML to more criticallyrecognize the true strengths and weaknesses of each\n",
      "component in any given study. Speciﬁcally, we wanted to\n",
      "review why and how CompChem+ML can provide useful\n",
      "insights into the study of molecules and materials.While developing this Review, we polled the scienti ﬁc\n",
      "community with an anonymous online survey that asked for\n",
      "questions and concerns regarding the use of ML models with\n",
      "chemistry applications. Respondents raised excellent points\n",
      "including:including:\n",
      "1. ML methods are becoming less understood while they\n",
      "are also more regularly used as black box tools.\n",
      "2. Many publications show inadequate technical expertise\n",
      "in ML (e.g., inappropriate splitting of training, testing,and validation sets).\n",
      "3. It can be diﬃcult to compare diﬀerent ML methods and\n",
      "know which is the best for a particular application or\n",
      "whether ML should even be used at all.\n",
      "4. Data quality and context are often missing from MLmodeling, and data sets need to be made freely available\n",
      "and clearly explained.\n",
      "Additionally, when asked about the most exciting active and\n",
      "emerging areas of ML in the next ﬁve years, respondentsmentioned a wide range of topics from catalysis discovery, drug\n",
      "and peptide design,“above the arrow” reaction predictions,\n",
      "and generative models that promise to fundamentally trans-\n",
      "form chemical discovery. When asked about challenges thatML will not surmount in the nextﬁve years, respondents\n",
      "mentioned modeling complex photochemical and electro-\n",
      "chemical environments, discovering exact exchange-correlation\n",
      "functionals, and completely autonomous reaction discovery.This Review will give our perspective on many of these topics.\n",
      "As context for this Review, Figure 1 shows a heatmap\n",
      "depicting the frequency of ML keywords found in scientiﬁcThis Review will give our perspective on many of these topics.\n",
      "As context for this Review, Figure 1 shows a heatmap\n",
      "depicting the frequency of ML keywords found in scientiﬁc\n",
      "articles that also have keywords associated with di ﬀerentAmerican Chemical Society (ACS) technical divisions.\n",
      "Preparing thisﬁgure required several steps. First, lists of ML\n",
      "keywords were chosen. Second, lists of keywords were created\n",
      "by perusing ACS division symposia titles from over the pastﬁve years. Third, Python scripts used Scopus Application\n",
      "Programming Interfaces (APIs) to identify the number of\n",
      "scientiﬁc publications that matched sets of ML and division\n",
      "symposia keywords. Figure 1 elucidates several interestingpoints. First, the most popular ML approaches across all\n",
      "divisions are clearly neural networks, followed by genetic\n",
      "algorithms and support vector machines/kernel methods.\n",
      "Second, divisions such as physical (PHYS), analytical(ANYL), and environmental (ENVR) are already using diverse\n",
      "sets of ML approaches, while divisions such as inorganic\n",
      "(INOR), nuclear (NUCL), and carbohydrate (CARB) are\n",
      "primarily employing more distinct subsets of approaches, whileother divisions, such as educational (CHED), history (HIST),\n",
      "law (CHAL), and business-oriented divisions (BMGT and\n",
      "SCHB), that is, divisions that produce much fewer scholarly\n",
      "journal articles, are not linking to publications that mentionML. Third, ML has had more prevalence across practically all\n",
      "divisions over time. For further insight,Table 1 lists the top\n",
      "four keywords obtained from recent ACS symposium titles, as\n",
      "well as their respective contribution percentage reﬂected inFigure 1. There, one sees that a handful of keywords can\n",
      "signiﬁcantly overshadow matches in some of the bins, for\n",
      "exampled, “electro”, “sensor”, “protein”, and“plastic”. With any\n",
      "ML application, there will be a risk of imperfect data or userbias, but this is a useful launch point to appreciate how and\n",
      "where ML is being used in chemical sciences. A key takeaway\n",
      "is that we are witnessing an unprecedented crescendo in\n",
      "interest in ML over the last ten years (e.g.,Figure 1c) thanks toimproved understanding of the intersectionality of traditional\n",
      "science and engineering disciplines with rapidly evolving\n",
      "disciplines such as CompChem and data science.\n",
      "1.2. Motivation for This Review1.2. Motivation for This Review\n",
      "The survey results and literature analysis above showed an\n",
      "opportunity for a tutorial reference to help readers address\n",
      "future research challenges that will require joint applications ofChemical Reviews pubs.acs.org/CR Review\n",
      "https://doi.org/10.1021/acs.chemrev.1c00107\n",
      "Chem. Rev. 2021, 121, 9816−9872\n",
      "9817Figure 1. Heatmaps illustrating the extent that ML terms appear in scientiﬁc papers aligned by American Chemical Society (ACS) technicaldivisions from 2000 to 2010 (a) and from 2010−present (b). (c) Line graph showing the number of occurrences of any ML term being found inpapers attributed to the ACS PHYS division, from 2000−present. Figures were made by Charles D. Griego. Python scripts used to generate theseﬁgures and correspondingTable 1 are freely available with a creative commons attribution license. Readers are welcome to use, adapt, and sharethese scripts with appropriate attribution:https://github.com/keithgroup/scopus_searching_ML_in_chem_literature.).\n",
      "Chemical Reviews pubs.acs.org/CR Review\n",
      "https://doi.org/10.1021/acs.chemrev.1c00107\n",
      "Chem. Rev. 2021, 121, 9816−9872\n",
      "9818CompChem, ML, and chemical and physical intuition (CPI).\n",
      "This review will classify concepts using a rendition of a“data to\n",
      "wisdom” hierarchy, Figure 2 . Scholars have noted short-\n",
      "comings with similar constructs,21 but we use it to reﬂect astepladder for scientiﬁc progress, starting from collecting data\n",
      "and ending with overall impact. CompChem, ML, and CPI\n",
      "each have di ﬀerent strengths and weaknesses and bring\n",
      "synergistic opportunities. CPI alone can be employed toclimb the ladder from data to impact, but current CPI may\n",
      "only provide limited understanding or applicability outside of\n",
      "available data sets. However, CompChem is extraordinarily\n",
      "well-suited for generating high quality data that contain usefulinformation (vide infra,section 2) often more easily than via\n",
      "traditional experimentation. ML is likewise extremely well-\n",
      "suited for recognizing and accurately quantifying nonlinear\n",
      "relationships (vide infra, section 3), a task that is especiallydiﬃcult for even the most expert-level CPI alone. A key\n",
      "opportunity is that useful ML requires robust data sets, and\n",
      "these can be provided by CompChem as long as the CPI\n",
      "component is selecting and correctly interpreting appropriatemethods for the task at hand to productively climb the ladder\n",
      "toward impact (vide infra,section 4). We stress that the impact\n",
      "generation process shown inFigure 2 is by no means a linear\n",
      "one  on the contrary, it contains many loops and dead ends.Figure 2. Data−knowledge−wisdom hierarchy stepladder.\n",
      "Table 1. List of Top Ranked Keywords (Per ACS Division) with Corresponding Percentage of Matches for Any ML Term\n",
      "division rank 1 rank 2 rank 3 rank 4division rank 1 rank 2 rank 3 rank 4\n",
      "PHYS electro * (56.3%) spectroscopy (9.7%) ion * (6.0%) nano * (5.5%)\n",
      "ANYL sensor * (55.4%) spectroscopy (13.1%) characterization * (11.6%) spectrometry (4.3%)ENVR *sensor* (60.9%) soil * (14.5%) water quality (4.2%) environmental monitor * (3.0%)\n",
      "AGFD protein * (31.9%) agricultur * (18.2%) food (10.8%) fruit * (5.7%)\n",
      "ENFL fuel * (19.2%) petroleum (11.6%) energy e ﬃciency (11.1%) batter * (10.7%)AGRO soil (43.3%) crop * (25.4%) groundwater (11.5%) developing countr * (4.4%)\n",
      "ORGN protein * (64.6%) amino acid * (19.7%) peptide * (8.4%) aromatic * (3.2%)\n",
      "POLY plastic * (51.1%) polymer * (37.7%) polymeriz * (5.0%) polymeric (2.8%)PMSE *polymer* (50.4%) *peptide* (30.4%) thin ﬁlm* (8.9%) tissue engineering (4.3%)\n",
      "BIOT biochemi * (37.9%) biophysic * (18.3%) systems biology (10.3%) biotechnology (9.9%)GEOC groundwater (33.4%) mining (31.6%) *geochem* (12.4%) anthropogenic (10.9%)\n",
      "MEDI protein interaction * (25.7%) drug discovery (19.1%) drug design (19.1%) antibiotic * (11.3%)COMP drug discovery (18.7%) drug design (18.6%) molecular model * (14.3%) protein database * (13.1%)\n",
      "COLL nanoparticle * (21.2%) adsorption (19.6%) thin ﬁlm* (14.9%) tribolog * (9.6%)BIOL drug discovery (41.9%) protein folding (19.3%) biosynthesis (12.2%) cytochrome * (12.2%)\n",
      "TOXI toxi * (99.2%) chemical exposure * (0.6%) antibody drug conjugate * (0.1%)CATL cataly * (64.0%) metal oxides (20.2%) photocataly * (5.3%) surface chemistry (2.8%)\n",
      "CINF drug discovery (51.7%) computational chemistry\n",
      "(17.5%)\n",
      "bio* modeling (7.8%) chem * database* (7.6%)bio* modeling (7.8%) chem * database* (7.6%)\n",
      "INOR electrochem * (67.1%) nanomaterial * (14.9%) organometallic * (5.8%) metal organic framework * (4.0%)\n",
      "NUCL nuclear fuel * (28.6%) isotope * (27.3%) radioisotope * (15.0%) nuclear medicine * (9.0%)CARB carbohydrate * (43.0%) glycoprotein * (42.4%) glycan * (6.6%) oligosaccharide * (5.6%)\n",
      "RUBB rubber * (100.0%)\n",
      "CELL cellulose (41.7%) polysaccharide * (26.3%) lignin (16.3%) lignocellulos * (9.7%)I&EC water puri ﬁcation (38.1%) industrial chem * (23.7%) rare earth element * (11.9%) industrial and engineering chemistry\n",
      "(8.3%)\n",
      "FLUO ﬂuorine* (99.8%) radiopharmaceutical chem *\n",
      "(0.2%)(0.2%)\n",
      "CHED chem * class* (76.4%) chem * communication* (8.5%) chem * educat* (5.5%) lab * safety (5.5%)\n",
      "CHAS chem * safety (51.5%) lab * safety (16.7%) environmental health and safety\n",
      "(16.7%)\n",
      "chem* regulations (15.2%)CHAS chem * safety (51.5%) lab * safety (16.7%) environmental health and safety\n",
      "(16.7%)\n",
      "chem* regulations (15.2%)\n",
      "BMGT chem * compan* (65.4%) chem * enterprise* (28.8%) chem * business* (3.8%) chem * research and development (1.9%)SCHB commercial chem * (50.0%) chem * sector* (28.6%) academic entrepreneur * (14.3%) science advoca * (7.1%)\n",
      "HIST chem * histor* (53.8%) evolution of chem * (30.8%) history of chem * (15.4%)\n",
      "PROF chem * education (100.0%)PROF chem * education (100.0%)\n",
      "CHAL pharmaceutical patent *\n",
      "(60.0%)\n",
      "chem* in commerce (30.0%) chem * patent* (10.0%)\n",
      "Chemical Reviews pubs.acs.org/CR Review\n",
      "https://doi.org/10.1021/acs.chemrev.1c00107\n",
      "Chem. Rev. 2021, 121, 9816−9872\n",
      "9819As we show later (in Section 4 ), within the troika of\n",
      "CompChem+ML+CPI, ML acts as a catalyst that accelerates\n",
      "explorative data-driven hypotheses generation. Automatically\n",
      "generated hypotheses are then validated and calibrated withCompChem and CPI to yield further improved ML modeling\n",
      "(enriched by more physical prior knowledge), which then\n",
      "loops back with improved hypotheses. This feedback loop is\n",
      "the key to the modern knowledge discovery leading to insight,wisdom and hopefully positive impacts to society.\n",
      "2. COMPCHEM AND NOTABLE INTERSECTIONS WITH\n",
      "ML\n",
      "2.1. Computational Modeling, Data, and Information\n",
      "Across Many Scales\n",
      "We consider quantum mechanics as described by theWe consider quantum mechanics as described by the\n",
      "nonrelativistic time-independent Schrödinger equation as our\n",
      "“standard model” because it accurately represents the physics\n",
      "of charged particles (electrons and nuclei) that make up almostall molecules and materials. Indeed, this opinion has been held\n",
      "by some for almost a century:\n",
      "The fundamental laws necessary for the mathematical\n",
      "treatment of a large part of physics and the whole ofchemistry are thus completely known, and the diﬃculty lies\n",
      "only in the fact that application of these laws leads to\n",
      "equations that are too complex to be solved.\n",
      "P. A. M. Dirac, 1929\n",
      "Any theoretical method for predicting molecular or materialphenomena mustﬁrst be rooted in quantum mechanics theory\n",
      "and then suitably coarse-grained and approximated so that it\n",
      "can be applied in a practical setting. CompChem, or more\n",
      "precisely, computational quantum chemistry deﬁnes computa-tionally driven numerical analyses based on quantum\n",
      "mechanics. In this section, we will explain how and why\n",
      "diﬀerent CompChem methods capture di ﬀerent aspects of\n",
      "underlying physics. Speciﬁcally, this section provides a conciseoverview of the broad range of CompChem methods that are\n",
      "available for generating data sets that would be useful for ML-\n",
      "assisted studies of molecules and materials.\n",
      "2.1.1. Models and Levels of Abstraction. Models2.1.1. Models and Levels of Abstraction. Models\n",
      "extract information from data. The renowned statistician\n",
      "George Box famously discussed“good models ” as those\n",
      "characterized as “simple”, “illuminating”, and “useful”.22 Goodmodels should beparsimonious and describe essential relation-\n",
      "ships without overelaboration. The ideal gas equation,PV =\n",
      "nRT, exempliﬁes a good model. The ideal gas equation relates\n",
      "macroscopic pressure (P), volume (V), number of molecules(n), and temperature (T) of gases under idealized conditions,\n",
      "without requiring explicit knowledge of the processes occurring\n",
      "on an atomic scale. Its simple functional form needs just oneparameter, the ideal gas constantR, and this makes it possible\n",
      "to formulate useful insights, such as how at constant pressure a\n",
      "gas expands with rising temperature. On the other hand, this\n",
      "elegant equation only holds for conditions where the gasbehaves as an ideal gas. The derivation of more accurate\n",
      "models of gases requires more mathematically complicated\n",
      "equations of state that rely on more free parameters23 that in\n",
      "turn obfuscate physical insights, require more computationaleﬀort to solve, and thus make the model less “good”. This\n",
      "example also oﬀers a convenient connection to ML models\n",
      "that will be discussed later in section 3. As mathematical\n",
      "models for complex phenomena become more complicatedand less intuitive to derive, ML models that infer nonlinear\n",
      "relationships from data become more applicable when\n",
      "increasing amounts of empirical data become available.\n",
      "Alternatively, the conventional CompChem treatmententails ﬁrst determining the system’s relevant geometry and\n",
      "its total ground state energy, and from that physical properties\n",
      "of interest (e.g., pressure, volume, band gap, polarizability, etc.)can be obtained using quantum and statistical mechanics. In\n",
      "this section, we discuss the relevant CompChem methods for\n",
      "these. While the mathematical physics for these methods might\n",
      "occasionally be too complicated for a user to fully understand,this section, we discuss the relevant CompChem methods for\n",
      "these. While the mathematical physics for these methods might\n",
      "occasionally be too complicated for a user to fully understand,\n",
      "many algorithms exist so that they can still be easily run in a“black-box ” way with modern computational chemistry\n",
      "software and accompanying tutorials.24−27 CompChem thus\n",
      "serves as an invaluable tool to generate data and information\n",
      "for knowledge and insights across many length and time scales.Figure 3 is an adaption of a multiscale hierarchy of diﬀerent\n",
      "classes of CompChem methods. It shows their applicability for\n",
      "modeling diﬀerent length and time scales and depicts how\n",
      "large scale models may be developed based on smaller scale\n",
      "theories.theories.\n",
      "2.1.2. CompChem Representations. Integral to every\n",
      "CompChem study is the user’s representation for the system,\n",
      "that is, how the user chooses to describe the system.\n",
      "CompChem representations can range from simple and lucid(e.g., a precise chemical system such as a water molecule\n",
      "isolated in a vacuum) to complex and ambiguous (e.g., a\n",
      "putative but speculative depiction of a solid−liquid interface\n",
      "under electrochemical conditions). Approximate wavefunc-tions (expressed on a basis set of mathematical functions) or\n",
      "approximate Hamiltonians (referred to as levels of theory) as\n",
      "described below in this section can also be considered\n",
      "representations. One might then say that many representationsfor diﬀerent components of a system will constitute an overall\n",
      "representation, and this is true. The point we make is that the\n",
      "validity ofany computational result depends on the overall\n",
      "representation, and sometimes an incorrect representation mayprovide a correct result due to“fortuitous error cancellation”.\n",
      "In CompChem studies, a valid representation is one that\n",
      "captures the nature of the physical phenomena of a system. For\n",
      "a molecular example, if one is determining the bond energy ofa large biodiesel molecule using CompChem methods,28 it\n",
      "may or may not be justiﬁed to approximate a nearby long-chain\n",
      "Figure 3. Hierarchy of computational methods and corresponding\n",
      "time and length scales. QM stands for Quantum Mechanics.Chemical Reviews pubs.acs.org/CR Review\n",
      "https://doi.org/10.1021/acs.chemrev.1c00107\n",
      "Chem. Rev. 2021, 121, 9816−9872\n",
      "9820alkyl group (−CnH(2n+1)) simply as a methyl (−CH3) or even a\n",
      "hydrogen atom. Indeed, choosing such a representation can\n",
      "sometimes be a useful example of CPI since alkyl bonds usually\n",
      "exhibit relatively short-ranged interactions (a feature that willbe discussed in the context of ML in more detail insection\n",
      "4.1.3.). An atomic scale geometry with fewer atoms would\n",
      "reduce the computational cost of the study or allow a more\n",
      "accurate but more computationally expensive calculation to berun. On the other hand, it might also be a poor choice if the\n",
      "chemical group, for example, a substituted alkyl group\n",
      "participated in physical organic interactions, such as subtle\n",
      "steric, induction, or resonance eﬀects.29 For a solid-stateexample, a user might exercise good CPI by assuming that a\n",
      "relatively small unit cell under periodic boundary conditions\n",
      "would capture salient features of a bulk material or a material\n",
      "surface (as is often the case for many metals). On the otherhand, subtle symmetry-breaking eﬀects in materials (e.g.,\n",
      "distortions arising from tilting octahedra groups in perov-\n",
      "skites,30 or surface reconstruction phenomena that occur on\n",
      "single crystals)31 might only be observed when consideringlarger and more computationally expensive unit cells. Relevant\n",
      "to both examples, it may also be that the CompChem method\n",
      "itself brings errors that obfuscate phenomena that the user\n",
      "intends to model. In general, CompChem errors may be due to1) errors introduced by the user in the initial set up of the\n",
      "CompChem application, or 2) errors in the CompChem\n",
      "method when treating the physics of the system. Insection 3,\n",
      "we will discuss how the choice of ML representation also playssimilarly critical roles in determining whether and to what\n",
      "extent an ML model is useful.\n",
      "2.1.3. Method Accuracy.The quantitative accuracy of a\n",
      "CompChem model stems from its suitability in describing thesystem. As explained above, an observed accuracy will depend\n",
      "on the representation being used. High-quality CompChem\n",
      "calculations have traditionally been benchmarked against data\n",
      "sets that consist of well-controlled and relatively precisethermochemistry experiments on small, isolated molecules.32,33\n",
      "The error bars for standard calorimetry experiments are\n",
      "approximately 4 kJ/mol (or 1 kcal/mol or 0.04 eV), and\n",
      "computational methods that can provide greater accuracy thanthis are stated as achieving“chemical accuracy”. Note that this\n",
      "term should be used when describing the accuracy of the\n",
      "method compared to the most accurate data possible; for\n",
      "example, if one CompChem method was found to reproduceanother CompChem method within 1 kJ/mol, but both\n",
      "methods reproduce experimental data with errors of 20 kJ/\n",
      "mol, then neither method should be called chemically accurate.\n",
      "There are many well-established reasons why CompChemmodels can bring errors. For example, errors may be due to\n",
      "size consistency34 or size extensivity 35 problems that are\n",
      "intrinsic within the CompChem method, larger systems\n",
      "sometimes embody signiﬁcant medium and long-rangesometimes embody signiﬁcant medium and long-range\n",
      "interactions (e.g., van der Waals forces)36 or self-interaction\n",
      "errors37 that might not be noticeable in small test cases. The\n",
      "recommended path forward is to consider which fundamentalinteractions are in play in the system and then use a\n",
      "CompChem model that is adequate at describing those\n",
      "interactions. Besides this, users should make use of existing\n",
      "tutorial references that provide practical knowledge aboutwhich parameters in a CompChem calculation should be\n",
      "carefully noted, for example ref38. Historically the most\n",
      "popular CompChem methods for molecular and materials\n",
      "modeling (the B3LYP39 and PBE 40 exchange correlationfunctionals, see section 2.2.3.) are often said to have an\n",
      "expected accuracy of about 10−15 kJ/mol (or 2−4 kcal/mol\n",
      "or 0.1−0.2 eV) when modeling diﬀerences between the total\n",
      "energies of two similar systems, and errors are expected to besomewhat larger when considering transition state energies.or 0.1−0.2 eV) when modeling diﬀerences between the total\n",
      "energies of two similar systems, and errors are expected to be\n",
      "somewhat larger when considering transition state energies.\n",
      "Though this is used as a simple rule, it is obviously anoversimpliﬁcation and actual accuracy is only assessed by\n",
      "thoughtful benchmarking of the case being considered.41−45\n",
      "2.1.4. Precision and Reproducibility. In CompChem,\n",
      "one normally assumes that any two users using the samerepresentation for the system with the same code on the same\n",
      "computing architecture will obtain the exact same result within\n",
      "the numerical precision of the computers being used. This is\n",
      "not always the case, especially for molecular dynamics (MD)simulations that often rely on stochastic methods.46 Computa-\n",
      "tional precision also becomes more concerning when there are\n",
      "diﬀerent versions of codes in circulation, errors that might arisefrom diﬀerent compilers and libraries, and a lack of consensus\n",
      "in the community about which computational methods and\n",
      "which default settings should be used for speciﬁc application\n",
      "systems, for example, grid density selections,47 or standardkeywords for molecular dynamics simulations.46,48 There have\n",
      "been eﬀorts to conﬁrm that diﬀerent codes can reproduce\n",
      "energies for the same system representation,48,49 but some\n",
      "commercial codes hold proprietary licenses that restrictpublications that critically benchmark calculation accuracy\n",
      "and timings across diﬀerent codes. A path forward to beneﬁt\n",
      "the advancement of insight is the development of (open)\n",
      "source codes50 that perform as well if not better thancommercial codes. While increased access to computational\n",
      "algorithms is beneﬁcial, it also raises the need for enforcing\n",
      "high standards of quality and reproducibility.51,52 We are also\n",
      "glad to see active developments to more lucidly show how anyset of computational data is generated, precisely with which\n",
      "codes, keywords, and auxiliary scripts and routines.53−56 We\n",
      "are now in an era where truly massive amounts of data and\n",
      "information can be generated for CompChem+ML eﬀorts. Togo forward, one needs to know what constitutes good and\n",
      "useful data, and the next section provides an overview of how\n",
      "to do this using CompChem.\n",
      "2.2. Hierarchies of Methods\n",
      "Earlier we mentioned that a usual task in CompChem is tocalculate the ground state energy of an atomic scale system.\n",
      "Indeed, CompChem methods can determine the energy for a\n",
      "hypothetical conﬁguration of atoms, and this constitutes the\n",
      "potential energy surface (PES) of the system (Figure 4). ThePES is a hypersurface spanning 3N dimensions, whereN is the\n",
      "number of atoms in the system. Since the PES is used to\n",
      "analyze chemical bonding between atoms within the system,\n",
      "the PES can also be simpliﬁed by ignoring translational androtational degrees of freedom for the entire system. This\n",
      "reduces the dimensionality of the PES from 3N to 3N − 5 for\n",
      "linear systems (e.g., diatomic molecules or perfectly linear\n",
      "molecules such as acetylene) or 3N − 6 for all other nonlinearsystems. Furthermore, since visualization is diﬃcult beyond\n",
      "three dimensions, PES drawings will show a 1-D or 2-D\n",
      "projection of this hypersurface where thez-axis is convention-\n",
      "ally used to represent the scale for system energy.Any arbitrary PES will contain several interesting features.\n",
      "Minima on the PES correspond to mechanically stable\n",
      "conﬁgurations of a molecule or material, for example reactant\n",
      "and product states of a chemical reaction or di ﬀerentChemical Reviews pubs.acs.org/CR Review\n",
      "https://doi.org/10.1021/acs.chemrev.1c00107\n",
      "Chem. Rev. 2021, 121, 9816−9872\n",
      "9821conformational isomers of a molecule. Because they are\n",
      "minima, the second derivative of the energy given by the PES\n",
      "with respect to any dimension will be positive. Minima can also\n",
      "be connected by pathways, which indicate chemical trans-formations (Figure 4, red line). Along such pathways, the\n",
      "second derivative can be positive, zero, or negative, but all\n",
      "other second derivatives must be positive. Transition states are\n",
      "ﬁrst-order saddlepoints and thus represent a maximum in onecoordinate and a minimum along all others. They correspond\n",
      "to the lowest energy barriers connecting two minima on the\n",
      "PES and are hence important for characterizing transitions\n",
      "between PES minima (e.g., chemical reactions). Second-ordersaddle points57 and bifurcating pathways58 can also exist, but\n",
      "these are not discussed further here.\n",
      "A wide range of higher-level properties of the system can be\n",
      "predicted or derived using the PES, including predictedthermodynamic binding constants, kinetic rate constants for\n",
      "reactions, or properties based on dynamics of the system. The\n",
      "task is then to choose an appropriate CompChem method that\n",
      "can carry out energy and gradient calculations on the system’sPES. Figure 5 shows several di ﬀerent hierarchies for\n",
      "CompChem methods capable of doing this. Note that all of\n",
      "these methods mentioned in thisﬁgure fall in the categories of\n",
      "the bottom two regions in the multiscale hierarchyFigure 3.All of these methods in principle could be used to develop\n",
      "Figure 4.Potential energy surface (PES) of aﬁctional system with the\n",
      "two coordinates R1 and R2. The minima of the PES correspond tostable states of a system, such as equilibrium con ﬁgurations and\n",
      "reactants or products. Minima can be connected by paths (red line),\n",
      "along which rearrangements and reactions can occur. The maximumalong such a path is called a transition state. Transition states areﬁrst-\n",
      "order saddle points, a maximum in one coordinate and minima in all\n",
      "others. They correspond to the minimum energy required totransition between two PES minima and play a crucial role in the\n",
      "description of chemical transformations.\n",
      "Figure 5. (a) “Magic cube”59 depiction of hierarchies of correlated wavefunction approaches. (b)“Jacob’s Ladder”60 depiction of hierarchies ofKohn−Sham density functional theory (DFT) approaches. (c) Hierarchies of atomistic potentials. (d) Overall hierarchies in predictive atomic scale\n",
      "modeling methods.\n",
      "Chemical Reviews pubs.acs.org/CR Review\n",
      "https://doi.org/10.1021/acs.chemrev.1c00107https://doi.org/10.1021/acs.chemrev.1c00107\n",
      "Chem. Rev. 2021, 121, 9816−9872\n",
      "9822coarse-grained or continuum models as well. Also note that\n",
      "methods in Figure 5 will bring very diﬀerent computational\n",
      "costs and opportunities for methods involving ML.\n",
      "2.2.1. Wavefunction Theory Methods. In standard2.2.1. Wavefunction Theory Methods. In standard\n",
      "computational quantum chemistry, a system’s energy can be\n",
      "computed in terms of the Schro ̈dinger equation.61−63 The\n",
      "wavefunction that will be used to represent the positions ofelectrons and nuclei in the system (Ψ(r, R)) is hard to intuit\n",
      "since it can be complex valued. However, its square describes\n",
      "the real probability density of the nuclear (R) and electronicpositions (r). In a real system, the position and interactions of\n",
      "a single particle in the system with respect to all other particles\n",
      "will be correlated , and this makes exactly solving the\n",
      "Schrödinger equation impossible for almost all systems ofpractical interest. To make the problem more tractable, one\n",
      "may exploit the Born−Oppenheimer approximation;64 since\n",
      "nuclei are expected to move much slower than the electrons\n",
      "they can be approximated as stationary at any point along thePES. This allows the energy to be calculated using the time-\n",
      "independent Schrödinger equation and solving the eigenvalue\n",
      "problem:\n",
      "H TV E()̂Ψ= ̂+ ̂ Ψ= Ψ (1)\n",
      "Here, the Hamiltonian operator ( Ĥ) is the sum of thekinetic (T̂) and potential (V̂) operators,Ψ is the wavefunction\n",
      "(i.e., an eigenfunction) that represents particles in the system,\n",
      "andE is the energy (i.e., an eigenvalue). In this way, nuclei canbe treated as ﬁxed point charges, and then, eq 1 can be\n",
      "transformed into the so-called electronic Schrödinger equation,\n",
      "where the Hamiltonian Ĥel and wavefunction Ψel(r; R) now\n",
      "only depend on the nuclear coordinates R in a parametric\n",
      "fashion:fashion:\n",
      "H TV V V\n",
      "E\n",
      "rR r rR R r\n",
      "rR\n",
      "rR\n",
      "(; ) () (; ) ( ) ()\n",
      "(; )\n",
      "(; )\n",
      "el el e eN NN ee\n",
      "el\n",
      "el el\n",
      "̂ Ψ= [ ̂ + ̂ + ̂ + ̂ ]\n",
      "Ψ\n",
      "=Ψ (2)\n",
      "The above expression hasĤel composed of single electron (e)\n",
      "and pairwise electron−nuclear (eN), nuclear−nuclear (NN),and electron−electron (ee) terms. Here, we will now implicitly\n",
      "assume the Born −Oppenheimer approximation throughout\n",
      "and leave oﬀ the subscript indicating the electronic problem.\n",
      "However, we note that the Born−Oppenheimer approximationis not always suﬃcient and computationally intensive non-\n",
      "adiabatic quantum dynamics may be required.65 In certain\n",
      "cases, semiclassical treatments are appropriate; for example,\n",
      "nonadiabatic eﬀects between electrons and nuclei can beconsidered using nuclear-electronic orbital methods.66\n",
      "A second common approximation is to expand the total\n",
      "electronic wavefunction in terms of one-electron wave-\n",
      "functions (i.e., spin orbitals): ϕ(ri). Electrons are Fermionsand therefore exhibit antisymmetry, which in turn results in the\n",
      "Pauli exclusion principle. Antisymmetry means that the\n",
      "interchange of any two particles within the system should\n",
      "bring an overall sign change to the wavefunction (i.e., from +to−, or vice versa). This property is conveniently captured\n",
      "mathematically by combining one electron spin orbitals into\n",
      "the form of a Slater determinant:\n",
      "μ\n",
      "∂∏ ∂\n",
      "μ\n",
      "n\n",
      "rr\n",
      "rr\n",
      "rr\n",
      "(, , ) 1\n",
      "() ()\n",
      "() ()\n",
      "n\n",
      "n\n",
      "n n n\n",
      "1\n",
      "1 11\n",
      "1\n",
      "ϕϕ\n",
      "ϕϕ\n",
      "Ψ ··· =\n",
      "!\n",
      "(3)() ()\n",
      "n\n",
      "n\n",
      "n n n\n",
      "1\n",
      "1 11\n",
      "1\n",
      "ϕϕ\n",
      "ϕϕ\n",
      "Ψ ··· =\n",
      "!\n",
      "(3)\n",
      "Note that a determinant’s sign changes whenever two columns\n",
      "or rows are interchanged, and in a Slater determinant this\n",
      "corresponds to interchanging electrons and thus the physicallyappropriate sign change for the overall wavefunction. Addi-\n",
      "tionally,\n",
      "n\n",
      "1\n",
      "!\n",
      "is a normalizing factor to ensure the wavefunction\n",
      "is unitary.\n",
      "The spin orbitals can be treated as a mathematical expansionusing a basis set ofμ functions χμ, each having coeﬃcients cμi,\n",
      "which are generally Gaussian basis functions,67−69 Slater-type\n",
      "hydrogenic orbitals,70 or plane waves under periodic boundary\n",
      "conditions:71−73\n",
      "ci i∑ϕχ=\n",
      "μ\n",
      "μ μ\n",
      "(4)conditions:71−73\n",
      "ci i∑ϕχ=\n",
      "μ\n",
      "μ μ\n",
      "(4)\n",
      "The diﬀerent types of mathematical functions bring diﬀerent\n",
      "strengths and weaknesses, but these will not be discussed\n",
      "further here. A universal point is that larger basis sets will havemore basis functions and thus give a moreﬂexible and physicalstrengths and weaknesses, but these will not be discussed\n",
      "further here. A universal point is that larger basis sets will have\n",
      "more basis functions and thus give a moreﬂexible and physical\n",
      "representation of electrons within the system. On one handthis can be crucial for capturing subtle electronic structure\n",
      "eﬀects due to electron correlation. On the other hand, larger\n",
      "basis sets also necessitate signiﬁcantly higher computational\n",
      "eﬀort. A standard technique to avoid high computational eﬀortin electronic structure calculations is to replace nonreacting\n",
      "core electrons with analytic functions using eﬀective core\n",
      "potentials (ECPs, i.e., pseudopotentials).74−89 This requires\n",
      "reformulating the basis sets that describe the valence space ofthe atoms, for example see refs90 and 91. Larger nuclei that\n",
      "bring higher atomic numbers and larger numbers of electrons\n",
      "will also exhibit relativistic eﬀects,92 and relativistic Hamil-\n",
      "tonians are based on the Dirac equation 93,94 or quantumelectrodynamics.95 These methods can range from reasonably\n",
      "cost-eﬀective methods96,97 to those bringing extremely high\n",
      "computational cost.98 Practical applications have traditionally\n",
      "used standard nonrelativistic Hamiltonian methods, along withECPs (or pseudopotentials) that have been explicitly\n",
      "developed to account for compressed core orbitals that result\n",
      "from relativistic eﬀects.\n",
      "Using the Born −Oppenheimer approximation ( eq 2 )\n",
      "together with a Slater determinant wavefunction ( eq 3 )expressed in aﬁnite basis set (eq 4) brings about the simplest\n",
      "wavefunction based method, the Hartree −Fock (HF)\n",
      "approach (for historical context see refs 99−101). The HF\n",
      "method is a mean ﬁeld approach, where each electron istreated as if it moves within the averageﬁeld generated by all\n",
      "other electrons. It is generally considered inaccurate when\n",
      "describing many chemical systems, but it continues to serve asa critical pillar for CompChem electronic structure calculations\n",
      "since it either establishes the foundation for all other accurate\n",
      "methods or provides energy contributions (i.e., exact\n",
      "exchange) that is not provided in some CompChem methods.CompChem methods that achieve accuracy higher than HF\n",
      "theory are said to containelectron correlation , a critical\n",
      "component for understanding molecules and materials (as\n",
      "described in more detail insection 2.2.2.). Expressing Ψ as aSlater determinant and rearranging eq 2 while temporarily\n",
      "neglecting nuclear−nuclear interactions allows one to deﬁne\n",
      "Chemical Reviews pubs.acs.org/CR Review\n",
      "https://doi.org/10.1021/acs.chemrev.1c00107\n",
      "Chem. Rev. 2021, 121, 9816−9872\n",
      "9823the HF energy in terms of integrals of the electronic spin\n",
      "orbitals:\n",
      "ß\n",
      "´≠ÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖ ÆÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖ\n",
      "´≠ÖÖÖÖÖÖÖÖÖ ÆÖÖÖÖÖÖÖÖÖ\n",
      "´≠ÖÖÖÖÖÖÖÖÖ ÆÖÖÖÖÖÖÖÖÖ\n",
      "E\n",
      "Z\n",
      "rr r\n",
      "rr rR r\n",
      "rr r r rr rr\n",
      "rr r r rr rr\n",
      "d( ) 1\n",
      "2 ()\n",
      "d( ) ( )\n",
      "d d () () 1 ()()\n",
      "d d () () 1 ()()2 ()\n",
      "d( ) ( )\n",
      "d d () () 1 ()()\n",
      "d d () () 1 ()()\n",
      "i\n",
      "i\n",
      "T\n",
      "i\n",
      "i\n",
      "i\n",
      "N\n",
      "V\n",
      "i\n",
      "ij i\n",
      "ii\n",
      "V\n",
      "jj\n",
      "ij i\n",
      "ij\n",
      "V\n",
      "ji\n",
      "HF 1 1\n",
      "2\n",
      "1\n",
      "11\n",
      "1\n",
      "1\n",
      ",\n",
      "12 1 1\n",
      "12\n",
      "(Coulomb)\n",
      "22\n",
      ",\n",
      "12 1 1\n",
      "12\n",
      "(exchange)\n",
      "22\n",
      "e\n",
      "eN\n",
      "ee\n",
      "ee\n",
      "∫\n",
      "∫\n",
      "∬\n",
      "∬\n",
      "∑\n",
      "∑ ∑\n",
      "∑\n",
      "∑\n",
      "ϕϕ\n",
      "ϕϕ\n",
      "ϕϕ ϕϕ\n",
      "ϕϕ ϕϕ\n",
      "=− * ∇\n",
      "− *\n",
      "|− |\n",
      "+ *\n",
      "|−|\n",
      "*\n",
      "− *\n",
      "|−|\n",
      "*\n",
      "α\n",
      "α\n",
      "αϕϕ ϕϕ\n",
      "=− * ∇\n",
      "− *\n",
      "|− |\n",
      "+ *\n",
      "|−|\n",
      "*\n",
      "− *\n",
      "|−|\n",
      "*\n",
      "α\n",
      "α\n",
      "α\n",
      ">\n",
      ">\n",
      "̂\n",
      "̂\n",
      "̂\n",
      "̂\n",
      "(5)\n",
      "where the ﬁrst two terms are referred to as one-electron\n",
      "integrals and represent the kinetic energy of the electrons and\n",
      "the potential energy contributions from electron-nucleiinteractions. The remaining terms are two-electron integrals\n",
      "that describe the potential energy arising from electron−\n",
      "electron interactions and are called Coulomb and exchange\n",
      "integrals. Using Lagrange multipliers, one can express the HFequation in a compact matrix form, the so-called Roothan−\n",
      "Hall equations,102−104 which allow for an eﬃcient solution:\n",
      "FCS Cϵ= (6)\n",
      "Each matrix has a size ofμ × μ, whereμ is the number of basisfunctions used to express the orbitals of the system.C is a\n",
      "coeﬃcient matrix collecting the basis coeﬃcients cμi (see eq 4),\n",
      "while S is the overlap matrix measuring the degree of overlapbetween individual basis functions andϵ is a diagonal matrix of\n",
      "the spin orbital energies. Finally,F is the Fock matrix, with\n",
      "elements of a similar form as ineq 5, but expressed in terms ofbasis functionsχμ. One important detail not readily apparent in\n",
      "eq 6is that the Fock matrix depends on the orbital coeﬃcients\n",
      "that must be provided beforeeq 6can be solved. As such,eq 6\n",
      "cannot be solved in closed form, but instead requires a so-called self-consistentﬁeld approach. Starting from an arbitrary\n",
      "set of trial (i.e., initial guess) functions, one iteratively solves\n",
      "for optimal molecular orbital coeﬃcients, which are then usedto construct a new Fock matrix, until a minimum energy is\n",
      "reached in accordance with the variational principle of\n",
      "quantum mechanics. Evaluating and transforming the two-\n",
      "electron integrals ineq 5 are a signiﬁcant bottleneck for thesecalculations and thus the computational e ﬀort of the HF\n",
      "methods formally scales as () 46 μ with the number of basis\n",
      "functions. This means that a calculation on a system twice as\n",
      "large will require at least 24 = 16 times as much computingtime. The electronic exchange interaction resulting from the\n",
      "antisymmetry of the wavefunction imposes a strong constraint\n",
      "on the mathematical form of ML models for electronic\n",
      "wavefunctions. Construction of eﬃcient and reliable antisym-metric ML models for the many-body wavefunction is an\n",
      "important area of current research.105,106\n",
      "2.2.2. Correlated Wavefunction Methods.The system’s\n",
      "correlation energy is de ﬁned as sum of electron −electroninteractions that originate beyond the mean-ﬁeld approxima-\n",
      "tion for electron−electron interactions that is provided by HF\n",
      "theory. While correlation energy makes up a rather small\n",
      "contribution to the overall energy of a system (usually about1% of the total energy), because internal energies in molecular\n",
      "and material systems are so enormous, this contribution\n",
      "becomes rather signiﬁcant. As an example, most molecular\n",
      "crystals would be unstable as solids if calculated using the HFlevel of theory. The missing component is attractive forces that\n",
      "are obtained from levels of theory that account for correlation\n",
      "energy. Correlation energies are obtained by calculating\n",
      "additional electron−electron interaction energies that arisefrom diﬀerent arrangements of electron conﬁgurations (i.e.,\n",
      "diﬀerent possible excited states) that are not treated with the\n",
      "mean ﬁeld approach of HF theory.\n",
      "The most complete correlation treatment is the fullconﬁguration interaction (FCI) method, which is the exact\n",
      "numerical solution of the electronic Schrödinger equation (in\n",
      "the complete basis limit) that considers interactions arising\n",
      "from all possible excited conﬁgurations of electrons. The FCInumerical solution of the electronic Schrödinger equation (in\n",
      "the complete basis limit) that considers interactions arising\n",
      "from all possible excited conﬁgurations of electrons. The FCI\n",
      "wavefunction takes the form of a linear combination of allpossible excited Slater determinants which can be generated\n",
      "from a single HF reference wavefunction by electron\n",
      "excitations:\n",
      "aa a ...FCI 0 HF\n",
      ",, , ,\n",
      "∑∑Ψ =Ψ + Ψ + Ψ+\n",
      "αβ\n",
      "β\n",
      "α\n",
      "β\n",
      "α\n",
      "αβγδ\n",
      "βδ\n",
      "αγ\n",
      "βδ\n",
      "αγ\n",
      "(7)\n",
      "where Ψβαβ\n",
      "β\n",
      "α\n",
      "β\n",
      "α\n",
      "αβγδ\n",
      "βδ\n",
      "αγ\n",
      "βδ\n",
      "αγ\n",
      "(7)\n",
      "where Ψβ\n",
      "α represents the Slater determinant obtained by\n",
      "exciting an electron from orbitalα into an unoccupied orbital\n",
      "β, and theas are expansion coeﬃcients determining the weightof the diﬀerent contributing conﬁgurations. Expectedly, FCI\n",
      "calculations scale extremely poorly with the number of\n",
      "electrons in the system ( n()6 ! ), as the number of possible\n",
      "conﬁgurations grows rapidly, making them feasible only forsmall molecules. For an example of the state of the art, FCI\n",
      "calculations have been used to benchmark highly accurate\n",
      "methods on calculations on a benzene molecule.107\n",
      "Most correlated wavefunction methods use a subset of thepossible conﬁgurations ineq 7to be computationally tractable.\n",
      "The conﬁguration interaction (CI)108 method for example\n",
      "only includes determinants up to a certain permutation level\n",
      "(e.g., single and double excitations in CISD). Alternatively,MPn35 (e.g., MP2) recovers the correlation energy by applying\n",
      "diﬀerent orders of perturbation theory. Coupled cluster theory,\n",
      "another widely used post-HF method, includes additional\n",
      "electron conﬁgurations via cluster operators.109 One coupledcluster method that involves single, double, and perturbative\n",
      "triples excitations, CCSD(T), is referred to as the“gold-\n",
      "standard” approach for CompChem electronic structure\n",
      "methods since it brings high accuracy for molecular energies.However, there are many newer advances that improve upon\n",
      "CCSD(T).107,110 Note that just because a method has a\n",
      "reputation for being accurate does not mean that it will be for\n",
      "all systems. For example, consider again the benzene molecule,which is best illustrated having dotted resonance bond\n",
      "depicting a planar molecule with equal C−C bond lengths.\n",
      "Such a geometry will not be found to be stable with many\n",
      "diﬀerent CompChem methods, in part because of subtlechemical bonding interactions or errors that arise from speciﬁc\n",
      "choices of basis sets used with diﬀerent levels of theory.111,112\n",
      "A key point to reiterate is that correlated wavefunction\n",
      "methods are founded on the HF theory, and so they are evenmore computationally demanding than HF calculations, for\n",
      "example, n()56 for MP2, n()66 for CCSD and CISD and\n",
      "n()76 for CCSD(T). However, this computational expense is\n",
      "Chemical Reviews pubs.acs.org/CR ReviewChemical Reviews pubs.acs.org/CR Review\n",
      "https://doi.org/10.1021/acs.chemrev.1c00107\n",
      "Chem. Rev. 2021, 121, 9816−9872\n",
      "9824alleviated by continually improving computing resources (e.g.,\n",
      "the usability of graphics processing units (GPUs))113−116 and\n",
      "the development of eﬃciency enhancing algorithms, such as\n",
      "pseudospectral methods, 117−119 resolution of the identity(RI),120 domain-based local pair natural orbital methods\n",
      "(DLPNO),121 and explicitly correlated R12/F12 methods.122\n",
      "There are also ongoing eﬀorts to develop other CompChem\n",
      "methods based on quantum Monte Carlo 123 and densitymatrix renormalization group theory (DMRG)124 to provide\n",
      "high accuracy with competitive scaling with other computa-\n",
      "tional methods. Eﬀorts are beginning to become implemented\n",
      "t h a tu s eM Lt oa c c e l e r a t et h e s et y p e so fc a l c u l a -tions.105,106,125−129\n",
      "Schemes have also been developed to exploit systematic\n",
      "errors between diﬀerent levels of theory with diﬀerent basis\n",
      "sets so that approximations can be extrapolated toward anexact result. Examples include the complete basis set (CBS),130\n",
      "Gaussian G n,131 Weizmann (W-) n132 methods, and high\n",
      "accuracy extrapolated ab initio thermochemistry (HEAT)133\n",
      "methods. For a recent review on these and other methods, seeref134. These schemes are also becoming a target of recent\n",
      "work using ML methods.135\n",
      "HF determinants provide good baseline approximations of\n",
      "the ground state electronic structure of many molecules, butthey may describe poorly more complicated bonding that\n",
      "arises during bond dissociation events, excited states, and\n",
      "conical intersections.136−139 Some many-body wavefunctions\n",
      "are best described as a superposition of two or moreconﬁgurations, for example, when other conﬁgurations in eq\n",
      "7 can have similar or higher expansion coeﬃcients a than the\n",
      "HF determinant. For this reason, high quality single reference\n",
      "methods like CCSD(T) fail because the theory assumes thatsalient electronic eﬀects are captured by the initial single HF\n",
      "conﬁguration. (In fact, methods such as CCSD(T) have been\n",
      "implemented with diagnostic approaches available that let\n",
      "users know when there may be cause for concern).140−142 Inthese cases, it may no longer be trivial toﬁnd reliable black-box\n",
      "or automated procedures (e.g., in situations involving\n",
      "resonance states, chemical reactions, molecular excited states,transition metal complexes, and metallic materials, etc.).136 So-\n",
      "called multiconﬁguration approaches,136 such as the general-\n",
      "ized valence bond (GVB) method143 or the complete active\n",
      "space self-consistentﬁeld (CASSCF),144 the multireference CI(MRCI) methods, 145 complete active space perturbation\n",
      "theory (CASPT2), 146 or multireference coupled cluster\n",
      "(MRCC),147,148 can more physically model these systems\n",
      "since they employ several suitable reference con ﬁgurationswith di ﬀerent degrees of correlation treatments. These\n",
      "methods are not black-box and should be expected to require\n",
      "an experienced practitioner with CPI to choose the reference\n",
      "states that can substantially inﬂuence the quality of results.149This is an area though where ML can bring progress in\n",
      "automating the selections of physically justi ﬁed active\n",
      "spaces.129\n",
      "In closing, there are a large number of available correlated\n",
      "wavefunction methods but many are even more costly than HFtheory by virtue of requiring an HF reference energy\n",
      "expression shown ineq 5. Figure 5a depicts a so-called\n",
      "“magic cube” (that is an extension beyond a traditional“Pople\n",
      "diagram”135,150) that concisely shows a full hierarchy ofcomputational approaches across di ﬀerent Hamiltonians,\n",
      "basis sets, and correlation treatment methods. This makes it\n",
      "easy to identify diﬀerent wavefunction methods that should be\n",
      "more accurate and more likely to provide useful atomic scaleinsights (as well as those that would be more computationally\n",
      "intensive). Another important aspect highlighted in the“magic\n",
      "cube” is that higher level wavefunction methods require largerbasis sets to successfully model electron correlation eﬀects. A\n",
      "CCSD(T) computation carried out with a small basis set forcube” is that higher level wavefunction methods require larger\n",
      "\n",
      "Research Paper Length:  50000\n"
     ]
    }
   ],
   "source": [
    "research_paper = research_paper[:100000]\n",
    "print(\"Research Paper: \", research_paper)\n",
    "print(\"Research Paper Length: \", len(research_paper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''\n",
    "You have been provided with a research paper in text format. Your task is to generate a mindmap structure in markdown format that summarizes the research paper.\n",
    "Your output should use the language \\\"en\\\" 0.3 times the length of the original research paper. Do not include anything in the response, that is not the part of mindmap and use the following template (any node in the mindmap should not exceed 10-12 words, also generate additional headings that aren't present in document if required for elaborative explaination):\n",
    "    # {{Title}} (should be the title of the research paper)\n",
    "    ## {{Subtitle01}} (as required and as many as required in markdown format)\n",
    "    - {{Emoji01}} Bulletpoint01 (as required and as many as required in markdown format)\n",
    "        - {{Emoji01.1}} Bulletpoint01.1 (as required and as many as sub levels required in markdown format)\n",
    "            - {{Emoji01.1.1}} Bulletpoint01.1.1 (as required and as many as sub levels required in markdown format)\n",
    "            - {{Emoji01.1.2}} Bulletpoint01.1.2 (as required and as many as sub levels required in markdown format)\n",
    "        - {{Emoji01.2}} Bulletpoint01.2 (as required and as many as sub levels required in markdown format)\n",
    "    - {{Emoji02}} Bulletpoint02 (as required and as many as required in markdown format)\n",
    "        - {{Emoji02.1}} Bulletpoint02.1 (as required and as many as sub levels required in markdown format)\n",
    "        - {{Emoji02.2}} Bulletpoint02.2 (as required and as many as sub levels required in markdown format)\n",
    "            - {{Emoji02.2.1}} Bulletpoint02.2.1 (as required and as many as sub levels required in markdown format)\n",
    "            - {{Emoji02.2.2}} Bulletpoint02.2.2 (as required and as many as sub levels required in markdown format)\n",
    "            - {{Emoji02.2.3}} Bulletpoint02.2.3 (as required and as many as sub levels required in markdown format)\n",
    "            - {{Emoji02.2.4}} Bulletpoint02.2.4 (as required and as many as sub levels required in markdown format)\n",
    "    ## {{Subtitle02}} (as required and as many as required in markdown format)\n",
    "    - {{Emoji03}} Bulletpoint03 (as required and as many as required in markdown format)\n",
    "        - {{Emoji03.1}} Bulletpoint03.1 (as required and as many as sub levels required in markdown format)\n",
    "        - {{Emoji03.2}} Bulletpoint03.2 (as required and as many as sub levels required in markdown format)\n",
    "            - {{Emoji03.2.1}} Bulletpoint03.2.1 (as required and as many as sub levels required in markdown format)\n",
    "            - {{Emoji03.2.2}} Bulletpoint03.2.2 (as required and as many as sub levels required in markdown format)\n",
    "    - {{Emoji04}} Bulletpoint04 (as required and as many as required in markdown format)\n",
    "        - {{Emoji04.1}} Bulletpoint04.1 (as required and as many as sub levels required in markdown format)\n",
    "            - {{Emoji04.1.1}} Bulletpoint04.1.1 (as required and as many as sub levels required in markdown format)\n",
    "            - {{Emoji04.1.2}} Bulletpoint04.1.2 (as required and as many as sub levels required in markdown format)\n",
    "        - {{Emoji04.2}} Bulletpoint04.2 (as required and as many as sub levels required in markdown format)\n",
    "            - {{Emoji04.2.1}} Bulletpoint04.2.1 (as required and as many as sub levels required in markdown format)\n",
    "            - {{Emoji04.2.2}} Bulletpoint04.2.2 (as required and as many as sub levels required in markdown format)\n",
    "    Summarize the text \\\"{research_paper}\\\" to generate a elaborated hierarchical mindmap structure (any node in the mindmap should not exceed 10-12 words, also generate additional headings that aren't present in document if required for elaborative explaination) markdown using the \\\"en\\\" language 0.3 times the length of the original research paper. Do not include anything in the response, that is not the part of mindmap\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''\n",
    "As a text script expert, please help me to write a short text script with the topic \\\\\"{research_paper}\\\\\".Your output should only and very strictly use the following template:\\\\n# {{Title}}\\\\n## {{Subtitle01}}\\\\n- {{Emoji01}} Bulletpoint01\\\\n- {{Emoji02}} Bulletpoint02\\\\n## {{Subtitle02}}\\\\n- {{Emoji03}} Bulletpoint03\\\\n- {{Emoji04}} Bulletpoint04\\\\n\\\\nSummarize the giving topic to generate a mind map (as many subtitles as possible, with a minimum of three subtitles, any node in the mindmap should not exceed 10-12 words) structure markdown.\\\\n Do not include anything in the response, that is not the part of mindmap.\\\\n  Importantly your output must use language \\\\\"English\\\\\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nAs a text script expert, please help me to write a short text script with the topic \\\"Combining Machine Learning and Computational Chemistry for\\nPredictive Insights Into Chemical Systems\\nJohn A. Keith,* Valentin Vassilev-Galindo, Bingqing Cheng, Stefan Chmiela, Michael Gastegger,\\nKlaus-Robert Müller,* and Alexandre Tkatchenko*Klaus-Robert Müller,* and Alexandre Tkatchenko*\\nCite This: Chem. Rev. 2021, 121, 9816−9872 Read Online\\nACCESS Metrics & More Article Recommendations\\nABSTRACT: Machine learning models are poised to make a transformative impact onchemical sciences by dramatically accelerating computational algorithms and amplifying\\ninsights available from computational chemistry methods. However, achieving this requires aconﬂuence and coaction of expertise in computer science and physical sciences. This Review\\nis written for new and experienced researchers working at the intersection of bothﬁelds. Weﬁrst provide concise tutorials of computational chemistry and machine learning methods,\\nshowing how insights involving both can be achieved. We follow with a critical review ofnoteworthy applications that demonstrate how computational chemistry and machine\\nlearning can be used together to provide insightful (and useful) predictions in molecular and\\nmaterials modeling, retrosyntheses, catalysis, and drug design.\\nCONTENTSCONTENTS\\n1. Introduction 9817\\n1.1. Background 9817\\n1.2. Motivation for This Review 9817\\n2. CompChem and Notable Intersections with ML 9820\\n2.1. Computational Modeling, Data, and Infor-\\nmation Across Many Scales 9820mation Across Many Scales 9820\\n2.1.1. Models and Levels of Abstraction 9820\\n2.1.2. CompChem Representations 9820\\n2.1.3. Method Accuracy 9821\\n2.1.4. Precision and Reproducibility 9821\\n2.2. Hierarchies of Methods 98212.2. Hierarchies of Methods 9821\\n2.2.1. Wavefunction Theory Methods 9823\\n2.2.2. Correlated Wavefunction Methods 9824\\n2.2.3. Density Functional Theory 9825\\n2.2.4. Semiempirical Methods 9826\\n2.2.5. Nuclear Quantum Eﬀects 98272.2.5. Nuclear Quantum Eﬀects 9827\\n2.2.6. Interatomic Potentials 9827\\n2.3. Response Properties 9829\\n2.4. Solvation Models 9829\\n2.5. Insightful Predictions for Molecular and\\nMaterial Properties 9830\\n3. Machine Learning Tutorial and Intersections3. Machine Learning Tutorial and Intersections\\nwith Chemistry 9830\\n3.1. What is ML? 9831\\n3.1.1. What Does ML Do Well? 9832\\n3.1.2. What Does ML Do Poorly? 9832\\n3.2. Types of Learning 9833\\n3.2.1. Supervised Learning 98333.2.1. Supervised Learning 9833\\n3.2.2. Unsupervised Learning 9833\\n3.2.3. Reinforcement Learning 9833\\n3.3. Universal Approximators 9834\\n3.4. ML Workﬂow 9834\\n3.4.1. Data Sets 9834\\n3.4.2. Descriptors 9835\\n3.4.3. Training 98353.4.2. Descriptors 9835\\n3.4.3. Training 9835\\n4. Applications of Machine Learning to Chemical\\nSystems 9836\\n4.1. Representing Chemical Systems 9837\\n4.1.1. Descriptors 9837\\n4.1.2. Representing Local Environments 9837\\n4.1.3. Locality Approximation 98394.1.3. Locality Approximation 9839\\n4.1.4. Advantages of Built-In Symmetries 9839\\n4.1.5. End-to-End NN Representations 9840\\n4.2. From Descriptors to Predictions 9840\\n4.3. CompChem Data 9842\\n4.3.1. Benchmark Data Sets 98424.3.1. Benchmark Data Sets 9842\\n4.3.2. Visualization of Data Sets 9842\\n4.3.3. Text and Data Mining for Chemistry 9843\\n4.4. Transforming Atomistic Modeling 9844\\n4.4.1. Predicting Thermodynamic Properties 9844\\n4.4.2. Nuclear Quantum Eﬀects 98444.4.2. Nuclear Quantum Eﬀects 9844\\nSpecial Issue: Machine Learning at the Atomic Scale\\nReceived: February 4, 2021\\nPublished: July 7, 2021\\nReviewpubs.acs.org/CR\\n© 2021 The Authors. Published by\\nAmerican Chemical Society\\n9816American Chemical Society\\n9816\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−98724.5. ML for Structure Search, Sampling, and\\nGeneration 9844\\n4.6. Multiscale Modeling 9845\\n5. Selected Applications and Paths toward Insights 9845\\n5.1. Molecular and Material Design 9845\\n5.2. Retrosynthetic Technologies 9846\\n5.3. Catalysis 98475.3. Catalysis 9847\\n5.4. Drug Design 9850\\n6. Conclusions and Outlook 9851\\nAuthor Information 9853\\nCorresponding Authors 9853\\nAuthors 9853\\nNotes 9853\\nBiographies 9853\\nAcknowledgments 9854\\nAcronyms 9854\\nReferences 9855\\n1. INTRODUCTION\\n1.1. BackgroundReferences 9855\\n1. INTRODUCTION\\n1.1. Background\\nA lasting challenge in applied physical and chemical sciences\\nhas been to answer the question: how can oneidentify and\\nmake chemical compounds or materials that have optimalproperties for a given purpose? A substantial part of research in\\nphysics, chemistry, and materials science concerns the\\ndiscovery and characterization of novel compounds that can\\nbeneﬁt society, but most advances still are generally attributedto trial-and-error experimentation, and this requires signiﬁcant\\ntime and cost. Current global challenges create greater urgency\\nfor faster, better, and less expensive research and development\\neﬀorts. Computational chemistry (CompChem) methods havesigniﬁcantly improved over time, and they promise paradigm\\nshifts in how compounds are fundamentally understood and\\ndesigned for speciﬁc applications.\\nMachine learning (ML) methods have in the past decadeswitnessed an unprecedented technological evolution enabling a\\nplethora of applications, some of which have become daily\\ncompanions in our lives.1−3 Applications of ML include\\ntechnological ﬁelds, such as web search, translation, naturallanguage processing, self-driving vehicles, control architectures,\\nand in the sciences, for example, medical diagnostics,4−8\\nparticle physics,9 nano sciences,10 bioinformatics,11,12 brain-computer interfaces,13 social media analysis,14 robotics,15,16\\nand team, social, or board games.17−19 These methods have\\nalso become popular for accelerating the discovery and design\\nof new materials, chemicals, and chemical processes.20 At thesame time, we have witnessed hype, criticism, and misunder-\\nstanding about how ML tools are to be used in chemical\\nresearch. From this, we see a need for researchers working at\\nthe intersection of CompChem+ML to more criticallyrecognize the true strengths and weaknesses of each\\ncomponent in any given study. Speciﬁcally, we wanted to\\nreview why and how CompChem+ML can provide useful\\ninsights into the study of molecules and materials.While developing this Review, we polled the scienti ﬁc\\ncommunity with an anonymous online survey that asked for\\nquestions and concerns regarding the use of ML models with\\nchemistry applications. Respondents raised excellent points\\nincluding:including:\\n1. ML methods are becoming less understood while they\\nare also more regularly used as black box tools.\\n2. Many publications show inadequate technical expertise\\nin ML (e.g., inappropriate splitting of training, testing,and validation sets).\\n3. It can be diﬃcult to compare diﬀerent ML methods and\\nknow which is the best for a particular application or\\nwhether ML should even be used at all.\\n4. Data quality and context are often missing from MLmodeling, and data sets need to be made freely available\\nand clearly explained.\\nAdditionally, when asked about the most exciting active and\\nemerging areas of ML in the next ﬁve years, respondentsmentioned a wide range of topics from catalysis discovery, drug\\nand peptide design,“above the arrow” reaction predictions,\\nand generative models that promise to fundamentally trans-\\nform chemical discovery. When asked about challenges thatML will not surmount in the nextﬁve years, respondents\\nmentioned modeling complex photochemical and electro-\\nchemical environments, discovering exact exchange-correlation\\nfunctionals, and completely autonomous reaction discovery.This Review will give our perspective on many of these topics.\\nAs context for this Review, Figure 1 shows a heatmap\\ndepicting the frequency of ML keywords found in scientiﬁcThis Review will give our perspective on many of these topics.\\nAs context for this Review, Figure 1 shows a heatmap\\ndepicting the frequency of ML keywords found in scientiﬁc\\narticles that also have keywords associated with di ﬀerentAmerican Chemical Society (ACS) technical divisions.\\nPreparing thisﬁgure required several steps. First, lists of ML\\nkeywords were chosen. Second, lists of keywords were created\\nby perusing ACS division symposia titles from over the pastﬁve years. Third, Python scripts used Scopus Application\\nProgramming Interfaces (APIs) to identify the number of\\nscientiﬁc publications that matched sets of ML and division\\nsymposia keywords. Figure 1 elucidates several interestingpoints. First, the most popular ML approaches across all\\ndivisions are clearly neural networks, followed by genetic\\nalgorithms and support vector machines/kernel methods.\\nSecond, divisions such as physical (PHYS), analytical(ANYL), and environmental (ENVR) are already using diverse\\nsets of ML approaches, while divisions such as inorganic\\n(INOR), nuclear (NUCL), and carbohydrate (CARB) are\\nprimarily employing more distinct subsets of approaches, whileother divisions, such as educational (CHED), history (HIST),\\nlaw (CHAL), and business-oriented divisions (BMGT and\\nSCHB), that is, divisions that produce much fewer scholarly\\njournal articles, are not linking to publications that mentionML. Third, ML has had more prevalence across practically all\\ndivisions over time. For further insight,Table 1 lists the top\\nfour keywords obtained from recent ACS symposium titles, as\\nwell as their respective contribution percentage reﬂected inFigure 1. There, one sees that a handful of keywords can\\nsigniﬁcantly overshadow matches in some of the bins, for\\nexampled, “electro”, “sensor”, “protein”, and“plastic”. With any\\nML application, there will be a risk of imperfect data or userbias, but this is a useful launch point to appreciate how and\\nwhere ML is being used in chemical sciences. A key takeaway\\nis that we are witnessing an unprecedented crescendo in\\ninterest in ML over the last ten years (e.g.,Figure 1c) thanks toimproved understanding of the intersectionality of traditional\\nscience and engineering disciplines with rapidly evolving\\ndisciplines such as CompChem and data science.\\n1.2. Motivation for This Review1.2. Motivation for This Review\\nThe survey results and literature analysis above showed an\\nopportunity for a tutorial reference to help readers address\\nfuture research challenges that will require joint applications ofChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9817Figure 1. Heatmaps illustrating the extent that ML terms appear in scientiﬁc papers aligned by American Chemical Society (ACS) technicaldivisions from 2000 to 2010 (a) and from 2010−present (b). (c) Line graph showing the number of occurrences of any ML term being found inpapers attributed to the ACS PHYS division, from 2000−present. Figures were made by Charles D. Griego. Python scripts used to generate theseﬁgures and correspondingTable 1 are freely available with a creative commons attribution license. Readers are welcome to use, adapt, and sharethese scripts with appropriate attribution:https://github.com/keithgroup/scopus_searching_ML_in_chem_literature.).\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9818CompChem, ML, and chemical and physical intuition (CPI).\\nThis review will classify concepts using a rendition of a“data to\\nwisdom” hierarchy, Figure 2 . Scholars have noted short-\\ncomings with similar constructs,21 but we use it to reﬂect astepladder for scientiﬁc progress, starting from collecting data\\nand ending with overall impact. CompChem, ML, and CPI\\neach have di ﬀerent strengths and weaknesses and bring\\nsynergistic opportunities. CPI alone can be employed toclimb the ladder from data to impact, but current CPI may\\nonly provide limited understanding or applicability outside of\\navailable data sets. However, CompChem is extraordinarily\\nwell-suited for generating high quality data that contain usefulinformation (vide infra,section 2) often more easily than via\\ntraditional experimentation. ML is likewise extremely well-\\nsuited for recognizing and accurately quantifying nonlinear\\nrelationships (vide infra, section 3), a task that is especiallydiﬃcult for even the most expert-level CPI alone. A key\\nopportunity is that useful ML requires robust data sets, and\\nthese can be provided by CompChem as long as the CPI\\ncomponent is selecting and correctly interpreting appropriatemethods for the task at hand to productively climb the ladder\\ntoward impact (vide infra,section 4). We stress that the impact\\ngeneration process shown inFigure 2 is by no means a linear\\none  on the contrary, it contains many loops and dead ends.Figure 2. Data−knowledge−wisdom hierarchy stepladder.\\nTable 1. List of Top Ranked Keywords (Per ACS Division) with Corresponding Percentage of Matches for Any ML Term\\ndivision rank 1 rank 2 rank 3 rank 4division rank 1 rank 2 rank 3 rank 4\\nPHYS electro * (56.3%) spectroscopy (9.7%) ion * (6.0%) nano * (5.5%)\\nANYL sensor * (55.4%) spectroscopy (13.1%) characterization * (11.6%) spectrometry (4.3%)ENVR *sensor* (60.9%) soil * (14.5%) water quality (4.2%) environmental monitor * (3.0%)\\nAGFD protein * (31.9%) agricultur * (18.2%) food (10.8%) fruit * (5.7%)\\nENFL fuel * (19.2%) petroleum (11.6%) energy e ﬃciency (11.1%) batter * (10.7%)AGRO soil (43.3%) crop * (25.4%) groundwater (11.5%) developing countr * (4.4%)\\nORGN protein * (64.6%) amino acid * (19.7%) peptide * (8.4%) aromatic * (3.2%)\\nPOLY plastic * (51.1%) polymer * (37.7%) polymeriz * (5.0%) polymeric (2.8%)PMSE *polymer* (50.4%) *peptide* (30.4%) thin ﬁlm* (8.9%) tissue engineering (4.3%)\\nBIOT biochemi * (37.9%) biophysic * (18.3%) systems biology (10.3%) biotechnology (9.9%)GEOC groundwater (33.4%) mining (31.6%) *geochem* (12.4%) anthropogenic (10.9%)\\nMEDI protein interaction * (25.7%) drug discovery (19.1%) drug design (19.1%) antibiotic * (11.3%)COMP drug discovery (18.7%) drug design (18.6%) molecular model * (14.3%) protein database * (13.1%)\\nCOLL nanoparticle * (21.2%) adsorption (19.6%) thin ﬁlm* (14.9%) tribolog * (9.6%)BIOL drug discovery (41.9%) protein folding (19.3%) biosynthesis (12.2%) cytochrome * (12.2%)\\nTOXI toxi * (99.2%) chemical exposure * (0.6%) antibody drug conjugate * (0.1%)CATL cataly * (64.0%) metal oxides (20.2%) photocataly * (5.3%) surface chemistry (2.8%)\\nCINF drug discovery (51.7%) computational chemistry\\n(17.5%)\\nbio* modeling (7.8%) chem * database* (7.6%)bio* modeling (7.8%) chem * database* (7.6%)\\nINOR electrochem * (67.1%) nanomaterial * (14.9%) organometallic * (5.8%) metal organic framework * (4.0%)\\nNUCL nuclear fuel * (28.6%) isotope * (27.3%) radioisotope * (15.0%) nuclear medicine * (9.0%)CARB carbohydrate * (43.0%) glycoprotein * (42.4%) glycan * (6.6%) oligosaccharide * (5.6%)\\nRUBB rubber * (100.0%)\\nCELL cellulose (41.7%) polysaccharide * (26.3%) lignin (16.3%) lignocellulos * (9.7%)I&EC water puri ﬁcation (38.1%) industrial chem * (23.7%) rare earth element * (11.9%) industrial and engineering chemistry\\n(8.3%)\\nFLUO ﬂuorine* (99.8%) radiopharmaceutical chem *\\n(0.2%)(0.2%)\\nCHED chem * class* (76.4%) chem * communication* (8.5%) chem * educat* (5.5%) lab * safety (5.5%)\\nCHAS chem * safety (51.5%) lab * safety (16.7%) environmental health and safety\\n(16.7%)\\nchem* regulations (15.2%)CHAS chem * safety (51.5%) lab * safety (16.7%) environmental health and safety\\n(16.7%)\\nchem* regulations (15.2%)\\nBMGT chem * compan* (65.4%) chem * enterprise* (28.8%) chem * business* (3.8%) chem * research and development (1.9%)SCHB commercial chem * (50.0%) chem * sector* (28.6%) academic entrepreneur * (14.3%) science advoca * (7.1%)\\nHIST chem * histor* (53.8%) evolution of chem * (30.8%) history of chem * (15.4%)\\nPROF chem * education (100.0%)PROF chem * education (100.0%)\\nCHAL pharmaceutical patent *\\n(60.0%)\\nchem* in commerce (30.0%) chem * patent* (10.0%)\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9819As we show later (in Section 4 ), within the troika of\\nCompChem+ML+CPI, ML acts as a catalyst that accelerates\\nexplorative data-driven hypotheses generation. Automatically\\ngenerated hypotheses are then validated and calibrated withCompChem and CPI to yield further improved ML modeling\\n(enriched by more physical prior knowledge), which then\\nloops back with improved hypotheses. This feedback loop is\\nthe key to the modern knowledge discovery leading to insight,wisdom and hopefully positive impacts to society.\\n2. COMPCHEM AND NOTABLE INTERSECTIONS WITH\\nML\\n2.1. Computational Modeling, Data, and Information\\nAcross Many Scales\\nWe consider quantum mechanics as described by theWe consider quantum mechanics as described by the\\nnonrelativistic time-independent Schrödinger equation as our\\n“standard model” because it accurately represents the physics\\nof charged particles (electrons and nuclei) that make up almostall molecules and materials. Indeed, this opinion has been held\\nby some for almost a century:\\nThe fundamental laws necessary for the mathematical\\ntreatment of a large part of physics and the whole ofchemistry are thus completely known, and the diﬃculty lies\\nonly in the fact that application of these laws leads to\\nequations that are too complex to be solved.\\nP. A. M. Dirac, 1929\\nAny theoretical method for predicting molecular or materialphenomena mustﬁrst be rooted in quantum mechanics theory\\nand then suitably coarse-grained and approximated so that it\\ncan be applied in a practical setting. CompChem, or more\\nprecisely, computational quantum chemistry deﬁnes computa-tionally driven numerical analyses based on quantum\\nmechanics. In this section, we will explain how and why\\ndiﬀerent CompChem methods capture di ﬀerent aspects of\\nunderlying physics. Speciﬁcally, this section provides a conciseoverview of the broad range of CompChem methods that are\\navailable for generating data sets that would be useful for ML-\\nassisted studies of molecules and materials.\\n2.1.1. Models and Levels of Abstraction. Models2.1.1. Models and Levels of Abstraction. Models\\nextract information from data. The renowned statistician\\nGeorge Box famously discussed“good models ” as those\\ncharacterized as “simple”, “illuminating”, and “useful”.22 Goodmodels should beparsimonious and describe essential relation-\\nships without overelaboration. The ideal gas equation,PV =\\nnRT, exempliﬁes a good model. The ideal gas equation relates\\nmacroscopic pressure (P), volume (V), number of molecules(n), and temperature (T) of gases under idealized conditions,\\nwithout requiring explicit knowledge of the processes occurring\\non an atomic scale. Its simple functional form needs just oneparameter, the ideal gas constantR, and this makes it possible\\nto formulate useful insights, such as how at constant pressure a\\ngas expands with rising temperature. On the other hand, this\\nelegant equation only holds for conditions where the gasbehaves as an ideal gas. The derivation of more accurate\\nmodels of gases requires more mathematically complicated\\nequations of state that rely on more free parameters23 that in\\nturn obfuscate physical insights, require more computationaleﬀort to solve, and thus make the model less “good”. This\\nexample also oﬀers a convenient connection to ML models\\nthat will be discussed later in section 3. As mathematical\\nmodels for complex phenomena become more complicatedand less intuitive to derive, ML models that infer nonlinear\\nrelationships from data become more applicable when\\nincreasing amounts of empirical data become available.\\nAlternatively, the conventional CompChem treatmententails ﬁrst determining the system’s relevant geometry and\\nits total ground state energy, and from that physical properties\\nof interest (e.g., pressure, volume, band gap, polarizability, etc.)can be obtained using quantum and statistical mechanics. In\\nthis section, we discuss the relevant CompChem methods for\\nthese. While the mathematical physics for these methods might\\noccasionally be too complicated for a user to fully understand,this section, we discuss the relevant CompChem methods for\\nthese. While the mathematical physics for these methods might\\noccasionally be too complicated for a user to fully understand,\\nmany algorithms exist so that they can still be easily run in a“black-box ” way with modern computational chemistry\\nsoftware and accompanying tutorials.24−27 CompChem thus\\nserves as an invaluable tool to generate data and information\\nfor knowledge and insights across many length and time scales.Figure 3 is an adaption of a multiscale hierarchy of diﬀerent\\nclasses of CompChem methods. It shows their applicability for\\nmodeling diﬀerent length and time scales and depicts how\\nlarge scale models may be developed based on smaller scale\\ntheories.theories.\\n2.1.2. CompChem Representations. Integral to every\\nCompChem study is the user’s representation for the system,\\nthat is, how the user chooses to describe the system.\\nCompChem representations can range from simple and lucid(e.g., a precise chemical system such as a water molecule\\nisolated in a vacuum) to complex and ambiguous (e.g., a\\nputative but speculative depiction of a solid−liquid interface\\nunder electrochemical conditions). Approximate wavefunc-tions (expressed on a basis set of mathematical functions) or\\napproximate Hamiltonians (referred to as levels of theory) as\\ndescribed below in this section can also be considered\\nrepresentations. One might then say that many representationsfor diﬀerent components of a system will constitute an overall\\nrepresentation, and this is true. The point we make is that the\\nvalidity ofany computational result depends on the overall\\nrepresentation, and sometimes an incorrect representation mayprovide a correct result due to“fortuitous error cancellation”.\\nIn CompChem studies, a valid representation is one that\\ncaptures the nature of the physical phenomena of a system. For\\na molecular example, if one is determining the bond energy ofa large biodiesel molecule using CompChem methods,28 it\\nmay or may not be justiﬁed to approximate a nearby long-chain\\nFigure 3. Hierarchy of computational methods and corresponding\\ntime and length scales. QM stands for Quantum Mechanics.Chemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9820alkyl group (−CnH(2n+1)) simply as a methyl (−CH3) or even a\\nhydrogen atom. Indeed, choosing such a representation can\\nsometimes be a useful example of CPI since alkyl bonds usually\\nexhibit relatively short-ranged interactions (a feature that willbe discussed in the context of ML in more detail insection\\n4.1.3.). An atomic scale geometry with fewer atoms would\\nreduce the computational cost of the study or allow a more\\naccurate but more computationally expensive calculation to berun. On the other hand, it might also be a poor choice if the\\nchemical group, for example, a substituted alkyl group\\nparticipated in physical organic interactions, such as subtle\\nsteric, induction, or resonance eﬀects.29 For a solid-stateexample, a user might exercise good CPI by assuming that a\\nrelatively small unit cell under periodic boundary conditions\\nwould capture salient features of a bulk material or a material\\nsurface (as is often the case for many metals). On the otherhand, subtle symmetry-breaking eﬀects in materials (e.g.,\\ndistortions arising from tilting octahedra groups in perov-\\nskites,30 or surface reconstruction phenomena that occur on\\nsingle crystals)31 might only be observed when consideringlarger and more computationally expensive unit cells. Relevant\\nto both examples, it may also be that the CompChem method\\nitself brings errors that obfuscate phenomena that the user\\nintends to model. In general, CompChem errors may be due to1) errors introduced by the user in the initial set up of the\\nCompChem application, or 2) errors in the CompChem\\nmethod when treating the physics of the system. Insection 3,\\nwe will discuss how the choice of ML representation also playssimilarly critical roles in determining whether and to what\\nextent an ML model is useful.\\n2.1.3. Method Accuracy.The quantitative accuracy of a\\nCompChem model stems from its suitability in describing thesystem. As explained above, an observed accuracy will depend\\non the representation being used. High-quality CompChem\\ncalculations have traditionally been benchmarked against data\\nsets that consist of well-controlled and relatively precisethermochemistry experiments on small, isolated molecules.32,33\\nThe error bars for standard calorimetry experiments are\\napproximately 4 kJ/mol (or 1 kcal/mol or 0.04 eV), and\\ncomputational methods that can provide greater accuracy thanthis are stated as achieving“chemical accuracy”. Note that this\\nterm should be used when describing the accuracy of the\\nmethod compared to the most accurate data possible; for\\nexample, if one CompChem method was found to reproduceanother CompChem method within 1 kJ/mol, but both\\nmethods reproduce experimental data with errors of 20 kJ/\\nmol, then neither method should be called chemically accurate.\\nThere are many well-established reasons why CompChemmodels can bring errors. For example, errors may be due to\\nsize consistency34 or size extensivity 35 problems that are\\nintrinsic within the CompChem method, larger systems\\nsometimes embody signiﬁcant medium and long-rangesometimes embody signiﬁcant medium and long-range\\ninteractions (e.g., van der Waals forces)36 or self-interaction\\nerrors37 that might not be noticeable in small test cases. The\\nrecommended path forward is to consider which fundamentalinteractions are in play in the system and then use a\\nCompChem model that is adequate at describing those\\ninteractions. Besides this, users should make use of existing\\ntutorial references that provide practical knowledge aboutwhich parameters in a CompChem calculation should be\\ncarefully noted, for example ref38. Historically the most\\npopular CompChem methods for molecular and materials\\nmodeling (the B3LYP39 and PBE 40 exchange correlationfunctionals, see section 2.2.3.) are often said to have an\\nexpected accuracy of about 10−15 kJ/mol (or 2−4 kcal/mol\\nor 0.1−0.2 eV) when modeling diﬀerences between the total\\nenergies of two similar systems, and errors are expected to besomewhat larger when considering transition state energies.or 0.1−0.2 eV) when modeling diﬀerences between the total\\nenergies of two similar systems, and errors are expected to be\\nsomewhat larger when considering transition state energies.\\nThough this is used as a simple rule, it is obviously anoversimpliﬁcation and actual accuracy is only assessed by\\nthoughtful benchmarking of the case being considered.41−45\\n2.1.4. Precision and Reproducibility. In CompChem,\\none normally assumes that any two users using the samerepresentation for the system with the same code on the same\\ncomputing architecture will obtain the exact same result within\\nthe numerical precision of the computers being used. This is\\nnot always the case, especially for molecular dynamics (MD)simulations that often rely on stochastic methods.46 Computa-\\ntional precision also becomes more concerning when there are\\ndiﬀerent versions of codes in circulation, errors that might arisefrom diﬀerent compilers and libraries, and a lack of consensus\\nin the community about which computational methods and\\nwhich default settings should be used for speciﬁc application\\nsystems, for example, grid density selections,47 or standardkeywords for molecular dynamics simulations.46,48 There have\\nbeen eﬀorts to conﬁrm that diﬀerent codes can reproduce\\nenergies for the same system representation,48,49 but some\\ncommercial codes hold proprietary licenses that restrictpublications that critically benchmark calculation accuracy\\nand timings across diﬀerent codes. A path forward to beneﬁt\\nthe advancement of insight is the development of (open)\\nsource codes50 that perform as well if not better thancommercial codes. While increased access to computational\\nalgorithms is beneﬁcial, it also raises the need for enforcing\\nhigh standards of quality and reproducibility.51,52 We are also\\nglad to see active developments to more lucidly show how anyset of computational data is generated, precisely with which\\ncodes, keywords, and auxiliary scripts and routines.53−56 We\\nare now in an era where truly massive amounts of data and\\ninformation can be generated for CompChem+ML eﬀorts. Togo forward, one needs to know what constitutes good and\\nuseful data, and the next section provides an overview of how\\nto do this using CompChem.\\n2.2. Hierarchies of Methods\\nEarlier we mentioned that a usual task in CompChem is tocalculate the ground state energy of an atomic scale system.\\nIndeed, CompChem methods can determine the energy for a\\nhypothetical conﬁguration of atoms, and this constitutes the\\npotential energy surface (PES) of the system (Figure 4). ThePES is a hypersurface spanning 3N dimensions, whereN is the\\nnumber of atoms in the system. Since the PES is used to\\nanalyze chemical bonding between atoms within the system,\\nthe PES can also be simpliﬁed by ignoring translational androtational degrees of freedom for the entire system. This\\nreduces the dimensionality of the PES from 3N to 3N − 5 for\\nlinear systems (e.g., diatomic molecules or perfectly linear\\nmolecules such as acetylene) or 3N − 6 for all other nonlinearsystems. Furthermore, since visualization is diﬃcult beyond\\nthree dimensions, PES drawings will show a 1-D or 2-D\\nprojection of this hypersurface where thez-axis is convention-\\nally used to represent the scale for system energy.Any arbitrary PES will contain several interesting features.\\nMinima on the PES correspond to mechanically stable\\nconﬁgurations of a molecule or material, for example reactant\\nand product states of a chemical reaction or di ﬀerentChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9821conformational isomers of a molecule. Because they are\\nminima, the second derivative of the energy given by the PES\\nwith respect to any dimension will be positive. Minima can also\\nbe connected by pathways, which indicate chemical trans-formations (Figure 4, red line). Along such pathways, the\\nsecond derivative can be positive, zero, or negative, but all\\nother second derivatives must be positive. Transition states are\\nﬁrst-order saddlepoints and thus represent a maximum in onecoordinate and a minimum along all others. They correspond\\nto the lowest energy barriers connecting two minima on the\\nPES and are hence important for characterizing transitions\\nbetween PES minima (e.g., chemical reactions). Second-ordersaddle points57 and bifurcating pathways58 can also exist, but\\nthese are not discussed further here.\\nA wide range of higher-level properties of the system can be\\npredicted or derived using the PES, including predictedthermodynamic binding constants, kinetic rate constants for\\nreactions, or properties based on dynamics of the system. The\\ntask is then to choose an appropriate CompChem method that\\ncan carry out energy and gradient calculations on the system’sPES. Figure 5 shows several di ﬀerent hierarchies for\\nCompChem methods capable of doing this. Note that all of\\nthese methods mentioned in thisﬁgure fall in the categories of\\nthe bottom two regions in the multiscale hierarchyFigure 3.All of these methods in principle could be used to develop\\nFigure 4.Potential energy surface (PES) of aﬁctional system with the\\ntwo coordinates R1 and R2. The minima of the PES correspond tostable states of a system, such as equilibrium con ﬁgurations and\\nreactants or products. Minima can be connected by paths (red line),\\nalong which rearrangements and reactions can occur. The maximumalong such a path is called a transition state. Transition states areﬁrst-\\norder saddle points, a maximum in one coordinate and minima in all\\nothers. They correspond to the minimum energy required totransition between two PES minima and play a crucial role in the\\ndescription of chemical transformations.\\nFigure 5. (a) “Magic cube”59 depiction of hierarchies of correlated wavefunction approaches. (b)“Jacob’s Ladder”60 depiction of hierarchies ofKohn−Sham density functional theory (DFT) approaches. (c) Hierarchies of atomistic potentials. (d) Overall hierarchies in predictive atomic scale\\nmodeling methods.\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107https://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9822coarse-grained or continuum models as well. Also note that\\nmethods in Figure 5 will bring very diﬀerent computational\\ncosts and opportunities for methods involving ML.\\n2.2.1. Wavefunction Theory Methods. In standard2.2.1. Wavefunction Theory Methods. In standard\\ncomputational quantum chemistry, a system’s energy can be\\ncomputed in terms of the Schro ̈dinger equation.61−63 The\\nwavefunction that will be used to represent the positions ofelectrons and nuclei in the system (Ψ(r, R)) is hard to intuit\\nsince it can be complex valued. However, its square describes\\nthe real probability density of the nuclear (R) and electronicpositions (r). In a real system, the position and interactions of\\na single particle in the system with respect to all other particles\\nwill be correlated , and this makes exactly solving the\\nSchrödinger equation impossible for almost all systems ofpractical interest. To make the problem more tractable, one\\nmay exploit the Born−Oppenheimer approximation;64 since\\nnuclei are expected to move much slower than the electrons\\nthey can be approximated as stationary at any point along thePES. This allows the energy to be calculated using the time-\\nindependent Schrödinger equation and solving the eigenvalue\\nproblem:\\nH TV E()̂Ψ= ̂+ ̂ Ψ= Ψ (1)\\nHere, the Hamiltonian operator ( Ĥ) is the sum of thekinetic (T̂) and potential (V̂) operators,Ψ is the wavefunction\\n(i.e., an eigenfunction) that represents particles in the system,\\nandE is the energy (i.e., an eigenvalue). In this way, nuclei canbe treated as ﬁxed point charges, and then, eq 1 can be\\ntransformed into the so-called electronic Schrödinger equation,\\nwhere the Hamiltonian Ĥel and wavefunction Ψel(r; R) now\\nonly depend on the nuclear coordinates R in a parametric\\nfashion:fashion:\\nH TV V V\\nE\\nrR r rR R r\\nrR\\nrR\\n(; ) () (; ) ( ) ()\\n(; )\\n(; )\\nel el e eN NN ee\\nel\\nel el\\n̂ Ψ= [ ̂ + ̂ + ̂ + ̂ ]\\nΨ\\n=Ψ (2)\\nThe above expression hasĤel composed of single electron (e)\\nand pairwise electron−nuclear (eN), nuclear−nuclear (NN),and electron−electron (ee) terms. Here, we will now implicitly\\nassume the Born −Oppenheimer approximation throughout\\nand leave oﬀ the subscript indicating the electronic problem.\\nHowever, we note that the Born−Oppenheimer approximationis not always suﬃcient and computationally intensive non-\\nadiabatic quantum dynamics may be required.65 In certain\\ncases, semiclassical treatments are appropriate; for example,\\nnonadiabatic eﬀects between electrons and nuclei can beconsidered using nuclear-electronic orbital methods.66\\nA second common approximation is to expand the total\\nelectronic wavefunction in terms of one-electron wave-\\nfunctions (i.e., spin orbitals): ϕ(ri). Electrons are Fermionsand therefore exhibit antisymmetry, which in turn results in the\\nPauli exclusion principle. Antisymmetry means that the\\ninterchange of any two particles within the system should\\nbring an overall sign change to the wavefunction (i.e., from +to−, or vice versa). This property is conveniently captured\\nmathematically by combining one electron spin orbitals into\\nthe form of a Slater determinant:\\nμ\\n∂∏ ∂\\nμ\\nn\\nrr\\nrr\\nrr\\n(, , ) 1\\n() ()\\n() ()\\nn\\nn\\nn n n\\n1\\n1 11\\n1\\nϕϕ\\nϕϕ\\nΨ ··· =\\n!\\n(3)() ()\\nn\\nn\\nn n n\\n1\\n1 11\\n1\\nϕϕ\\nϕϕ\\nΨ ··· =\\n!\\n(3)\\nNote that a determinant’s sign changes whenever two columns\\nor rows are interchanged, and in a Slater determinant this\\ncorresponds to interchanging electrons and thus the physicallyappropriate sign change for the overall wavefunction. Addi-\\ntionally,\\nn\\n1\\n!\\nis a normalizing factor to ensure the wavefunction\\nis unitary.\\nThe spin orbitals can be treated as a mathematical expansionusing a basis set ofμ functions χμ, each having coeﬃcients cμi,\\nwhich are generally Gaussian basis functions,67−69 Slater-type\\nhydrogenic orbitals,70 or plane waves under periodic boundary\\nconditions:71−73\\nci i∑ϕχ=\\nμ\\nμ μ\\n(4)conditions:71−73\\nci i∑ϕχ=\\nμ\\nμ μ\\n(4)\\nThe diﬀerent types of mathematical functions bring diﬀerent\\nstrengths and weaknesses, but these will not be discussed\\nfurther here. A universal point is that larger basis sets will havemore basis functions and thus give a moreﬂexible and physicalstrengths and weaknesses, but these will not be discussed\\nfurther here. A universal point is that larger basis sets will have\\nmore basis functions and thus give a moreﬂexible and physical\\nrepresentation of electrons within the system. On one handthis can be crucial for capturing subtle electronic structure\\neﬀects due to electron correlation. On the other hand, larger\\nbasis sets also necessitate signiﬁcantly higher computational\\neﬀort. A standard technique to avoid high computational eﬀortin electronic structure calculations is to replace nonreacting\\ncore electrons with analytic functions using eﬀective core\\npotentials (ECPs, i.e., pseudopotentials).74−89 This requires\\nreformulating the basis sets that describe the valence space ofthe atoms, for example see refs90 and 91. Larger nuclei that\\nbring higher atomic numbers and larger numbers of electrons\\nwill also exhibit relativistic eﬀects,92 and relativistic Hamil-\\ntonians are based on the Dirac equation 93,94 or quantumelectrodynamics.95 These methods can range from reasonably\\ncost-eﬀective methods96,97 to those bringing extremely high\\ncomputational cost.98 Practical applications have traditionally\\nused standard nonrelativistic Hamiltonian methods, along withECPs (or pseudopotentials) that have been explicitly\\ndeveloped to account for compressed core orbitals that result\\nfrom relativistic eﬀects.\\nUsing the Born −Oppenheimer approximation ( eq 2 )\\ntogether with a Slater determinant wavefunction ( eq 3 )expressed in aﬁnite basis set (eq 4) brings about the simplest\\nwavefunction based method, the Hartree −Fock (HF)\\napproach (for historical context see refs 99−101). The HF\\nmethod is a mean ﬁeld approach, where each electron istreated as if it moves within the averageﬁeld generated by all\\nother electrons. It is generally considered inaccurate when\\ndescribing many chemical systems, but it continues to serve asa critical pillar for CompChem electronic structure calculations\\nsince it either establishes the foundation for all other accurate\\nmethods or provides energy contributions (i.e., exact\\nexchange) that is not provided in some CompChem methods.CompChem methods that achieve accuracy higher than HF\\ntheory are said to containelectron correlation , a critical\\ncomponent for understanding molecules and materials (as\\ndescribed in more detail insection 2.2.2.). Expressing Ψ as aSlater determinant and rearranging eq 2 while temporarily\\nneglecting nuclear−nuclear interactions allows one to deﬁne\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9823the HF energy in terms of integrals of the electronic spin\\norbitals:\\nß\\n´≠ÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖ ÆÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖ\\n´≠ÖÖÖÖÖÖÖÖÖ ÆÖÖÖÖÖÖÖÖÖ\\n´≠ÖÖÖÖÖÖÖÖÖ ÆÖÖÖÖÖÖÖÖÖ\\nE\\nZ\\nrr r\\nrr rR r\\nrr r r rr rr\\nrr r r rr rr\\nd( ) 1\\n2 ()\\nd( ) ( )\\nd d () () 1 ()()\\nd d () () 1 ()()2 ()\\nd( ) ( )\\nd d () () 1 ()()\\nd d () () 1 ()()\\ni\\ni\\nT\\ni\\ni\\ni\\nN\\nV\\ni\\nij i\\nii\\nV\\njj\\nij i\\nij\\nV\\nji\\nHF 1 1\\n2\\n1\\n11\\n1\\n1\\n,\\n12 1 1\\n12\\n(Coulomb)\\n22\\n,\\n12 1 1\\n12\\n(exchange)\\n22\\ne\\neN\\nee\\nee\\n∫\\n∫\\n∬\\n∬\\n∑\\n∑ ∑\\n∑\\n∑\\nϕϕ\\nϕϕ\\nϕϕ ϕϕ\\nϕϕ ϕϕ\\n=− * ∇\\n− *\\n|− |\\n+ *\\n|−|\\n*\\n− *\\n|−|\\n*\\nα\\nα\\nαϕϕ ϕϕ\\n=− * ∇\\n− *\\n|− |\\n+ *\\n|−|\\n*\\n− *\\n|−|\\n*\\nα\\nα\\nα\\n>\\n>\\n̂\\n̂\\n̂\\n̂\\n(5)\\nwhere the ﬁrst two terms are referred to as one-electron\\nintegrals and represent the kinetic energy of the electrons and\\nthe potential energy contributions from electron-nucleiinteractions. The remaining terms are two-electron integrals\\nthat describe the potential energy arising from electron−\\nelectron interactions and are called Coulomb and exchange\\nintegrals. Using Lagrange multipliers, one can express the HFequation in a compact matrix form, the so-called Roothan−\\nHall equations,102−104 which allow for an eﬃcient solution:\\nFCS Cϵ= (6)\\nEach matrix has a size ofμ × μ, whereμ is the number of basisfunctions used to express the orbitals of the system.C is a\\ncoeﬃcient matrix collecting the basis coeﬃcients cμi (see eq 4),\\nwhile S is the overlap matrix measuring the degree of overlapbetween individual basis functions andϵ is a diagonal matrix of\\nthe spin orbital energies. Finally,F is the Fock matrix, with\\nelements of a similar form as ineq 5, but expressed in terms ofbasis functionsχμ. One important detail not readily apparent in\\neq 6is that the Fock matrix depends on the orbital coeﬃcients\\nthat must be provided beforeeq 6can be solved. As such,eq 6\\ncannot be solved in closed form, but instead requires a so-called self-consistentﬁeld approach. Starting from an arbitrary\\nset of trial (i.e., initial guess) functions, one iteratively solves\\nfor optimal molecular orbital coeﬃcients, which are then usedto construct a new Fock matrix, until a minimum energy is\\nreached in accordance with the variational principle of\\nquantum mechanics. Evaluating and transforming the two-\\nelectron integrals ineq 5 are a signiﬁcant bottleneck for thesecalculations and thus the computational e ﬀort of the HF\\nmethods formally scales as () 46 μ with the number of basis\\nfunctions. This means that a calculation on a system twice as\\nlarge will require at least 24 = 16 times as much computingtime. The electronic exchange interaction resulting from the\\nantisymmetry of the wavefunction imposes a strong constraint\\non the mathematical form of ML models for electronic\\nwavefunctions. Construction of eﬃcient and reliable antisym-metric ML models for the many-body wavefunction is an\\nimportant area of current research.105,106\\n2.2.2. Correlated Wavefunction Methods.The system’s\\ncorrelation energy is de ﬁned as sum of electron −electroninteractions that originate beyond the mean-ﬁeld approxima-\\ntion for electron−electron interactions that is provided by HF\\ntheory. While correlation energy makes up a rather small\\ncontribution to the overall energy of a system (usually about1% of the total energy), because internal energies in molecular\\nand material systems are so enormous, this contribution\\nbecomes rather signiﬁcant. As an example, most molecular\\ncrystals would be unstable as solids if calculated using the HFlevel of theory. The missing component is attractive forces that\\nare obtained from levels of theory that account for correlation\\nenergy. Correlation energies are obtained by calculating\\nadditional electron−electron interaction energies that arisefrom diﬀerent arrangements of electron conﬁgurations (i.e.,\\ndiﬀerent possible excited states) that are not treated with the\\nmean ﬁeld approach of HF theory.\\nThe most complete correlation treatment is the fullconﬁguration interaction (FCI) method, which is the exact\\nnumerical solution of the electronic Schrödinger equation (in\\nthe complete basis limit) that considers interactions arising\\nfrom all possible excited conﬁgurations of electrons. The FCInumerical solution of the electronic Schrödinger equation (in\\nthe complete basis limit) that considers interactions arising\\nfrom all possible excited conﬁgurations of electrons. The FCI\\nwavefunction takes the form of a linear combination of allpossible excited Slater determinants which can be generated\\nfrom a single HF reference wavefunction by electron\\nexcitations:\\naa a ...FCI 0 HF\\n,, , ,\\n∑∑Ψ =Ψ + Ψ + Ψ+\\nαβ\\nβ\\nα\\nβ\\nα\\nαβγδ\\nβδ\\nαγ\\nβδ\\nαγ\\n(7)\\nwhere Ψβαβ\\nβ\\nα\\nβ\\nα\\nαβγδ\\nβδ\\nαγ\\nβδ\\nαγ\\n(7)\\nwhere Ψβ\\nα represents the Slater determinant obtained by\\nexciting an electron from orbitalα into an unoccupied orbital\\nβ, and theas are expansion coeﬃcients determining the weightof the diﬀerent contributing conﬁgurations. Expectedly, FCI\\ncalculations scale extremely poorly with the number of\\nelectrons in the system ( n()6 ! ), as the number of possible\\nconﬁgurations grows rapidly, making them feasible only forsmall molecules. For an example of the state of the art, FCI\\ncalculations have been used to benchmark highly accurate\\nmethods on calculations on a benzene molecule.107\\nMost correlated wavefunction methods use a subset of thepossible conﬁgurations ineq 7to be computationally tractable.\\nThe conﬁguration interaction (CI)108 method for example\\nonly includes determinants up to a certain permutation level\\n(e.g., single and double excitations in CISD). Alternatively,MPn35 (e.g., MP2) recovers the correlation energy by applying\\ndiﬀerent orders of perturbation theory. Coupled cluster theory,\\nanother widely used post-HF method, includes additional\\nelectron conﬁgurations via cluster operators.109 One coupledcluster method that involves single, double, and perturbative\\ntriples excitations, CCSD(T), is referred to as the“gold-\\nstandard” approach for CompChem electronic structure\\nmethods since it brings high accuracy for molecular energies.However, there are many newer advances that improve upon\\nCCSD(T).107,110 Note that just because a method has a\\nreputation for being accurate does not mean that it will be for\\nall systems. For example, consider again the benzene molecule,which is best illustrated having dotted resonance bond\\ndepicting a planar molecule with equal C−C bond lengths.\\nSuch a geometry will not be found to be stable with many\\ndiﬀerent CompChem methods, in part because of subtlechemical bonding interactions or errors that arise from speciﬁc\\nchoices of basis sets used with diﬀerent levels of theory.111,112\\nA key point to reiterate is that correlated wavefunction\\nmethods are founded on the HF theory, and so they are evenmore computationally demanding than HF calculations, for\\nexample, n()56 for MP2, n()66 for CCSD and CISD and\\nn()76 for CCSD(T). However, this computational expense is\\nChemical Reviews pubs.acs.org/CR ReviewChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9824alleviated by continually improving computing resources (e.g.,\\nthe usability of graphics processing units (GPUs))113−116 and\\nthe development of eﬃciency enhancing algorithms, such as\\npseudospectral methods, 117−119 resolution of the identity(RI),120 domain-based local pair natural orbital methods\\n(DLPNO),121 and explicitly correlated R12/F12 methods.122\\nThere are also ongoing eﬀorts to develop other CompChem\\nmethods based on quantum Monte Carlo 123 and densitymatrix renormalization group theory (DMRG)124 to provide\\nhigh accuracy with competitive scaling with other computa-\\ntional methods. Eﬀorts are beginning to become implemented\\nt h a tu s eM Lt oa c c e l e r a t et h e s et y p e so fc a l c u l a -tions.105,106,125−129\\nSchemes have also been developed to exploit systematic\\nerrors between diﬀerent levels of theory with diﬀerent basis\\nsets so that approximations can be extrapolated toward anexact result. Examples include the complete basis set (CBS),130\\nGaussian G n,131 Weizmann (W-) n132 methods, and high\\naccuracy extrapolated ab initio thermochemistry (HEAT)133\\nmethods. For a recent review on these and other methods, seeref134. These schemes are also becoming a target of recent\\nwork using ML methods.135\\nHF determinants provide good baseline approximations of\\nthe ground state electronic structure of many molecules, butthey may describe poorly more complicated bonding that\\narises during bond dissociation events, excited states, and\\nconical intersections.136−139 Some many-body wavefunctions\\nare best described as a superposition of two or moreconﬁgurations, for example, when other conﬁgurations in eq\\n7 can have similar or higher expansion coeﬃcients a than the\\nHF determinant. For this reason, high quality single reference\\nmethods like CCSD(T) fail because the theory assumes thatsalient electronic eﬀects are captured by the initial single HF\\nconﬁguration. (In fact, methods such as CCSD(T) have been\\nimplemented with diagnostic approaches available that let\\nusers know when there may be cause for concern).140−142 Inthese cases, it may no longer be trivial toﬁnd reliable black-box\\nor automated procedures (e.g., in situations involving\\nresonance states, chemical reactions, molecular excited states,transition metal complexes, and metallic materials, etc.).136 So-\\ncalled multiconﬁguration approaches,136 such as the general-\\nized valence bond (GVB) method143 or the complete active\\nspace self-consistentﬁeld (CASSCF),144 the multireference CI(MRCI) methods, 145 complete active space perturbation\\ntheory (CASPT2), 146 or multireference coupled cluster\\n(MRCC),147,148 can more physically model these systems\\nsince they employ several suitable reference con ﬁgurationswith di ﬀerent degrees of correlation treatments. These\\nmethods are not black-box and should be expected to require\\nan experienced practitioner with CPI to choose the reference\\nstates that can substantially inﬂuence the quality of results.149This is an area though where ML can bring progress in\\nautomating the selections of physically justi ﬁed active\\nspaces.129\\nIn closing, there are a large number of available correlated\\nwavefunction methods but many are even more costly than HFtheory by virtue of requiring an HF reference energy\\nexpression shown ineq 5. Figure 5a depicts a so-called\\n“magic cube” (that is an extension beyond a traditional“Pople\\ndiagram”135,150) that concisely shows a full hierarchy ofcomputational approaches across di ﬀerent Hamiltonians,\\nbasis sets, and correlation treatment methods. This makes it\\neasy to identify diﬀerent wavefunction methods that should be\\nmore accurate and more likely to provide useful atomic scaleinsights (as well as those that would be more computationally\\nintensive). Another important aspect highlighted in the“magic\\ncube” is that higher level wavefunction methods require largerbasis sets to successfully model electron correlation eﬀects. A\\nCCSD(T) computation carried out with a small basis set forcube” is that higher level wavefunction methods require larger\\nbasis sets to successfully model electron correlation eﬀects. A\\nCCSD(T) computation carried out with a small basis set for\\nexample might only oﬀer the same accuracy as MP2 whilebeing two orders of magnitude more expensive to evaluate.108\\nAs was mentioned earlier with the benzene system, spurious\\nerrors with diﬀerent basis sets might still be found that indicate\\nproblems with speciﬁc combinations of levels of theory andbasis sets. The deep complexity of correlated wavefunction\\nmethods makes this a promising area for continued eﬀorts in\\nCompChem+ML research.\\n2.2.3. Density Functional Theory. Density-functionaltheory (DFT)151 is another method to calculate the quantum\\nmechanical internal energy of a system using an energy\\nexpression that relies on functionals (i.e., a function of a\\nfunction) of electronic densityρ = |Ψel(r; R)|2:\\nETVρρρ[] = [] + [ ] (8)ETVρρρ[] = [] + [ ] (8)\\nCompared to wavefunction theory, DFT should be far more\\neﬃcient since the dimensionality of a density representation\\nfor electrons will always be three rather than the 3n dimensionsfor any n-electron system des cribed by a many-body\\nwavefunction method. DFT has an important drawback that\\nthe exact expression for the energy functional is currently\\nunknown, all approximations bring some degree of uncontrol-lable error, and this has precipitated disagreeable opinions\\nfrom purists in chemical physics, especially those who are\\ndeveloping correlated wavefunction methods. However, there\\nis also substantial evidence that DFT approximations arereasonably reliable and accurate for many practical applications\\nthat bring information, knowledge, and sometimes insight. We\\nnow provide a bird’s-eye view of DFT-based methods.\\nOne thrust of DFT developments since its inception hasfocused on designing accurate expressions strictly in terms of a\\ndensity representation, and these approaches are referred to as\\n“kinetic energy (KE-)” or “orbital-free (OF-)” DFT.152 Someenergy contributions (e.g., nuclear-electron energy and classical\\nelectron−electron energy terms) can be expressed exactly, but\\nother terms, such as the kinetic energy as a function of the\\ndensity are not known and must be approximated. OF-DFT isvery computationally eﬃcient (these methods should scale\\nlinearly with system size153,154) but these formulations have\\nnot yet been developed to rival the accuracy or transferability\\nof wavefunction methods, though they have been used forstudying diﬀerent classes of chemical and materials sys-\\ntems.155−157 OF-DFT methods are also used in exciting\\napplications modeling chemistry and materials under extreme\\nconditions.158−160 One should expect that once highly accurateforms are developed and matured, accurate CompChem\\ncalculations on electronic structures on systems having more\\nthan a million atoms might become commonplace. Indeed,\\nthere are eﬀorts to use ML to develop more physical OFDFT\\nmethods.161,162methods.161,162\\nThe most commonly used form of DFT (which is also one\\nof the most widely used CompChem methods in use today) is\\ncalled Kohn−Sham (KS-)DFT.163 In KS-DFT, one assumes a\\nﬁctitious system of noninteracting electrons with the sameground state density as the real system of interest. This makes\\nit possible to split the energy functional ineq 8 into a new\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9825form that involves an exact expression of the kinetic energy for\\nnoninteracting electrons:\\nET V V T Vni eN ee ee eeρρ ρρ ρ ρ[ ]= [ ]+ [ ]+ [ ]+Δ [ ]+Δ [ ]\\n(9)\\nHere, Tni[ρ] is the kinetic energy of the noninteractingelectrons, VeN[ρ] is the exact nuclear-electron potential, and\\nVee[ρ] is the Coulombic (classical) energy of the non-\\ninteracting electrons. The last two terms are corrections due\\nto the interacting nature of electrons and nonclassicalelectron−electron repulsion. KS-DFT also expands the three-\\ndimensional electron density into a spin orbital-basisϕ similar\\nto HF theory to deﬁne the one-electron kinetic energy in a\\nstraightforward manner. This allows the Tni, VeN, and Veeexpressions to be evaluated exactly and one arrives at the KS\\nenergy:\\nÄ\\nÇ\\nÅÅÅÅÅÅÅÅÅÅ\\nÉ\\nÖ\\nÑÑÑÑÑÑÑÑÑÑ\\n´≠ÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖ ÆÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖ´≠ÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖ ÆÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖ\\n´≠ÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖ ÆÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖ\\nE\\nZ\\nE\\nrr r\\nrr rR r\\nrr r r\\nrr r\\nd( ) 1\\n2 ()\\nd( ) ( )\\nd( ) 1rr rR r\\nrr r r\\nrr r\\nd( ) 1\\n2 ()\\nd( ) ( )\\nd( ) 1\\n2 d () ()\\ni\\nii\\nT\\ni\\ni\\nN\\ni\\nV\\ni\\nii\\nV\\nKS 1 1\\n2\\n1\\n11\\n1\\n1\\n11\\n1\\n1\\n(Coulomb)\\nxc\\nni\\neN\\nee\\n∫\\n∫\\n∫∫\\n∑\\n∑∑\\n∑\\nρϕ ϕ\\nϕϕ\\nϕ ρ ϕ\\nρ\\n[] = − * ∇\\n− *\\n|− |\\n+ * ′ ′\\n|− ′ |\\n+[ ]\\nα\\nα\\nα\\n̂\\n(10)− *\\n|− |\\n+ * ′ ′\\n|− ′ |\\n+[ ]\\nα\\nα\\nα\\n̂\\n(10)\\nThe last two correction terms in eq 9 arise from electron\\ninteractions, and these are combined into the so-called\\n“exchange-correlation” term ( Exc), which uniquely de ﬁneswhich scheme of KS-DFT is being used. In theory, an exactExc\\nterm would capture all di ﬀerences between the exact FCI\\nenergy and the system of noninteracting electrons for a ground\\nstate.\\nThe KS-DFT equations can be cast in a similar form as theRoothan−Hall equations (eq 6), which allows for a computa-\\ntionally eﬃcient solution. Moreover, the elements of the KS\\nmatrix (which replaces the Fock matrix F) are easier to\\nevaluate due to the fact that several of the computationallyintensive integrals are now accounted for viaExc. Hence, the\\nformal scaling for KS-DFT is n()36 with respect to the number\\nof electrons. Even though this is much poorer scaling than\\nideally linear scaling OF-DFT, the exact treatment ofnoninteracting electrons makes KS-DFT more accurate.\\nFurthermore, there are several modern exchange-correlation\\nfunctionals that routinely achieve much higher accuracy than\\nHF theory with less computational cost, and thus KS-DFT is acompetitive alternative with many correlated wavefunction\\nmethods in many modern applications.\\nA remaining problem is constructing a practical expression\\nfor the exchange-correlation functional, as its exact functionalform remains unknown. This has spawned a wealth of\\napproximations that have been founded with diﬀerent degrees\\nof ﬁrst principles and/or empirical schemes. Classes of KS-\\nDFT functionals are de ﬁned by whether the exchange-correlation functional is based on just the homogeneous\\nelectron gas (i.e., the“local density approximation”, LDA), that\\nand its derivative (i.e., the “generalized gradient approxima-\\ntion”, GGA), as well as other additional terms that shouldresult in physically improved descriptions or error cancella-\\ntions. The resulting hierarchy of KS-DFT functionals is often\\nreferred to as a “Jacob’s Ladder ” of DFT ( Figure 5 b).\\nGenerally, the higher up the ladder one goes, the moreaccurate but more computationally demanding the calcu-\\nlation.164 However, the intrinsic inexactness in DFT makes it\\ndiﬃcult to assess which functionals are physically better than\\nothers.165,166 Nevertheless, the Jacob ’s Ladder hierarchy isuseful for clearly designating how and why newer methods\\nshould perform in speciﬁc applications (for perspective see refs\\n167−169).\\nIndeed, by being based on a ground-state representation for\\nhomogeneous electron gas, DFT calculations can sometimesbring more easily physical insight into some systems that are\\nvery challenging for wavefunction theory to examine (e.g.,\\nmetals, where HF theory provides divergent exchange energybring more easily physical insight into some systems that are\\nvery challenging for wavefunction theory to examine (e.g.,\\nmetals, where HF theory provides divergent exchange energy\\nbehaviors170,171). On the other hand, DFT is also generally notwell-suited for studying phy sical phenomena involving\\nlocalized orbitals or band structures such as those found in\\nsemiconducting materials with small band gaps, molecular or\\nmaterial excited charge transfer states, or interaction forces thatcan arise due to excited states, e.g. dispersion (or London)\\nforces. The former features can normally be treated using\\nHubbard-corrected DFT+U models that require a system-\\nspeciﬁc U−J parameter172,173 or more generalizable but muchmore computationally expensive hybrid DFT approaches.\\nDispersion forces (i.e., van der Waals interactions) are\\nnonexistent in semilocal DFT approximations, and it is now\\ncommonplace to introduce them into DFT calculations using avariety of diﬀerent methods.36\\nThere is also growing interest in using embedded\\nCompChem calculation schemes that can partition systems\\ninto discrete regions that could be treated with highly accuratecorrelated wavefunction theory and computationally eﬃcient\\nKS-DFT schemes separately. 174−178 DFT has also been\\nextended to the modeling of excited states in the form of\\ntime-dependent (TD-)DFT.179 Similar to ground state DFT,TDDFT is a less computationally expensive alternative to\\nexcited state wavefunction-based methods. The approach\\nyields reasonable results where excitations induce only small\\nchanges in the ground state density, e.g. low lying excitedstates.179,180 However, due to its single reference nature,\\nTDDFT tends to break down in situations where more than\\none electronic conﬁguration contribute signi ﬁcantly to the\\nexcited state. Just as with correlated wavefunction methods,there are already signs of CompChem+ML eﬀorts to improve\\nthe applicability of DFT-based methods.181−185\\n2.2.4. Semiempirical Methods.Correlated wavefunctions\\nand, to a lesser degree, KS-DFT are still very computationallydemanding and only of limited use for large scale simulations.\\nFurther approximations based on wavefunctions and DFT\\nmethods have been developed to simplify and accelerate\\nenergy calculations. These so-called semiempirical methodsstill explicitly consider the electronic structure of a molecule\\nbut in a more approximate way than methods described above.\\nSemiempirical approaches based on wavefunction theory\\ninclude methods like extended Hückel theory and neglect ofdiatomic diﬀerential overlap (NDDO).186 Both approaches are\\nsimpli ﬁcations of the HF eqs ( eq 5 ) by introducing\\napproximations to the di ﬀerent integrals. In the NDDO\\napproach,187 only the two-electron integrals in eq 5 areconsidered, where the two orbitals on the right and left-hand\\nside of the rr\\n1\\nij|−| operator are located on the same atom. The\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9826remaining two-center (and one-center) integrals are then\\napproximated by introducing a set of empirical functions, one\\nfor each unique type of integral. Moreover, the overlap matrix\\nin eq 6is assumed to be diagonal, which greatly simpliﬁes theenergy evaluation. This reduces the required computational\\neﬀort tremendously and allows the scaling of these approaches\\nto be reduced to N() 26 . NDDO serves as a basis for more\\nsophisticated semiempirical schemes, such as AM1,188 PM7,189and MNDO,190 where the energy is usually determined self-\\nconsistently using a minimally sized basis set. Inadequacies in\\ntheory can be compensated by diﬀerent empirical para-\\nmetrization schemes that can allow these calculations to rivalthe accuracy of higher level theory for some systems. For\\nexample Dral et al.191 provided a recent“big-data” analysis of\\nthe performance of several semiempirical methods with large\\ndata sets.data sets.\\nSemiempirical schemes are also carried over to approximate\\nKS-DFT with so-called density functional tight binding\\n(DFTB).192 DFTB simpli ﬁes the KS eqs ( eq 10 )b y\\ndecomposing the total electron density ρ into a density offree and neutral atomsρ0 and a small perturbation termδρ0 (ρ\\n= ρ0 + δρ0). Expandingeq 10in the perturbationδρ0 makes it\\npossible to partition the total energy into three terms\\namendable to diﬀerent approximation schemes:amendable to diﬀerent approximation schemes:\\nEE E E0 rep Coul 0 BS 0δρ δρ δρ[] = + [] + [] (11)\\nErep is a repulsive potential containing interactions between the\\nnuclei and contributions from the exchange correlationfunctional (these are typically approximated via pairwise\\npotentials). The chargeﬂuctuation term ECoul is modeled as\\na Coulomb potential of Gaussian charge distributions\\ncomputed from the approximate density. Finally,EBS refersto the “band structure” term, which considers the electronic\\nstructure and contains contributions from Tni, VeN, and the\\nexchange correlation functional (seeeq 10). To computeEBS,\\nthe density is expressed in a minimal basis of atomic orbitals,similar as in NDDO. The necessary Hamiltonian and overlap\\nintegrals are then evaluated via an approximate scheme based\\non Slater−Koster transformations. In addition to the energy,\\natomic partial charges are also computed in this step, which arethen used inECoul. As a consequence, DFTB equations can also\\nbe solved self-consistently. DFTB methods are parametrized by\\nﬁnding suitable forms for the repulsive potential and adjusting\\nthe parameters used in the Slater−Koster integrals. Non-self-consistent and self-consiste nt tight-binding DFT meth-\\nods193,194 have been developed for simulating large scale\\nsystems. Semiempirical methods have also been a target of\\ndiﬀerent ML schemes, yielding improved parametrizationschemes and more accurate functional approximations.195−198\\n2.2.5. Nuclear Quantum Eﬀects. The quantum nature of\\nlighter elements, such as H−Li, and even heavier elements that\\nform strong chemical bonds (C −C bond in graphene forexample199) gives rise to signiﬁcant nuclear quantum eﬀects\\n(NQEs). Such eﬀects are responsible for large diﬀerences from\\nthe Dulong−Petit limit of the heat capacity of solids, isotope\\neﬀects, and the deviations of the particle momentumdistribution from the Maxwell −Boltzmann equation.200 To\\ncapture NQEs, path-integral molecular dynamics\\n(PIMD)201,202 or centroid molecular dynamics (CMD)203,204\\ncan be used, but these methods are associated with muchhigher computational costs (usually about 30 times higher)\\ncompared with classical MD simulations using point nuclei.\\nMoreover, because systems may be inﬂuenced by competing\\nNQEs, the extent of NQEs is sensitive to the potential energysurface assumed. (Semi)local DFT approaches may not even\\nqualitatively predict isotope fractionation ratios, and usually\\nhybrid DFT is needed to reach quantitative accuracy. 205\\nHowever, employing hybrid DFT calculations or other highlevel methods in PIMD/CMD simulations can accrue\\nextremely high computational costs. For this reason, MLHowever, employing hybrid DFT calculations or other high\\nlevel methods in PIMD/CMD simulations can accrue\\nextremely high computational costs. For this reason, ML\\nforce ﬁelds have been proposed as eﬃcient means to carry outPIMD simulations, enabling essentially exact quantum-\\nmechanical treatment of both electronic and nuclear degrees\\nof freedom, at least for small molecules with dozens of\\natoms.206,207\\n2.2.6. Interatomic Potentials. Interatomic potentialsintroduce an additional level of abstraction compared to\\nmethods described above. Instead of using exact quantum\\nmechanical expressions to create the PES for the system,\\nanalytic functions are used to model a presupposed PES thatcontains explicit interactions between atoms, while electrons\\nare treated in an implicit manner (sometimes using partial\\ncharge schemes). 251−256 Interatomic potentials thus are\\n(oftentimes dramatically) more computationally eﬃcient thancorrelated wavefunction, DFT, and semiempirical approaches.\\nThis eﬃciency makes it possible to study even larger systems of\\natoms (e.g., biomolecules, surfaces, and materials) than is\\npossible with other computational methods. Note that diﬀerentempirical potentials bring substantially diﬀerent computational\\neﬃciencies; for example Lennard-Jones (LJ) potentials are\\nTable 2. Types of Interatomic Potentials and Their Areas of Application\\npotential reactive typical applications examplespotential reactive typical applications examples\\npairwise-distance-\\nbased\\nsometimes materials, liquids Lennard-Jones, 208,209 Morse,210, Buckingham211\\ndistance and angle-\\nbaseddistance and angle-\\nbased\\nusually no materials, liquids many water potentials (e.g., SPC, TIP4P, mW), 212 Stillinger−Weber213\\nclass I (nonpolariz-\\nable) force ﬁelds\\nno proteins, lipids, polymers, nucleic acids,\\ncarbohydrates, organic molecules,carbohydrates, organic molecules,\\nliquids\\nAMBER,214,215 GAFF,216 CHARMM,217 GROMOS,218−220 OPLS,221,222 DREIDING,223\\nMMFF94,224 UFF,225 COMPASS,226 INTERFACE,227 interatomic potentials for ionic\\nsystems228systems228\\nclass II (polarizable) no proteins, lipids, polymers, nucleic acids,\\ncarbohydrates, organic molecules,\\nliquids\\nAMOEBA,229 classical Drude oscillator models,230 ﬂuctuating charge (FQ) models,231 MB-Pol,232 distributed point polarizable models (DPP2),233 and many more234\\nembedded atom\\nmethod (EAM)-like\\nyes reactions within solid materials EAM, 235 MEAM,236 Finnis−Sinclair,237 Sutton−Chen238\\nbond-order potentials\\n(BOPs)bond-order potentials\\n(BOPs)\\nyes reactions within solids, liquids, gases Brenner, 239 Tersoﬀ,240,241 REBO,239,242 COMB,243,244 ReaxFF,245,246 APT247\\nother quantum me-\\nchanics-derived\\nforceﬁeldsother quantum me-\\nchanics-derived\\nforceﬁelds\\nyes reactions within liquids and gases EVB 248 and related models249,250\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9827more eﬃcient than classical forceﬁelds (FFs) like AMBER and\\nCHARMM, while those are more eﬃcient than most bond-\\norder potentials, such as ReaxFF. 245,246 T h ed e g r e eo f\\neﬃciency arises from the balance of using accurate orphysically justi ﬁed functional forms, approximations, and\\nmodel parametrizations. There are many diﬀerent formulations\\n(see Figure 5c), and we will discuss the most general classes.\\nAn overview of the di ﬀerent types of potentials and theirfeatures is provided inTable 2. For extensive discussions on\\nthese methods including semiempirical approaches, we refer to\\nthe extensive review by Akimov and Prezhdo (ref257). An\\nexcellent review for interatomic potentials is provided byHarrison et al. (ref258), and an excellent overview of modern\\nmethods can be found in a special issue ofJ. Chem. Phys.259\\nThe distinctions between diﬀerent types of FFs can be blurry\\nsometimes, and we will diﬀerentiate categories in ascendingcomplexity. One of the simplest interatomic potentials is the LJ\\npotential:260\\nÄ\\nÇ\\nÅÅÅÅÅÅÅÅÅÅÅÅÅ\\ni\\nk\\njjjjjj\\ny\\n{\\nzzzzzz\\ni\\nk\\njjjjjj\\ny\\n{\\nzzzzzz\\nÉ\\nÖ\\nÑÑÑÑÑÑÑÑÑÑÑÑÑ\\nE rr4\\nij i\\nij\\nij\\nij\\nij\\nij\\nLJ\\n,\\n12 6\\n∑ ε\\nσσ\\n=−\\n> (12)ij i\\nij\\nij\\nij\\nij\\nij\\nLJ\\n,\\n12 6\\n∑ ε\\nσσ\\n=−\\n> (12)\\nIt models the total energy as the sum of all pairwise interaction\\nbetween atoms i and j using an attractive and repulsive term\\ndepending on the interatomic distance rij. εij modulates thestrength of the interaction function, whileσij deﬁnes where it\\nreaches its minimum. The LJ potential is a prototypical“good\\nmodel” of interatomic potentials, as it has a suﬃciently simple\\nphysical form with only two parameters while still yieldinguseful results.\\nFor covalent systems, such as bulk carbon or silicon, just\\npairwise distances are not su ﬃcient to capture the local\\ncoordination of the atoms, and many empirical poten-\\ntials212,213,261 for these systems were expressed as a functionof the pairwise distances and three-body terms within a certain\\ncutoﬀ distance. The pairwise term can take the form of LJ-type,\\nelectrostatic, or harmonic potentials, and the three-body termis usually a function of the angles formed by sets of three\\natoms.\\nSo-called class I classical FFs introduce a more complicated\\nenergy expression:\\nEk r r k\\nk\\nqq\\nr E\\n() ( )\\n1c o s ( )\\nij ij ij ijk ijk ijk\\nijkl ijkl ijkl\\nij i\\nij\\nij\\ntot\\nbonds\\n2\\nanglesijkl ijkl ijkl\\nij i\\nij\\nij\\ntot\\nbonds\\n2\\nangles\\n2\\ndihedral\\n() ()\\n,\\nLJ\\n∑∑\\n∑∑ ∑\\nθθ\\nγϕ ϕ\\n=− ̅ +− ̅\\n+[ − − ̅ ]+ +\\nγ\\nγ γ\\n>\\n(13)\\nThe ﬁrst three terms are the energy contributions of the\\ndistances (rij), angles (θijk) and dihedral angles (ϕijkl) betweenbonded atoms. Because of this, they are also referred to as\\nbonded contributions. Bond and angle energies are modeled\\nvia harmonic potentials, with thekij and kijk parameters\\nmodulating the potential strength and r̅ij and θ̅ijk are theequilibrium distances and angles. The dihedral term is\\nmodeled with a Fourier series to capture the periodicity of\\ndihedral angles, withkijkl and ϕijkl as free parameters. The last\\ntwo terms account for nonbonded interactions. The long-rangeelectrostatics are modeled as the Coulomb energy between\\nchargesqi and qj, and the van der Waals energy is treated via a\\nLJ potential (eq 12). In Class I/II FFs, empirical parameters\\nare tabulated for a variety of elements in wide ranges ofchemical environments (for example ref262). Parameters for\\nany one system should not necessarily be assumed to transfer\\nwell to other systems, and reparametrizations may be needed\\ndepending on the application. Diﬀerent sets of parametrizationschemes give rise to di ﬀerent types of classical FFs, with\\nCHARMM, 217 Amber, 214 ,215 GROMOS, 218 −220 and\\nOPLS221,222 being a few of many examples.\\nAn extension beyond these FFs are class II (i.e., “polar-izable”) FFs, where the static charges are replaced by\\nenvironment dependent functions (e.g., AMOEBA 263). A\\nsigniﬁcant advantage to the class I and II types of FFs is that\\nthey are computationally e ﬃcient, which makes them wellsuited for MD simulations of complex and extended (bio)-signiﬁcant advantage to the class I and II types of FFs is that\\nthey are computationally e ﬃcient, which makes them well\\nsuited for MD simulations of complex and extended (bio)-\\nmolecules, such as proteins, lipids, or polymers. Implementa-tions of FF calculations on GPUs makes these simulations\\nextremely productive.264−268 A disadvantage of Class I and II\\ntypes of interatomic potentials is that they rely on predeﬁned\\nbonding patterns to compute the total energy, and this limitstheir transferability. In general, bonds between atoms are\\ndeﬁned at the beginning of the simulation run and cannot\\nchange. Furthermore, bonding terms make use of harmonic\\npotentials that are not suitable for modeling bond dissociation.Reactive potentials, which eschew harmonic potential\\ndependencies and thus can describe the formation and\\nbreaking of chemical bonds, include the embedded atom\\nmethod (EAM,Figure 5c), which is used widely in materialsscience.235 EAM is a type of many-body potential primarily\\nused for metals, where each atom is embedded in the\\nenvironment of all others. The total energy is given by\\nÄ\\nÇ\\nÅÅÅÅÅÅÅÅÅÅÅÅ\\nÉ\\nÖ\\nÑÑÑÑÑÑÑÑÑÑÑÑ\\nEF V r () 1\\n2 ()\\ni\\nN\\ni i\\nji\\nij ijtot ∑∑ ρ= ̃ +EF V r () 1\\n2 ()\\ni\\nN\\ni i\\nji\\nij ijtot ∑∑ ρ= ̃ +\\n≠ (14)\\nFi is an embedding function andρ̃i an approximation to the\\nlocal electron density based on the environment of atom i.\\nFi(ρ̃i) can be seen as a contribution due to nonlocalizedelectrons in a metal.Vij is a term describing to the core−core\\nrepulsion between atoms. An EAM potential is determined by\\nthe functional forms used forFi and Vij, as well as how the\\ndensity is expressed. Its dependence on the local environmentwithout the need for predeﬁned bonds make EAM well suited\\nfor modeling material properties of metals. An extension of\\nEAM is modiﬁed EAM (MEAM),236 which includes direc-\\ntional dependence in the description of the local densityρ̃i, butthis brings greater computational cost. EAMs also form the\\nconceptual basis of the embedded atom neural network\\n(EANN) machine learning potentials (MLPs).269\\nAnother common type of reactive potentials are bond-orderpotentials (BOPs). In general, BOPs model the total energy of\\na system as interactions between the neighboring atoms:\\nEV r b V r f r() () ()\\nij i\\nij ij k ij ijtot\\n,\\nrep ( ) att cut∑=[ − ]\\n> (15)ij ij k ij ijtot\\n,\\nrep ( ) att cut∑=[ − ]\\n> (15)\\nVrep and Vatt are repulsive and attractive potentials depending\\non the interatomic distancerij. A cutoﬀ function fcut restricts all\\ninteractions to the local atomic environment.bij(k) is the bondorder term, from which the potential takes its name. This term\\nmeasures the bond order between atomsi and j (i.e., “1” for a\\nsingle bond, “2” for a double bond, and“0.6” for a partially\\ndissociated bond). Bond orders can also depend ondissociated bond). Bond orders can also depend on\\nneighboring atomsk in some implementations. BOPs are\\ntypically used for covalently bound systems, such as bulk solids\\nChemical Reviews pubs.acs.org/CR ReviewChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9828and liquids containing hydrogen, carbon or silicon (e.g., carbon\\nnanotubes and graphene). Depending on the exact form of the\\nexpressions ineq 15,d iﬀerent types of BOPs are obtained,\\nsuch as Tersoﬀ240,241 and REBO239,242 potentials. BOPs canalso be extended to incorporate dynamically assigned charges,\\nyielding potentials like COMB243,270 or ReaxFF.245,246 As with\\nEAMs, BOPs have also been used as a starting point for\\nconstructing more elaborate MLPs271−273 that will also bediscussed in more detail insection 3.\\nWhile e ﬃcient and versatile, all interatomic potentials\\ndescribed above are inherently constrained by their functional\\nforms. A diﬀerent approach is pursued by MLPs, such asBehler−Parinello Neural Networks,274 q-SNAP,275 and GAP\\npotentials 276 (Figure 5 c). In MLPs, suitable functional\\nexpressions for interactions and energy are determined in a\\nfully data-driven manner and ultimately only limited by theamount and quality of available reference data. One can then\\nuse substantially more data to generate a much more accurate\\nMLP than would be possible when using, for instance, a\\nReaxFF potential trained on similar data sets.277ReaxFF potential trained on similar data sets.277\\nFor the sake of completeness, we note that all approaches\\ndescribed here are fully atomistic−each atom is modeled as an\\nindividual entity. It is also possible to combine groups of atomsinto pseudoparticles giving rise to so-called coarse grained\\nmethods. On an even higher level of abstraction, whole\\nenvironments can be modeled as a single continuum. As such\\napproaches are not subject of the present review, we refer theinterested reader, for example, to refs278 and 279.\\n2.3. Response Properties\\nOnce an energy calculation is completed by one of the\\nCompChem methods above, many other interesting molecular\\nproperties can be calculated. Most of these properties can beobtained as the response of the energy to a perturbation, for\\nexample, changes in nuclear coordinatesR, external electric (ϵ)\\nor magnetic (B) ﬁelds or the nuclear magnetic moments {Ii}.\\nGiven an expression for the energy, which depends on theabove quantities, so-called response properties can be\\ncomputed via the corresponding partial derivatives of the\\nenergy. A general response propertyΠ then takes the form\\nnn nn E RB I\\nRB I\\n(, , , ) (, , , )nn nn\\ni\\nnn n\\ni\\nnRB I i\\ni\\ni\\nRB I\\nRB IΠ = ∂ϵi\\nnn n\\ni\\nnRB I i\\ni\\ni\\nRB I\\nRB IΠ = ∂ϵ\\n∂∂ ϵ ∂∂\\nϵ\\n+++ϵ\\nϵ (16)\\nwhere the ns indicate the n-th order partial derivative with\\nrespect to the quantity in the subscript.102\\nA common response property is nuclear forcesF = −Π (1,0, 0, 0) that are the negativeﬁrst derivatives of the energy with\\nrespect to the nuclear positions. Such calculations allow a\\nplethora of diﬀerent geometry optimization schemes for\\nchemical structures on the PES. Hessian calculationscorresponding to the second derivative of energy with respect\\nto nuclear positions are necessary to conﬁrm the location of\\nﬁrst-order saddle points on the PES and identify normal modesand their frequencies for vibrational partition functions that are\\nuseful for modeling temperature dependencies based on\\nstatistical thermodynamics. Hessian calculations are computa-\\ntionally costly, since they normally involve calculations basedon ﬁnite diﬀerences methods involving many nuclear force\\ncalculations. Many methods have been developed to allow\\nCompChem algorithms to sample minimum energy regions of\\nthe PES280−284 or precisely locate points of interest. 285,286Historically, many of these techniques have relied on\\napproximate or full Hessian calculations, 287 but other\\napproaches, such as the nudged-elastic band 288,289 and\\nstring290−292 methods, are popular alternatives that do notrequire a Hessian calculation. There have also been eﬀorts\\nusing di ﬀerent forms of ML to accelerate procedures or\\novercome long-standing challenges in eﬃcient sampling of and\\noptimization on the PES.293−298optimization on the PES.293−298\\nThe general expression above can provide a wealth of other\\nquantities, some of which are relevant for molecular spectros-optimization on the PES.293−298\\nThe general expression above can provide a wealth of other\\nquantities, some of which are relevant for molecular spectros-\\ncopy or provide a direct connection to experiment (seeTable3). Infrared spectra can be simulated based on dipole moments\\nμ = −Π (0, 1, 0, 0), while molecular polariziabilitiesα = −Π\\n(0, 2, 0, 0) oﬀer access to polarized and depolarized Raman\\nspectra. Nuclear magnetic shielding tensorsσ = Π (0,0,1,1) area central response property of a magneticﬁeld. These allow the\\ncomputation of chemical shifts recorded in nuclear magnetic\\nresonance (NMR) spectroscopy via their trace Trii\\n1\\n3 σσ =[ ].1\\n3 σσ =[ ].\\nThe beauty of this formalism lies in the fact that a single energy\\ncalculation method provides access to a wide range of quantum\\nchemical properties in a highly systematic manner. A largenumber of modern MLPs use the response of the potential\\nenergy with respect to nuclear positions to obtain energy\\nconserving forces. However, far fewer applications model\\nperturbations with respect to electric and magneticﬁelds. Ref299 extends the descriptor used in the Faber−Christensen−\\nHuang−Lilienfeld (FCHL) Kernel by adding an explicitﬁeld\\ndependent term that makes it possible to predict dipole\\nmoments across chemical compound space. Ref300 introducesa general neural network (NN) framework to model\\ninteractions of a system with vectorﬁelds, which was then\\nused to predict dipole moments, polarizabilities and nuclear\\nmagnetic shielding tensors as response properties.\\n2.4. Solvation Models2.4. Solvation Models\\nAn important aspect of CompChem is molecular descriptions\\nfrom within a solution environment. Simulating a dynamical\\nenvironment composed of many surrounding molecules isusually not feasible with electronic-structure methods. To\\ncircumvent this problem, solvation modeling schemes have\\nbeen devised (see refs301−306 for discussions on this topic).\\nThe most popular approaches are so-called polarizablecontinuum solvent models (PCM). 279 They model the\\nelectrostatic interaction of a solute molecule with its\\nenvironment by representing the charge distribution of the\\nsolvent molecules as a continuous electricﬁeld, the reactionﬁeld. This dielectric continuum can be interpreted as a\\nthermally averaged representation of the environment and is\\ntypically assigned a constant permittivity depending on the\\nparticular solvent to be modeled (ε = 80.4 for water). Thesolute is placed inside a cavity embedded in this continuum.\\nTable 3. Response Properties of the Potential Energy\\nnR nϵ nB nI property\\n0000 energy\\n1000 forces\\n2000 Hessian (harmonic frequencies)\\n0100 dipole moment (IR)0100 dipole moment (IR)\\n1100 infrared absorption intensities (IR)\\n0200 polarizability (Raman)\\n0011 nuclear magnetic shielding (NMR)\\n0002 nuclear spin−spin coupling (NMR)\\n0110 optical rotation (circular dichroism)0110 optical rotation (circular dichroism)\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9829The charge distribution of the molecule then polarizes the\\ncontinuous medium, which in turn acts back on the molecule.\\nTo compute the electrostatic interactions arising from this\\nmutual polarization with electronic structure theory, a self-consistent scheme is employed. After constructing a suitable\\nmolecular cavity, a Poisson problem of the following form is\\nsolved:\\nVrr r() () 4 () mπρ−∇[ϵ ∇ ] = (17)\\nHere, ρm(r) is the charge distribution of the solute andϵ(r) isthe position dependent permittivity, which usually is set to one\\nwithin the cavity and theε of the solvent on the outside.V(r)\\nis the electrostatic potential composed of the two terms\\nVV Vrr r() () ()ms=+ (18)VV Vrr r() () ()ms=+ (18)\\nwhere Vm(r) is the solute potential andVs(r) is the apparent\\npotential due to the surface charge distributionσ(s)\\nV r s\\nrs s() () ds ∫\\nσ= |−|Γ (19)\\nΓ indicates the surface of the cavity. Eq 17 is solvednumerically to obtain the surface charge distribution σ(s).\\nOnce σ(s) has been determined in this fashion, the potential is\\ncomputed according to eq 19 and used to construct an\\neﬀective Hamiltonian of the form\\nH HV r()eff ŝ = ̂+ (20)H HV r()eff ŝ = ̂+ (20)\\nwhere Ĥ is the vacuum Hamiltonian. These equations are then\\nsolved self-consistently in a Roothan−Hall or KS approach,\\nyielding the electrostatic solvent −solute interaction energy.This scheme is also called the self-consistent reaction ﬁeld\\napproach (SCRF).\\nContinuum models diﬀer in how the cavities are constructed\\nand how eq 17 is solved to obtain the surface charge\\ndistribution. Variants include the original PCM model, alsoreferred to as dielectric PCM (D-PCM),307 the integral\\nequation formulation of PCM (IEFPCM), 308 SMD,309\\nconductor PCM (C-PCM),310 or the conductor-like screening\\nmodel (COSMO).311 The latter two approaches replace thedielectric medium by a perfect conductor to allow for a\\nparticularly eﬃcient computation ofσ(s). PCMs can be further\\nextended with statistical thermodynamics treatments to\\naccount for solutes having diﬀerent size and concentrationeﬀects, and this leads to models such as COSMO-RS.312\\nA drawback of most PCM-like approaches is that they\\nneglect local solvent structures. Thus, they cannot reliably\\naccount for situations where explicit solvent interactions areimportant, for example, when for stabilizing speciﬁc sites for a\\ntransition state through hydrogen bonding.301 Furthermore,\\nwhile implicit models might be parametrized toﬁt bulk-like\\nproperties of mixed or ionic solvents (e.g., ref 313.), thecomplex local solvent environment presented by these systems\\nare treatable by other means. For mixed solvent systems a\\nrange of hybrid schemes such as COSMO-RS,305 reference\\ninteraction site models (RISMs) 314,315 or QM/MM316−318approaches have been developed. As an in-depth discussion of\\nthese alternative schemes exceeds the scope of this Review, we\\ninstead refer to other references.319,320\\nML models are becoming used to describe solvent eﬀects.Ref 300 introduces a continuum ML model based on a\\nreaction ﬁeld that can predict energies and response properties\\nfor continuum solvents, it can extrapolate to solvents not seen\\nduring training, and it can be extended to operate in a QM/MM fashion to account for explicit solvents eﬀects in a Claisen\\nrearrangement reaction. Ref 321 implemented automatable\\ncalculation schemes and unsupervised ML to allow predictions\\nof single ion solvation energies for monovalent and divalentcations and anions based on physically rigorous quasi-chemical\\ntheory.322,323 Ref 324 used convolutional NNs and MD\\nsimulations to carry out high-throughput screening of mixed\\nsolvent systems. Ref325 implemented eﬃcient ways to carryout ML-based QM/MM MD simulations.\\n2.5. Insightful Predictions for Molecular and Material\\nProperties\\nBy solving for electronic structures, by whatever means is\\nappropriate, one obtains molecular energies and energyspectrum (typically corresponding toquasiparticles given by\\nKS or HF orbitals). From these, one can then computeappropriate, one obtains molecular energies and energy\\nspectrum (typically corresponding toquasiparticles given by\\nKS or HF orbitals). From these, one can then compute\\nmolecular or material properties that arise from quantummechanical and statistical operators, for example, thermody-\\nnamic energies, response properties, highest and lowest\\noccupied molecular orbital energies, and band gaps, among\\nother properties. Many properties are deﬁned by the charactersof the orbitals, and having knowledge of these should always be\\nhelpful and aid in deriving useful insight into designing\\nmolecules and materials for a particular function. Furthermore,\\none is often interested in how these molecules behave overtime (i.e., the dynamics given some statistical ensemble that\\ndepends on temperature, pressure, etc) over all possible\\ndegrees of freedom. By understanding how energies and forces\\nchange over time, one can predict thermal and pressuredependencies as well as spectroscopic properties for advanced\\nknowledge that builds toward insightful predictions.\\nMolecular and materials chemistry is vastly complex and\\nvariable, and one often faces a question of whether to spanwider chemical spaces versus take deeper explorations of a\\nspeciﬁc phenomenon. A key problem is that even after the\\neﬀort of either approach, it is also not as clear how information\\nfor one system might be related to another to provide moreknowledge. For instance, one may decide to calculate all\\npossible properties of ethanol with a CompChem method, but\\nunderstanding how any calculated property would be\\ncorrelated to an analogous property of isopropanol is stillusually diﬃcult to do. There is great interest in understanding\\nchemical and materials space through applications of\\nquantitative structure activity/property relationships,326,327\\ncheminformatics, 328 conceptual DFT, 329 and alchemicalperturbation DFT. 330 All these applications bene ﬁt from\\ngreater access to CompChem data, and all have promise as\\nbeing interfaced with ML for transformative applications to\\ncatalyze wisdom and impact.\\n3. MACHINE LEARNING TUTORIAL AND3. MACHINE LEARNING TUTORIAL AND\\nINTERSECTIONS WITH CHEMISTRY\\nML has had a dramatic impact on many aspects of our daily\\nlives and has arguably become one of the most far-reaching\\ntechnologies of our era. It is hard to overstate its importance insolving long-standing computer science challenges, such as\\nimage classi ﬁcation331−334 or natural language process-\\ning,335−339 tasks that require knowledge that is hard to capture\\nin a traditional computer program.340−342 Previous classicalartiﬁcial intelligence (AI) approaches relied on very large sets\\nof rules and heuristics, but these were unable to cover the full\\nscope of these complex problems. Over the past decade,\\nChemical Reviews pubs.acs.org/CR ReviewChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9830advances in ML algorithms and computer technology made it\\npossible to learn underlying regularities and relevant patterns\\nfrom massive data sets that enable automatic constructions of\\npowerful models that can sometimes even outperform humansat those tasks.\\nThis development inspire d researchers to approach\\nchallenges in science with the same tools, driven by the hope\\nthat ML would revolutionize their respectiveﬁelds in a similar\\nway. Here, we give an overview of these developments inchemistry and physics to serve as an orientation for newcomers\\nto ML. We willﬁrst explain what tasks ML is good at and when\\nit might not be the best solution to a problem. We will start by\\nintroducing theﬁeld of ML in general terms and dissect itsstrengths and weaknesses.\\n3.1. What is ML?\\nIn the most general sense, ML algorithms estimate functional\\nrelationships without being given any explicit instructions of\\nhow to analyze or draw conclusions from the data. Learningalgorithms can recover mappings between a set of inputs and\\ncorresponding outputs or just from the inputs alone. Without\\noutput labels, the algorithm is left on its own to discover\\nstructure in the data.structure in the data.\\nUniversal approximators343,344 are commonly used for that\\npurpose. These reconstruct any function that fulﬁlls a few basic\\nproperties, such as continuity and smoothness, as long asenough data is available. Smoothness is a crucial ingredient\\nthat makes a function learnable, because it implies that\\nneighboring points are correlated in similar ways. That\\nproperty means that one can draw successful conclusionsabout unknown points as long as they are close to the training\\ndata (coming from the same underlying probability distribu-\\ntion).341 In contrast, completely random processes in the\\nabove sense allow no predictions.above sense allow no predictions.\\nAn association that immedi ately springs to mind is\\ntraditional regression analysis, but ML goes a step further.Figure 6.Supervised learning algorithms have to balance two sources of error during training: the bias and variance of the model. A highly biasedmodel is based onﬂawed assumptions about the problem at hand (under-ﬁtting). Conversely, a high variance causes a model to follow smallvariations in the data too closely, therefore making it susceptible to picking up random noise (overﬁtting). The optimal bias-variance trade-oﬀminimizes the generalization error of the model, for example, how well it performs on unknown data. It can be estimated with cross-validation\\ntechniques.\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107https://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9831Regression analyses aim to reconstruct the function that goes\\nthrough a set of known data points with the lowest error, but\\nML techniques aim to identify functions to predict\\ninterpolations between data points and thus minimize theprediction error for new data points that might later appear.345\\nThose contrasting objectives are mirrored in the di ﬀerent\\noptimization targets. In traditional regression, the optimization\\ntask\\nÄ\\nÇ\\nÅÅÅÅÅÅÅÅÅÅÅ\\nÉ\\nÖ\\nÑÑÑÑÑÑÑÑÑÑÑ\\nf fy xarg min ( ( ), )É\\nÖ\\nÑÑÑÑÑÑÑÑÑÑÑ\\nf fy xarg min ( ( ), )\\nf i\\nM\\ni i3\\n-\\n∑̂ =\\n∈ (21)\\nonly measures the ﬁt to the data, but learning algorithms\\ntypically aim toﬁnd models f̂ that satisfy\\nÄ\\nÇ\\nÅÅÅÅÅÅÅÅÅÅÅ\\nÉ\\nÖ\\nÑÑÑÑÑÑÑÑÑÑÑ\\nf fy xarg min ( ( ), )\\nf i\\nM\\ni i\\n23\\n-\\n∑̂ =+ ∥ Γ Θ ∥f i\\nM\\ni i\\n23\\n-\\n∑̂ =+ ∥ Γ Θ ∥\\n∈ (22)\\nBoth optimization targets reward a closeﬁt, often using the\\nsquared loss ff xy x y(() , ) (() ) 23 ̂ = ̂ − . However, the key\\ndiﬀerence is an additionalregularization term in eq 22, whichinﬂuences the selection of candidate models by introducing\\nadditional properties that promote generalization. To under-\\nstand why this is necessary, it is helpful to consider thateq 22\\nis only a proxy for the optimization problem\\nÄ\\nÇ\\nÅÅÅÅÅÅÅ\\nÉ\\nÖÄ\\nÇ\\nÅÅÅÅÅÅÅ\\nÉ\\nÖ\\nÑÑÑÑÑÑÑf fy pxxarg min ( ( ), )d ( , y)\\nf\\ni i3\\n-\\n∫̂ =\\n∈ (23)\\nthat we would actually like to solve. In an ideal world, we\\nwould minimize the loss function over thecomplete distributionof inputs and labels p(x, y). However, this is obviously\\nimpossible in practice, so we apply the principle of Occam’s\\nrazor that presumes that simpler (parsimonious) hypothesesa r em o r el i k e l yt ob ec o r r e c t .W i t ht h i sa d d i t i o n a l\\nconsideration we hope to be able to recover a reasonably\\ngeneral model, despite only having seen aﬁnite training set. Acommon way to favor simpler models is via an additional term\\nin the cost function, which is what∥ΓΘ∥2 in eq 22 expresses.\\nHere, Γ is a matrix that deﬁnes “simplicity” with regard to the\\nmodel parameters Θ. Usually, IλΓ= (where I is the identitymatrix andλ > 0) is chosen to simply favor a small L2-norm on\\nthe parameters, such that the solution does not rely on\\nindividual input features too strongly. This particular approach\\nis called Tikhonov regularization,346−348 but other regulariza-tion techniques also exist.349,350\\nA model that is heavily regularized (i.e., using a largeλ) will\\neventually become biased in that it is too simplistic toﬁt the\\ndata well. In contrast, a lack of regularization might yield anoverly complex model with highvariance. Such an“overly ﬁt”\\nmodel will follow the data exactly to the point that it also\\nmodels the noise components and consequently fails to\\ngeneralize (seeFigure 6). Finding the appropriate amount ofregularization λ to manage under- and overﬁtting is known as\\nattaining a goodbias-variance trade-off.351 We will introduce a\\nprocess called cross-validation to address this challenge further\\nbelow (see section 3.4.3).below (see section 3.4.3).\\n3.1.1. What Does ML Do Well?Implicit Knowledge from\\nData. ML algorithms can infer functional relationships from\\ndata in a statistically rigorous way without detailed knowledgeabout the problem at hand. ML thus captures implicit\\nknowledge from a data set−even aspects where CPI might\\nnot be available. Traditional modeling approaches, such as the\\nclassical force ﬁelds discussed in section 2.2.6 ,r e l yo npreconceived notions about the PES that is being modeled\\nand, thus, the way the physical system behaves. In contrast, ML\\nalgorithms start from a loss function and a much more general\\nmodel class. Within the limits permitted by the noise inherentto the data, generalization can be improved to arbitrary\\naccuracy given increasingly larger informative training data\\nsets. This process allows us to explore a problem even before\\nthere is a reasonably full understanding. An ML predictor canserve as a starting point for theory building and be regarded as\\na versatile tool in the modeling loop: building predictive\\nmodels, improving them, enriching them by formal insight, andserve as a starting point for theory building and be regarded as\\na versatile tool in the modeling loop: building predictive\\nmodels, improving them, enriching them by formal insight, and\\nimproving further and ultimately extracting a formal under-standing. More and more research eﬀorts start to combine\\ndata-driven learning algorithms with rigorous scienti ﬁco r\\nengineering theory to yield novel insights and applica-\\ntions.10,16,352\\nRedundancy in CompChem Calculations.For a quantumRedundancy in CompChem Calculations.For a quantum\\nchemical property for compounds in a data set, CompChem\\ncalculations need to be repeated independently for each input,\\neven if they are very similar. No formally rigorous methodexists to exploit redundancies in the calculations in such a\\nscenario. The empiricism of learning algorithms however does\\nprovide a pathway to extract information based on compound\\nstructure similarity. A data-driven angle allows one to askquestions in new ways that give rise to new perspectives on\\nestablished problems. For example, unsupervised algorithms\\nlike clustering or projection methods group objects according\\nto latent structural patterns and provide insights that wouldremain hidden when only looking at individual compounds.\\n3.1.2. What Does ML Do Poorly?Lack of Generality\\nand Precision. Some diﬃcult problems in chemistry and\\nphysics can be solved accurately with CompChem, but doingso would require signiﬁcant resources. For example, enumerat-\\ning all pairwise interactions in a many-body system will\\ninevitably scale quadratically, and there is no obvious path\\naround this. One might ask if empirical approaches can addresssuch fundamental problems more e ﬃciently, but this is\\nunfortunately not possible since ML is more suited forﬁnding\\nsolutions in general function spaces rather than in\\ndeterministic algorithms where constraints guide the solutionprocess. However, if we were not as interested inﬁnding a full\\nsolution but rather some aspect of it, the stochastic nature of\\nML can be beneﬁcial. For instance, a traditional ML approach\\nmight not be the best tool for explicitly calculating theSchrödinger equation, but it might be a far more useful tool for\\ndeveloping a force ﬁeld that returns the energy of a system\\nwithout the need for a cumbersome wavefunction and a self-\\nconsistent algorithm. As an example, Hermann et al.105 useddeep NNs to show how ML methods may be suitable for\\novercoming challenges faced by traditional CompChem\\napproaches.\\nReliance on High-Quality Data.ML algorithms require a\\nlarge amount of high quality data, and it is hard to decide apriori when a data set is suﬃcient. Sometimes, a data set may\\nbe large, but it does not adequately sample all the relevant\\nsystems one intends to model. For example, an MD simulation\\nmight generate many thousands of molecular conﬁrmationsused to train an ML forceﬁeld, but perhaps that sampling only\\noccurred in a local region of the PES. In this case, the ML force\\nﬁeld would be eﬀective at modeling regions of the PES it was\\ntrained to but useless in other regions until more data andbroader sampling occurred. This feature is general to all\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9832empirical models that are generally limited in their\\nextrapolation abilities.\\nInability to Derive High-Level Concepts. Standard ML\\nalgorithms cannot conceptualize knowledge from a data set.\\nTwo main reasons are the nonlinearity and excessiveparametric complexity of most models that allow many equally\\nviable solutions for the same problem.353,354 It can be hard to\\ngain insight into the modeled relationship because it is not\\nbased on a small set of simple rules. Techniques have emergedto make ML models interpretable (explainable AI−XAI355).\\nWhile helpful, drawing scientiﬁc insight clearly still requires\\nhuman expertise.352,355−361 Furthermore, the path from an ML\\nmodel back to a physical set of equations is being explored, butit is far from being fully established automatically.362−368\\nProne to Artifacts. Despite following the rules of best\\npractice, ML algorithms can give unexpected and undesired\\nresults. Instead of extracting meaningful relationships, theymay occasionally exploit nuisance patterns within the under-\\nlying experimental design, like the model architecture, the loss\\nfunction or artifacts in the data set. This results in a“clever\\nHans” predictor,360 which technically manages the learningproblem but uses a trivial solution that is only applicable within\\nthe narrow scope of the particular experimental setup at hand.\\nThe predictor will appear to be performing well, while actuallyharvesting the wrong information and, therefore, not allowing\\nany generalization or transferable insights.\\nFor example, a recently proposed random forest predictor\\nfor the success of Buchwald−Hartwig coupling reactions369was later revealed to give almost the same performance when\\nthe original inputs were replaced by Gaussian noise.370,371 This\\nﬁnding strongly suggested that the ML algorithm exploited\\nsome hidden underlying structure in the input data,irrespective of the chemical knowledge that was provided\\nthrough the descriptor. Even though the model might appear\\nquite useful, any conclusions that rely on the importance of the\\nchemical features used in the model were thus renderedquestionable at best. This example demonstrates that out-of-\\nsample validation alone is often not suﬃcient to establish that a\\nproposed model has indeed learned something meaningful.\\nTherefore, the hypothesis described by the model must bechallenged in extensive testing in practically relevant scenarios\\nlike actual physical simulations. In other words the ML model\\nneeds to lead to a better understanding of the modeling itself\\nand the underlying chemistry.\\n3.2. Types of Learning3.2. Types of Learning\\nML models are classiﬁed by the type of learning problem they\\nsolve. Consider for instance a data scientist who develops an\\nML model that can predict acidity constants (pKa values) forany molecule. A researcher with knowledge of physical organic\\nchemistry might be aware of the empirical Taft equation29 that\\nprovides a linear free energy relationship between molecules\\non the basis of empirical parameters that account for amolecule’s fundamental ﬁeld, inductive, resonance, and steric\\neﬀects (e.g., values related to Hammettρ and σ values). There\\nare several ways the data scientist might develop an ML model\\nfor this or another application. Examples mentioned hereinclude supervised, unsupervised, and reinforcement learning.\\n3.2.1. Supervised Learning.Supervised learningaddresses\\nlearning problems where the ML modelf :\\nML\\n?@̂ ⎯→⎯ connects\\na set of known inputs? and outputs @, either to perform aregression or classiﬁcation task. While the former maps onto a\\ncontinuous space (e.g., energy, polarizability), the latter\\noutputs a categorical value (e.g., acid or base; metal or\\ninsulator) for each data point.insulator) for each data point.\\nUsing the p Ka predictor example, a supervised learning\\nalgorithm could be trained to correlate recognizable chemical\\npatterns or structures to experimentally known pKa values. Thegoal would be to deduce the relationship between these inputsalgorithm could be trained to correlate recognizable chemical\\npatterns or structures to experimentally known pKa values. The\\ngoal would be to deduce the relationship between these inputs\\nand outputs, such that the model is able to generalize beyondthe known training set. A standard universal approximator has\\nto accomplish this learning task without any preconceived\\nnotion about the problem at hand and will, therefore, likely\\nrequire many examples before it can make accurate predictions.Recently, a lot of research is being carried out that investigates\\nways to incorporate high-level concepts into the learning\\nalgorithm in the form of prior knowledge.207,372 In this vein,\\none could take into account chemically relevant parameters,such as Hammett constants so that the parametrized ML\\nmodel incorporates the modiﬁed Hammett or Taft equation.\\nAn example of a classiﬁcation problem in materials science is\\nthe categorization of materials, where identifying character-istics of the electronic structure can be used to distinguish\\nbetween insulators and metals.373\\n3.2.2. Unsupervised Learning. Unsupervised learning\\ndescribes problems in which only the inputs are known, withno corresponding labels. In this setting, the goal is to recover\\nsome of the underlying structure of the data to gain a higher-\\nlevel understanding. Unsupervised learning problems are not as\\nrigorously deﬁned as supervised problems in the sense thatthere can be multiple correct answers, depending on the model\\nand objective function that is applied.\\nFor example, one might be interested in separating\\nconformers of a molecule from an MD trajectory, givenexclusively the positions of the atoms. Aclustering algorithm\\n(like the k-means algorithm) could identify those conformers\\nby grouping the data based on common patterns.374,375\\nAlternatively, a projection technique could reveal a low-dimensional representation of the data set.376 Often data is\\nrepresented in high dimension, despite being intrinsically low-\\ndimensional. With the right projection technique, it is possibleto retain the meaningful properties in a representation with\\nfewer degrees of freedom. A conceptually simple embedding\\nmethod is principal component analysis (PCA) in which the\\nrelationship that is sought to be preserved is the scalar productbetween the data points.340 There are many other linear and\\nnonlinear projection methods , such as multidimensional\\nscaling,377 kernel PCA (KPCA),378,379 t-distributed stochastic\\nneighbor embedding (t-SNE), 380 sketch-map,381 and theuniform manifold approximation and projection (UMAP).382\\nFinally, anomaly detection is another extension of unsuper-\\nvised learning, where’outliers’ to the available data can be\\ndiscovered.383 However, without knowing the labels (in thisexample, the potential energy associated with each geometry),\\nthere is no way to conclusively verify that the result is correct.\\nT h el i t e r a t u r ei sg r a d u a l l ys e e i n gm o r ei n s t a n c e so funsupervised learning, particular to reveal important chemical\\nproperties to eﬃciently explore chemical/materials spaces.\\n3.2.3. Reinforcement Learning. Reinforcement learning\\n(RL) describes problems that combine aspects of supervisedand unsupervised learning. RL problems often involve deﬁning\\nan agent within an environment that learns by receiving\\nfeedback in the form of punishments and rewards. The\\nprogress of the agent is characterized by a combination ofexplorative activity and exploitation of already gathered\\nknowledge.384 For chemistry applications, RL techniques are\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9833being increasingly used for ﬁnding molecules with desired\\nproperties in large chemical spaces.10\\n3.3. Universal Approximators\\nUniversal approximators have their origins in the 1960s, where\\nthe hope was to construct“learning machines” that havesimilar capabilities as the human brain. An early mathematical\\nmodel of a single simpliﬁed neuron emerged that was called a\\nperceptron (eq 24).385,386\\ni\\nk\\njjjjjj\\ny\\n{\\nzzzzzzf wx bx() s i g n\\ni\\nN\\nii\\n1\\n∑=−\\n= (24)y\\n{\\nzzzzzzf wx bx() s i g n\\ni\\nN\\nii\\n1\\n∑=−\\n= (24)\\nHere, x denotes theN-dimensional input to the perceptron. It\\nhas N + 1 parameters consisting ofwi (so-called weights) and a\\nsingle b (a so-called threshold) that are adapted to the data.This adaption process is typically called“learning” (vide infra),\\nand it amounts to minimizing a predeﬁned loss function.\\nIn the 1960s, this simple NN had very limited use, as it was\\nonly able to model a linear separating hyperplane. Even simplenonlinear functions like the XOR were out of reach.387 Thus,\\nexcitement waned but then reappeared two decades later with\\nthe emergence of novel models consisting of more neurons and\\ntheir arrangement in multilayer NN structures388 (see eq 25).Recent algorithmic and hardware advances now allow deep\\nand increasingly complex architectures.1,2\\ni\\nk\\njjjjjjj\\nÄ\\nÇ\\nÅÅÅÅÅÅÅÅÅÅÅ\\nÉ\\nÖ\\nÑÑÑÑÑÑÑÑÑÑÑ\\ny\\n{\\nzzzzzzzf ww x bx() g gk\\nj\\nH\\nkj\\ni\\nN\\nji i\\n11\\n∑∑=−\\n== (25)j\\nH\\nkj\\ni\\nN\\nji i\\n11\\n∑∑=−\\n== (25)\\nIn eq 25, g(·) denotes an activation function that is a nonlinear\\ntransformation that allows complex mappings between input\\nand output. As with the perceptron, the parameters ofmultilayer NNs can be learned eﬃciently using iterative\\nalgorithms that compute the gradient of the loss-function using\\nthe so-called back-propagation (BP) algorithm.388−390 In the\\nlate 1980s, artiﬁcial NNs were then proven to be universalapproximators of smooth nonlinear functions,343,391,392 and so\\nthey gained broad interest even outside the ML community\\nthat then was still relatively small.\\nIn 1995, a novel technique called Support Vector Machine(SVM)345,393 and kernel-based learning were then pro-\\nposed,379,394−396 which came with some useful theoretical\\nguarantees. SVMs implement a nonlinear predictor:\\nf yK bxx x() ( , )\\nj\\nN\\nj jj\\n1\\n∑ α=−\\n= (26)f yK bxx x() ( , )\\nj\\nN\\nj jj\\n1\\n∑ α=−\\n= (26)\\nwhere K is the so-called kernel. The kernel implicitly deﬁnes an\\ninner product in some feature space and thus avoids an explicit\\nmapping of the inputs. This“kernel trick”397 makes it possibleto introduce nonlinearity into any learning algorithm that can\\nbe expressed in terms of inner products of the input.379 It has\\nsince been applied to many other algorithms beyond SVMs,394\\nsuch as Gaussian Processes (GP), 348 PCA,378,379 andindependent component analysis (ICA).398\\nThe most e ﬀective kernels are tailored to the speci ﬁc\\nlearning task at hand, but there are many generic choices, such\\nas the polynomial kernelK(xj, x)=( ⟨xj, x⟩ − b)d, whichdescribes inner products between degree d polynomials.\\nAnother popular choice is the Gaussian kernel K(xj, x)=\\nexp(−(xj − x)2/(2σ2)). It is one of the most versatile kernels\\nbecause it only imposes smoothness assumptions on thesolution depending on the width parameterσ.347,395\\nAs seen in eq 26, an SVM can also be understood as a\\nshallow NN with aﬁxed set of nonlinearities. In other words,\\nthe kernel explicitly deﬁnes a similarity metric to compare datapoints, whereas NNs have more freedom to shape this\\ntransformation during training because they nest parametriz-\\nable nonlinear transformations on multiple scales. This\\ndiﬀerence gives both techniques unique strengths anddrawbacks. Despite that, there exists a duality between both\\napproaches that allows NNs to be translated into kernel\\nmachines and analyzed more formally (see refs399−401).\\nIn the context of CompChem, both NNs and kernel-basedmethods are the most used ML approaches. Simpler learners,\\nsuch as nearest neighbor models or decision trees can still be\\nsurprisingly eﬀective. Those have also been successfully used to\\nsolve a wide spectrum of problems including drug design,such as nearest neighbor models or decision trees can still be\\nsurprisingly eﬀective. Those have also been successfully used to\\nsolve a wide spectrum of problems including drug design,\\nchemical synthesis planning, and crystal structure classiﬁca-tion.402−407\\n3.4. ML Workﬂow\\nIn the following, we summarize the overall ML process,\\nstarting from a data set all the way to a trained and tested\\nmodel. The ML workﬂow typically includes the following\\nstages:\\n1 Gathering and preparing the datastages:\\n1 Gathering and preparing the data\\n2 Choosing a representation\\n3 Training the model\\n3a Train model candidates\\n3b Evaluate model accuracy\\n3c Tune hyperparameters\\n4 Testing the model out of sample4 Testing the model out of sample\\nNote, that the progression to a good ML model is not\\nnecessarily linear and some steps (except the out of sample\\ntest) may require reiteration as we learn about the problem at\\nhand.hand.\\n3.4.1. Data Sets. On a fundamental level, ML models\\ncould be simply regarded as sophisticated parametrizations of\\ndata sets. While the architectural details of the model matter,\\nthe reference data set forms the backbone that ultimatelydetermines the model’se ﬀectiveness. If the data set is not\\nrepresentative of the problem at hand, the model will be\\nincomplete and behave unpredictably in situations that have\\nbeen improperly captured. The same applies to any othershortcomings of the data set, such as biases or noise artifacts\\nthat will also be reﬂected in the model. Some of these data set\\nissues are likely to remain unnoticed when following the\\nstandard model selection protocol since training and test datasets are usually sampled from the same distribution. If the\\nsampling method is too narrow, errors seen during the cross-\\nvalidation procedure may appear to be encouragingly small,\\nbut the ML model will fail catastrophically when applied to areal problem. If the training and test sets come from diﬀerent\\ndistributions, then techniques to compensate this covariate\\nshift can be used.408,409\\nRobust models can generally only be constructed fromcomprehensive data sets, but it is possible to incorporate\\ncertain patterns into models to make them more data-eﬃcient.\\nPrior scientiﬁc knowledge or intuition about speciﬁc problems\\ncan be used to reduce the function space from which an MLalgorithm has to select a solution. If some of the unphysical\\nsolutions are removed a priori, less data are necessary to\\nidentify a good model. This is why NNs and kernel methods,\\nChemical Reviews pubs.acs.org/CR ReviewChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9834despite both being broad universal function classes, bring\\ndiﬀerent scaling behaviors. The choice of the kernel function\\nprovides a direct way to include prior knowledge such as\\ninvariances, symmetries, or conservation laws, whereas NNsare typically used if the learning problem cannot be\\ncharacterized as speci ﬁcally.207,372,410 In general, without\\nprior knowledge, NNs often require larger data sets to produce\\nthe same accuracy as well-constrained kernel methods thatembody problem knowledge. This consideration is particularly\\nimportant if the data is expensive, for example, if it comes from\\nhigh quality experiments or expensive computations.\\n3.4.2. Descriptors.To apply ML, the data set needs to beencoded into a numerical representation (i.e., features/\\ndescriptors) that allows the learning algorithm to extract\\nmeaningful patterns and regularities.411−419 This is particularly\\nchallenging for unstructured data like molecular graphs thathave well-deﬁned invariable or equivariable characteristics that\\nare hard to capture in a vectorial representation. For example,\\natoms of the same type are indistinguishable from each other,but it is hard to represent them without imposing some kind of\\norder (which inevitably assigns an identity to each atom).\\nFurthermore, physical systems can be translated and rotated in\\nspace without aﬀecting many attributes. Only a representationthat is adapted to those transformations can solve the learning\\nproblem eﬃciently.\\nIt turned out to be a major challenge to reconcile all\\ninvariances of molecular systems in a descriptor withoutsacriﬁcing its uniqueness or computability. Some representa-\\ntions cannot avoid collisions, where multiple geometries map\\nonto the same representation. Others are unique, but\\nprohibitively expensive to generate. Many solutions to thisproblem have been proposed, based on general strategies such\\nas invariant integration, 207 parameter sharing, 352,421−423\\ndensity representations,276 or ﬁnger printing techniques.424−433\\nAlternatively, an NN model infers the representation fromdata.352,424,434,435 To date, none of the proposed approaches\\nare without compromise, which is why the optimal choice of\\ndescriptor depends on the learning task at hand.\\n3.4.3. Training. The training process is the key step thatties together the data set and model architecture. Through the\\nchoice of the model architecture, we implicitly de ﬁne a\\nfunction space of possible solutions, which is then conditioned\\non the training data set by selecting suitable parameters. Thisoptimization task is guided by a loss function that encodes our\\ntwo somewhat opposing objectives: (1) achieving a goodﬁtt o\\nthe data, while (2) keeping the parametrization general enough\\nsuch that the trained model becomes applicable to data that isnot covered in the training set (see the two terms ineq 22).\\nSatisfying the latter objectives involves a process calledmodel\\nselection in which a suitable model is chosen from a set of\\nvariants that have been trained with exclusive focus on theﬁrstobjective. Depending on the model architecture, more or less\\nsophisticated optimization algorithms can be applied to train\\nthe set of model candidates.\\nKernel-based learning algorithms are typically linear in theirparameters α⃗ (see eq 26). Coupled with a quadratic loss\\nfunction, ff xx(() , y ) (() y ) 23 ̂ = ̂ − ,t h e yy i e l dac o n v e x\\noptimization problem. Convex problems can be solved quickly\\nand reliably due to only having a single solution that isguaranteed to be globally optimal. This solution can be found\\nalgebraically by taking the derivative of the loss function and\\nsetting it to zero. For example, kernel ridge regression (KRR)\\nand GPs then yield a linear system of the formand GPs then yield a linear system of the form\\nf xK(() , y ) ( I ) y 03 λ α∇ ̂ =+ − =α (27)\\nwhich is typically solved in a numerically robust way by\\nfactorizing the kernel matrixK. There exist a broad spectrumof matrix factorization algorithms, such as the Choleskywhich is typically solved in a numerically robust way by\\nfactorizing the kernel matrixK. There exist a broad spectrum\\nof matrix factorization algorithms, such as the Cholesky\\ndecomposition , that exploit the symmetry and positivedeﬁniteness properties of kernel matrices.436−440 Factorization\\napproaches are, however, only feasible if enough memory is\\navailable to store the matrix factors, and this can be a limitationfor large-scale problems. In that case, numerical optimization\\nalgorithms provide an alternative: they take a multistep\\napproach to solve the optimization problem iteratively by\\nfollowing the gradient:following the gradient:\\n´≠ÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖ ÆÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖf x(( ) , y )tt\\nK\\n1\\ne.g.,( I) y t 1\\n3γαα=− ∇ ̂\\nλ\\nα\\nα\\n−\\n+− − (28)\\nwhere γ is the step size (or learning rate). Iterative solversfollow the gradient of the loss function until it vanishes at a\\nminimum, which is much less computationally demanding per\\nstep, because it only requires the evaluation of the modelf̂.I n\\nparticular, kernel models can be evaluated without storingK(see eq 28).\\nNNs are constructed by nesting nonlinear functions in\\nmultiple layers, which yields nonconvex optimization prob-\\nlems. Closed-form solutions similar to eq 27 do not exist,\\nwhich means that NNs can only be trained iteratively, that is,analogous to eq 28. Several variants of this standard gradient\\ndescent algorithm exist including stochastic or mini-batch\\ngradient descent, where only ann-sized portion of the training\\ndata (x,y)i:i+n is considered in every step. Because of multiplelocal minima and saddle points on the loss surface, the global\\nminimum is exponentially hard to obtain (since these\\nalgorithms usually converge to a local minimum). However,\\nthanks to the strong modeling power of NNs, local solutionsare usually good enough.441\\nHyperparameters. In addition to the parameters that are\\ndetermined whenﬁtting an ML model to the data set (i.e., the\\nnode weights/biases or regression coeﬃcients), many modelscontain so-called hyperparameters that need to beﬁxed before\\ntraining. Two types of hyperparameters can be distinguished:\\nones that inﬂuence the model, such as the type of kernel or the\\nNN architecture, and ones that a ﬀect the optimizationalgorithm, for example, the choice of regularization scheme\\nor the aforementioned learning rate. Both tune a given model\\nto the prior beliefs about the data set and thus play a signiﬁcant\\nrole in model eﬀectiveness. Hyperparameters can be used togauge the generalization behavior of a model.\\nHyperparameter spaces are often rather complex: certain\\nparameters might need to be selected from unbounded value\\nspaces, others could be restricted to integers or haveinterdependencies. This is why they are usually optimized\\nusing primitive exhaustive search schemes like grid or random\\nsearches in combination with educated guesses for suitable\\nsearch ranges. Common gradient-based optimization methodstypically cannot be appliedfor this task. Instead, the\\nperformance of a given set of hyperparameters is measured\\nby evaluating the respective model on another training data set\\ncalled the validation data set (seeFigure 6). This process isalso referred to as model selection.\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9835Model Selection. Cross-validation or out-of-sample testing\\nis a technique to assess how a trained ML model will generalize\\nto previously unseen data.340,395 For a reasonably complex\\nmodel, it is typically not challenging to generate the rightresponses for the data known from the training set. This is why\\nthe training error is not indicative of how the model will fulﬁll\\nits ultimate purpose of predicting responses for new inputs.Alas, since the probability distribution of the data is typically\\nunknown, it is not possible to determine this so-called\\ngeneralization errorexactly. Instead, this error is often estimatedusing an independent test subset that is held back and later\\npassed through the trained model to compare its responses to\\nthe known test labels. If the model suﬀers from overﬁtting onthe training data, this test will yield large errors. It is important\\nto remember not to tweak any parameters in response to these\\ntest results, as this will skew this assessment of the modelperformance and will lead to overﬁtting on the test set.442\\nBesides cross-validation, there are alternative ways to\\nestimate the generalization error, for example via maximization\\nof the marginal likelihood in Bayesian inference.443−445 Somewell-deﬁned learning scenarios even allow the computation of\\nrigorous upper bounds for the generalization error.345,446−448\\n4. APPLICATIONS OF MACHINE LEARNING TO\\nCHEMICAL SYSTEMS\\nWe now discuss ways that CompChem methods described insection 2and ML methods insection 3can be implemented as\\nCompChem+ML approaches for insights into chemical\\nsystems. We often notice the lack of details aboutwhy an\\nML model is used and how it actually contributes toworthwhile and scientiﬁc insights. Thus, we will summarize\\nthe underlying attributes of conventional CompChem+ML\\neﬀorts and then explain why these attributes are important for\\nspeciﬁc applications.speciﬁc applications.\\nTo begin, consider molecules or materials in a data set, and\\nany entry will be related to another based on an abstract\\nconcept of“similarity”. While similarity is an application-dependent concept, it should go hand in hand with CPI. For\\ninstance, physical properties of chemical systems can be\\nattributed to the structure or composition of the chemical\\nfragments within those systems. Thus, if chemical structuresand compositions of two entries in the database were similar,\\nthen their physical properties would also likely be similar.\\nFor CompChem+ML using a supervised algorithm, a\\nCompChem prediction might be made on a hypotheticalsystem, pinpointed by an ML model that was trained to\\nidentify chemical fragments that correlate with labeled physical\\nproperties. This would be a direct exploitation of chemical\\nsimilarity. Alternatively, for CompChem+ML using anunsupervised algorithm, the ML model would identify an\\nunderlying distribution or key features based on the similarity\\nbetween pairs of entries in the data set without labels. This\\nwould be a more nuanced leveraging of chemical similarity. Inboth cases the accuracy, eﬃciency and reliability of the ML\\nmodels depend strongly on how similarity is de ﬁned and\\nmeasured.\\nIn this section, we will ﬁrst describe state-of-the-art\\ndescriptors and kernels for atomic systems that can be usedto quantify the similarity between chemical systems. We will\\nthen explain the essential attributes of good atomic descriptors.\\nTable 4. ML Descriptors Found in the Literaturea\\ninvariancesdinvariancesd\\ndescriptors comp. e ﬃciencyb periodicc unique TRP global smooth e\\natom-centered symmetry functions (ASCF)411 Ⓑ 1,2,3-body terms, cutoﬀ √ X √√√ X √\\nsmooth overlap of atomic positions (SOAP)412 Ⓑ density based, SO(3) rotational groupintegration\\n√ X √√√ X √\\nCoulomb matrix (CM)413 Ⓐ 1,2-body terms X √√ √ X √√\\nsine matrix414 Ⓐ 1,2-body terms √√ √ √ X √√\\nEwald sum matrix414 Ⓐ 1,2-body terms √√ √ √ X √√\\nbag of bonds (BoB)415 Ⓐ 1,2-body terms X X √√ ○ √ XFaber−Christensen−Huang−Lilienfeld (FCHL)416 Ⓒ 1,2,3-body terms √ X √√√ X √Ewald sum matrix414 Ⓐ 1,2-body terms √√ √ √ X √√\\nbag of bonds (BoB)415 Ⓐ 1,2-body terms X X √√ ○ √ X\\nFaber−Christensen−Huang−Lilienfeld (FCHL)416 Ⓒ 1,2,3-body terms √ X √√√ X √\\nspectrum of London and Axilrod−Teller−Muto potential\\n(SLATM)417(SLATM)417\\nⒹ 1,2,3,4-body terms √ X √√√ X √\\nmany-body tensor representation (MBTR)418 Ⓒ 1,2,3-body terms X X √√√ √ √\\natomic cluster expansion420 Ⓐ 1,2-body terms √ X √√√ √ √invariant many-body interaction descriptor (MBI)460 Ⓑ 1,2,3-body terms X X √√√ X √\\nneural network architectures\\ndeep potentialsmooth edition (DeepPot-SE)461,462 Ⓑ 1,2,3-body terms, cutoﬀ √ X √√√ X √MPNN, SchNet352,434 Ⓐ/Ⓑ 1,2-body terms, hierarchical √ X √√√ X √\\nCormorant463 Ⓑ 1,2-body terms, hierarchical X X √√√ X √\\ntensor ﬁeld networks464 Ⓑ 1,2-body terms √ X √√√ X √\\nsimilarity metrics\\nroot mean square deviation of atomic positions\\n(RMSD)454(RMSD)454\\nⒶ 1,2-body terms, input matching X X ○○ X √ X\\noverlap matrix454 Ⓐ 1,2-body terms, input matching X X √√√ √ X\\nREMatch459 Ⓒ 1,2-body terms, input matching X X √√√ √ X\\nsGDML207 Ⓐ 1,2-body terms √√ √ √ ○f √√sGDML207 Ⓐ 1,2-body terms √√ √ √ ○f √√\\na“√” = satisﬁes condition;“○” = partially satisﬁes condition;“X” = does not satisfy condition.bComputational eﬃciency ranks with gradesⒶ−Ⓓin descending order. The eﬃciency class reﬂects the extent that the descriptor requires expensive operations (e.g., a hierarchical processing ormatching of inputs).cDescriptor has been used within periodic boundary conditions.d“T” = translational;“R” = rotational;“P” = permutational.eIn this context, a descriptor is referred to as smooth if itsﬁrst derivative with respect to nuclear positions is continuous.fOnly invariant to\\npermutations represented in the training data.\\nChemical Reviews pubs.acs.org/CR ReviewChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9836Lastly for this section, we will elucidate why and how speciﬁc\\ncombinations of these descriptors and ML algorithms are\\nbeginning to revolutionize theﬁeld of CompChem.\\n4.1. Representing Chemical Systems\\nIn CompChem, molecules and materials are usuallyIn CompChem, molecules and materials are usually\\nrepresented by the Cartesian coordinates and the chemical\\nelements of all the atoms. Thus, the size of the vector\\nrepresentation containing the coordinates and charges will beandNN39A{ }, respectively, for a system of size N. Even\\nthough these atomic coordinates provide a complete\\ndescription of the system, they are hardly ever used as the\\ninput of a ML model because this vector would introducesubstantial superﬂuous redundancy. For instance, an ML\\nmodel might treat two identical molecules that are rotated or\\ntranslated as diﬀerent molecules, and that in turn might cause\\nthe ML model to predict diﬀerent physical properties for thetwo otherwise indistinguishable molecules. There are further\\ndiﬃculties when comparing molecules having di ﬀerent\\nnumbers of atoms. To work around these problems, atomic\\ncoordinates are usually converted into an appropriaterepresentationψ that is suitable for a particular task. Such\\nconversions are useful because they allow the incorporation of\\nphysical invariances. Mathematically speaking, the representa-\\ntion fulﬁlls\\nS(,)( (,) )NN NN339A 9Aψ ψ{} = {} (29)S(,)( (,) )NN NN339A 9Aψ ψ{} = {} (29)\\nwhere S indicates a symmetry operation, for example, a rigid\\nrotation about an axisCi, an exchange of two identical atoms,\\nor a translation of the whole system in the Cartesian space, etc.It can also be advantageous to adopt a coarse-grained\\nrepresentation of the system.449,450 For example, dihedral\\nangles of a peptide might be accounted for without the\\npositions of the side-chains, positions of ions in a solutionmight be accounted for without the explicit coordinates of\\nsolvents, or just the center of mass for a water molecule might\\nbe accounted for in place of the full three-centered atomistic\\nrepresentation. The choice of these coarse-grained representa-tions provides a way to incorporate prior knowledge of the\\ndata, or such representations can be learned from an\\nunsupervised learning step.451\\n4.1.1. Descriptors. Atomistic systems can be representedin a myriad of ways. Some descriptions are designed to\\nemphasize particular aspects of a system, while others aim to\\ndisambiguate similar chemical or physical principles across a\\nwide range of molecules or materials. The set of desirableproperties in a representation thus depends on the task at\\nhand. All adhere to the aforementioned physical symmetries\\nand invariances needed for chemical systems. Many have\\nsimilar theoretical foundations that can be understood as thebasis onto which the atomic density is projected,452 and the\\nconnection between them has been summarized in a recent\\nreview.453\\nTable 4 gives a coarse characterization of popular\\nrepresentations.276,411,412,415,417,418,454,455 To create this over-view, we had to adopt a reductionist perspective, which\\ninevitably hides the complexities involved in developing robust\\natomistic representations. Whether a representation satisﬁes a\\nparticular property can sometimes not be answered unequiv-ocally. For example, is a descriptor unique if the ML model\\nshowed pathologically erroneous results? Should a symmetry\\nbe perfectly satisﬁed, even if it is a bad ML feature? We\\ntherefore stress that the table simply presents representationsand their attributes. A representation that satis ﬁes more\\nattributes is not necessarily better if it also lacks another\\nimportant attribute. We kindly refer the reader to the\\nrespective original publications for more information.The descriptors in Table 4 can be classi ﬁed into two\\ncategories: global and atomic (i.e., not global). Traditional\\ndescriptors used in cheminformatics are global descriptors\\nbased on the covalent connectivity of atoms. These includesimple valence counting and common neighbor analysis,456 thedescriptors used in cheminformatics are global descriptors\\nbased on the covalent connectivity of atoms. These include\\nsimple valence counting and common neighbor analysis,456 the\\npresence or absence of predeﬁned atomic fragments (e.g., theMorgan ﬁngerprints427), pairwise distances between atoms\\n(e.g., Coulomb Matrix, 413 Sine Matrix, 414 Ewald Sum\\nMatrix,414 Bag of Bonds (BoB)415), etc. Coulomb matrices\\nhave known problems because of lack of smoothness, but theseare partly addressed by employing the Wasserstein norm,\\nrather than Euclidean or Manhattan norms. 457 However,\\natomic descriptors411,412,416−420,458 are generally more popular\\nthan the global ones in ML and CompChem. In atomicdescriptors, a chemical system is described as a set of atomic\\nenvironments, , ... ...iN1? ?? , and each consists of the atoms\\n(chemical species and position) within a sphere of radiusrcut\\ncentered at a speciﬁc atomi. One needs to combine the set ofatomic descriptors of all environments to construct a\\ndescriptor for the entire atomic structure. The most\\nstraightforward way to do this is to average the atomic\\ndescriptors,\\nA N() 1 ()\\niA\\nN\\ni\\nA\\nA\\n?∑ ψΦ=\\n∈ (30)descriptors,\\nA N() 1 ()\\niA\\nN\\ni\\nA\\nA\\n?∑ ψΦ=\\n∈ (30)\\nwhere the sum runs over allNA atoms i in structureA and i? is\\nthe environment around atom i. When there are multiple\\nchemical species, the descriptors for the local environments ofdiﬀerent species can either be included in the single sum, or\\nthe averaging can be performed for the environments of each\\nspecies separately and the species-speciﬁc averaged local\\ndescriptors can be concatenated. This can be done byconsidering the root mean square displacement (RMSD),454\\nthe best match between the environments of the two structures\\n(best-match),459 or by combining local descriptors using a\\nregularized entropy match (RE-Match).459regularized entropy match (RE-Match).459\\n4.1.2. Representing Local Environments.We will now\\ndescribe the Smooth Overlap of Atomic Positions (SOAP)\\ndescriptors412 since many other descriptors based on theatomic density are similar and diﬀer mainly by how the density\\nis projected onto basis functions.420,452 To construct SOAP\\ndescriptors, one ﬁrst considers an atomic environment? that\\ncontains only one atomic species, and a Gaussian function ofwidth σ is then placed on each atomi in ? to make an atomic\\ndensity function:\\nÄ\\nÇ\\nÅÅÅÅÅÅÅÅÅÅ\\nÉ\\nÖ\\nÑÑÑÑÑÑÑÑÑÑ\\nfr rr r() e x p\\n2\\n()\\ni\\ni\\n2\\n2 cuti?\\n?\\n∑ρ\\nσ\\n=− |−| ||\\n∈ (31)\\nHere, r denotes a point in Cartesian space,ri is the position ofatom i relative to the central atom of?, and the cutoﬀ function\\nfcut smoothly decays to zero beyond the cutoﬀ radius rcut. This\\ndensity representation ensures invariance with respect to\\ntranslations and permutations of atoms of the same species butnot rotations. To obtain a rotationally invariant descriptor, one\\nexpands the density in a basis of spherical harmonics,Ylm(r̂),\\nand a set of orthogonal radial functions,gn(|r|), as\\nChemical Reviews pubs.acs.org/CR ReviewChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9837cg Yrr r() ( ) ()\\nnlm\\nnlm n lm? ∑ρ =| | ̂\\n(32)\\nto construct the power spectrum of the density using the\\nexpansion coeﬃcients:\\nl cc() 8\\n21 ()nn l\\nm\\nnlm n l m? ∑ψ = +\\n*′ ′\\n(33)\\nOne then obtains a vector of descriptors ψ ={ ψnn′l}b yconsidering all componentsl ≤ lmax and n, n′ ≤ nmax that act as\\nband limits that control the spatial resolution of the atomic\\ndensity. The generalization to more than one chemical speciesis straightforward:459 one constructs separate densities for each\\nspecies α and then computes the power spectra ()nn l ?ψαα\\n′\\n′ for\\neach pair of elementsα and α′, where the two species indices\\ncorrespond to the c* and c coeﬃcients, respectively. Theresulting vectors corresponding to each of theα and α′ pairs\\nare then concatenated to obtain the descriptor vector of the\\ncomplete environment.\\nAtom-centered symmetry functions (ACSFs), or sometimescalled Behler−Parrinello symmetry functions, 411 descriptors\\ndiﬀer from SOAP in that they project the atomic densities over\\nselected 2-body or 3-body symmetry functions. FCHL 417\\ndescriptors follow similar principles while also considering thecorrelations between the atomic densities coming from\\ndiﬀerent chemical species. The many-body tensor representa-\\ntion (MBTR)418 approach involves taking the histograms of\\natom counts, inverse pairwise distances, and angles. Atomiccluster expansion (ACE) descriptors420 ﬁrst express atomic\\ndensities using spherical harmonics and then generate invariant\\nproducts by contracting the spherical harmonics with the\\nClebsch−Gordan coeﬃcients.Clebsch−Gordan coeﬃcients.\\nLength-Scale Hyperparameters. Most atomic descriptors\\nuse length-scale hyperparameters speciﬁcally chosen for a given\\nproblem and system. 276,411,412,415,417,418,454,455 There areseveral ways to automate hyperparameter selections. Ref374\\nintroduced general heuristics for choosing the SOAP hyper-\\nparameters for a system with arbitrary chemical composition\\nbased on characteristic bond lengths. Ref465 adopts thestrategy to ﬁrst generate a comprehensive set of ACSFs and\\nthen select a subset using the sparsiﬁcation methods such as\\nfarthest point sampling (FPS)466 and CUR matrix decom-\\nposition.467\\nIncompleteness of Atomic Descriptors. A structuraldescriptor is complete when there is no pair of conﬁgurations\\nthat produces the same descriptor.468 For atomic descriptors,\\nthis means that diﬀerent atomic environmentsafter consid-\\nering the invariances of rotation, translation, and permutationof identical atomsshould adopt distinct descriptors. Without\\ncompleteness, any ML model using the descriptors as input\\nwill give identical predictions of physically diﬀerent systems.\\nEnsuring completeness while preserving the invariances isnontrivial, however. One of the simplest descriptors is based\\non permutationally invariant pairwise atomic distances (2-body\\ndescriptors), and ref412 demonstrated that these are generally\\nnot complete since one can construct two distinct tetrahedrausing the same set of distances. Many have assumed that\\nFigure 7.KPCA maps of carbon atom environments in the QM9 database. Maps are color-coded according to Mulliken charges (a), hybridization(b), whether atoms are found in rings (c) and according to local energies predicted by a machine learning potential (d). Reprinted with permission\\nfrom ref 374. Copyright 2020 American Chemical Society.\\nChemical Reviews pubs.acs.org/CR ReviewChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9838permutationally invariant 3-body atomic descriptors uniquely\\nspecify atomic environments because of the tremendous\\nsuccess of ML models for chemical systems and particularly\\nMLPs. However, refs 469 and 468 exemplify that structuraldegeneracies can be found even when using 3- or 4-body\\ndescriptors. This underscores an important shortcoming of\\nstate-of-the-art 3-body descriptors, such as ACSF,411 SOAP,412\\nFCHL,417 and MBTR. 418 ACE420 should be a completedescriptor of local environments, but its reliance on spherical\\nharmonic expansion and the subsequent contraction makes\\ntheir evaluations expensive. Hence, there are still opportunities\\nto develop improved atomic descriptors.to develop improved atomic descriptors.\\n4.1.3. Locality Approximation. Representing a many-\\nbody chemical system in terms of atomic environments brings\\nphysical signiﬁcance since certain extensive physical properties(e.g., the total energy, total electrostatic charge, and polar-\\nizability of a system) can be approximated by the sum of the\\natomic contributions coming from each atomic environment,for example, ()i i?θΘ= ∑ .T h i sa p p r o x i m a t i o ni sv a l i d\\nbecause the atomic contribution associated with a central\\natom is largely determined by its neighbors, and long-range\\ninteractions can be approximated in a mean-ﬁeld mannerwithout explicitly considering distant atoms. Such“locality” is\\ntacitly assumed in many ML models for CompChem, and it is\\na crucial necessity for most common atomistic potentials and\\nMLPs (section 2.2.6.). Most MLPs (e.g., BPNN,274 GAP,276and DeepMD462) approximate the total energy of a system as\\nsums of local atomic energies.\\nFigure 7 illustrates locality by showing a KPCA map of the\\natom environments of carbon in the QM9 set (seesection 3.3for more detailed descriptions of the data set). By color-coding\\nthe KPCA plot with the local energies from a SOAP-based\\nGAP model trained on QM9 energies,470 one observes a\\nsystematic and smooth trend in energies across clusters. Thetotal molecular energy can then be accurately predicted by the\\nsum of local energies, which means the total energy can be\\napproximated on the basis of all the local environments\\ncontained in the molecule. For example, an NN potentialtrained on liquid water simulations can predict the densities,\\nlattice energies, and vibrational properties of diverse ice phases\\nbecause the local atomic environments found in liquid water\\nspan the similar environments as those observed in icephases.471 Another GAP potential of carbon trained on\\namorphous structures and other crystalline phases predicted\\nnovel carbon structures in random structure searches as well as\\napproximate reaction barriers.472,473approximate reaction barriers.472,473\\nThe locality approximation is typically rationalized based on\\nthe multiscale nature of interatomic interactions in chemical\\nsystems. It is generally expected that shorter interatomicdistances correspond to stronger interactions, such that a\\ncutoﬀ may be imposed after a certain radial distanced given a\\ncertain energy accuracy thresholdϵ. The multiscale nature of\\nphysical interactions underlies the usual classi ﬁcation ofchemical interactions, from strong covalent bonds and ionic\\ninteractions to weaker noncovalent hydrogen bonds and van\\nder Waals interactions. However, our understanding of\\nnoncovalent interactions in large molecules and materials isstill emerging,36 and no general rules-of-thumb exist to deﬁne\\nthe cutoﬀ distance d corresponding to a deﬁned ϵ. Moreover,\\nthe suﬃciency of the locality argument also depends on the\\nphase of the system and whether the system is extended ornot.474 Hence, for systems having long-range interactions\\n(which includes most chemical systems), the locality\\nassumption needs revision. There are currently three schools\\nof approaches handling the long-range interactions. Theﬁrst isto use global ML models, such as (s-)GDML,207,372 which\\nlearn global interactions directly. Global models tend to be\\nmore data-eﬃcient because they focus on learning a fullto use global ML models, such as (s-)GDML,207,372 which\\nlearn global interactions directly. Global models tend to be\\nmore data-eﬃcient because they focus on learning a full\\nmolecular or material PES, but this signi ﬁcantly limitstransferability since the ML model alone can only be used\\non the system it was trained upon. The second is to learn the\\ncharges475,476 and multipoles477 for each atom, and then the\\nlong-range electrostatic interactions based on environment-dependent charges or multipoles can be explicitly included\\nusing Coulomb’s law. To ensure that the sum of the atomic\\ncharges reaches neutrality, charge equilibration schemes can be\\nused.478 The third is to capture the long-range electrostaticeﬀects by introducing a nonlocal long-distance equivariant\\n(LODE) representation,479,480 which is dependent on the\\nelectrostatic ﬁeld generated by the decorated atom density.\\n4.1.4. Advantages of Built-In Symmetries. Built-insymmetry in ML models substantially compresses the\\ndimensionality of atomic representations and ensures that\\nphysically equivalent systems are predicted to have identical\\nproperties. One of the most rigorous ways of imposingsymmetry onto a modelf is via the invariant integration over\\nthe relevant group:\\nf fxP x() ( )sym :\\n∫=\\nπ π∈ (34)\\nwhere Pπx is a permutation of the input. However, the\\ncardinality of even basic symmetry groups is exceedingly high,which makes this operation prohibitively expensive. This\\ncombinatorial challenge can be solved by limiting the invariant\\nintegral to the physical point group andﬂuxional symmetries\\nthat actually occur in the training data set, as done insGDML.207 Alternative approaches, such as parameter\\nsharing352,421−423 or density representations, 276 have also\\nproven eﬀective. For example, the DeepMD potential has\\ntwo versions, the Smooth Edition (DeepPot-SE) explicitlypreserves all the natural symmetries of the molecular system,\\nand the other version that does not.462 The DeepPot-SE oﬀers\\nmuch improved stability and accuracy.207,462\\nFor ML predictions of scalar properties, the rotationallyinvariant atomic descriptor framework described earlier is\\nappropriate. One may wish to predict vectorial or tensorial\\nproperties including dipole moments, polarizability, and\\nelasticity. A covariant version of descriptors may be advanta-geous, and this can be expressed as\\nS S( ( )) ( ( ))NN3399ψψ{} = {} (35)\\nwhere S indicates a symmetry operation such as a rigid rotation\\nabout an axis. Ref 481 proposed a general method fortransforming a standard kernel forﬁtting scalar properties into\\na covariant one. Ref 482 derived a rotational-symmetry-\\nadapted SOAP kernel, which can be understood as using the\\nangular-dependent SOAP vectors based on spherical harmon-ics expansions as the descriptors. Note that the SOAP kernels\\nfor learning scalar properties introduced in ref 412 remove\\nangular dependencies by summing up the SOAP vectors in\\nseparate spherical harmonics channels.separate spherical harmonics channels.\\nSymmetry can be further exploited into “alchemical ”\\nrepresentations that incorporate similarity between chemical\\nspecies that are relatable by changing one atom into another.The FCHL417 representation considers the similarity between\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9839elements in the same row and columns of the periodic table\\nand performs very well on chemical compounds across\\nchemical space. Ref 483 compiled a data-driven periodic\\ntable of the elements byﬁtting to an elpasolite data set using analchemical representation.\\n4.1.5. End-to-End NN Representations.All descriptors\\nintroduced above rely on a suitable set of hyperparameters\\n(e.g., length scales, radial and angular resolution). Determiningan optimal set of hyperparameters can be a tedious process,\\nespecially when heuristics are unavailable or fail due to the\\nstructural and compositional complexity of the system. A poor\\nchoice of descriptors can limit the accuracy of theﬁnal MLmodel, for example, when certain interatomic distances can not\\nbe resolved.\\nEnd-to-end NN representations follow a diﬀerent strategy to\\nlearn a representation directly from reference data. Using atomtypes and positions of a system as inputs, end-to-end NNs\\nconstruct a set of atom-wise featuresxi. These features are then\\nused to predict the property of interest, for example, the energyas a sum of atom-wise contributions. Unlike static descriptors,\\nthe representation is also optimized as part of the overall\\ntraining process. This way end-to-end NNs can adapt to\\nstructural features in the data and the target properties in afully automatic fashion to eliminate the need for extensive\\nfeature engineering from the practitioner.\\nThe deep tensor NN framework (DTNN)352 introduced a\\nprocedure to iteratively reﬁne a set of atom-wise features {xi}based on interactions with neighboring atoms. Higher-order\\ninteractions can then be captured in an hierarchical fashion.\\nFor example, aﬁrst information pass would only capture radial\\ninformation, but further interactions would recover angularrelations and so on. In DTNN, a learnable representation\\ndepending only on atom typesxi\\n0 = ezi serves as an initial set of\\nfeatures. These are then reﬁned by successive applications of\\nan update function depending on the atomic environment thattakes the general form\\ni\\nk\\njjjjjjj\\ny\\n{\\nzzzzzzzFG fx x x x g rr rr, , () ()i\\nll\\ni\\nl\\nj\\nl\\ni\\nl\\nj\\nll\\nij ij\\n1\\ncut∑= + [ |−| ] |−|+\\n(36)\\nHere, l indicates the number of overall update steps. The sumruns over all atoms j in the local environment, and a cutoﬀ\\nfunction fcut ensures smoothness of the representation. Each\\nfeature is updated with information from all neighboring atoms\\nthrough the interaction functionG. Apart from the neighborfeatures xj, G also depends on the interatomic distance|ri − rj|,\\nwhich is usually expressed in the form of a radial basis vectorg.\\nAfter the update, an atom-wise transformationF can be appliedto further modulate the features. Since each update depends\\nonly on the interatomic distances and the summation over\\nneighboring atoms is commutative, end-to-end NNs of this\\ntype automatically achieve a representation that is invariant torotation, translation and permutations of atoms. Using these\\natom-type dependent embeddings compactly encodes ele-\\nmental information. This is advantageous for systems\\ncomprised of many diﬀerent chemical elements. Such multi-component systems can be problematic to treat with\\npredeﬁned descriptors (e.g., ACSFs or SOAP), as these\\ntypically introduce additional entries for each possible\\ncombination of atom types, resulting in a large number of\\ndescriptor dimensions.descriptor dimensions.\\nSince the introduction of DTNN, many diﬀerent types of\\nend-to-end NNs have been developed, and these vary by the\\nchoice for the functionsF and G. For example, SchNet434 usescontinuous convolutions inspired by convolutional neural\\nnetworks (CNNs) to describe the interatomic interactions.\\nIn this case, the update ineq 36 takes the form\\ni\\nk\\njjjjjjj\\ny\\n{\\nzzzzzzzfx xx g r r r rNN NN ( ) ( )i\\nl\\ni\\nll\\nj\\nj\\nlll\\nij ij\\n1l\\ni\\nll\\nj\\nj\\nlll\\nij ij\\n1\\ntr rad cut∑= + [ |−| ] |−|+\\n(37)\\nwhere the feature transformation (NN tr) and the radial\\ndependence (NNrad) are both modeled as trainable NNs.\\nOther ML models introduce additional physical information.(37)\\nwhere the feature transformation (NN tr) and the radial\\ndependence (NNrad) are both modeled as trainable NNs.\\nOther ML models introduce additional physical information.\\nThe hierarchical interac ting particle NN (HIP-NN) 484enforces a physically motivated partitioning of the overall\\nenergy between the di ﬀerent re ﬁnement steps, while the\\nPhysNet architecture485 introduces explicit terms for long-\\nrange electrostatic and dispersion interactions. In ref 421,Gilmer et al. categorize graph networks of this general type as\\nmessage-passing NNs (MPNNs) and introduce the concept of\\nedge updates. These make it possible to use interatomic\\ninformation beside the radial distance metric in the reﬁnementprocedure, and they have since been adapted for other\\narchitectures.486 Another interesting extension are end-to-end\\nNNs incorporating higher-order features beside the scalarxi\\nused in the original DTNN framework. These are equivariantfeatures that encode rotational symmetry and can be based on\\nangles, dipole moment vectors, or features that can be\\nexpressed as spherical harmonics withl > 0. This enables the\\nexchange of only radial information between atoms in eachinteraction pass and instead include higher structural\\ninformation, such as dipole −dipole interactions or angular\\ninformation. In addition, equivariant end-to-end NNs can also\\nbe used to predict vectorial or tensorial properties in a mannersimilar to the rotational-symmetry-adapted SOAP kernel.\\nExamples include TensorField networks, 464 Cormorant,463\\nDimeNet,487 PiNet,488 and FieldSchNet.300\\n4.2. From Descriptors to Predictions4.2. From Descriptors to Predictions\\nAfter a descriptor vector for each chemical structure is deﬁned,\\none can then construct the design matrix and the kernel matrix\\nfor a set of structures. These matrices can then be used as theinput of ML models. As described insection 2, supervised ML\\nmethods, such as NNs and GPs, can be used to approximate\\nnonlinear and high-dimensional functions, particularly when\\nmassive amounts of training data become available. Thus, oneshould expect that using CompChem would be very useful for\\ngenerating a large amount of almost noise-free training data of\\nspeciﬁc systems or atomic con ﬁgurations, as long as a\\nphysically accurate method is being applied in the right waywith appropriate computational resources. In contrast,\\nexperimental observations can be di ﬃcult to measure and\\nreproduce precisely. Note that the aim of most CompChem\\n+ML eﬀorts have a similar scope as decades-old quantitativestructure activity/property relationship (QSAR/QSPR) mod-\\nels that are often based on experiments or CompChem\\nmodeling.326,327,489 Thus, researchers in CompChem+ML\\nshould be aware of potentially relatable work done by theQSAR/QSPR communities, and to what extent questions\\nbeing posed have been suﬃciently answered. On the other\\nhand, ML usually provides higher accuracy than non-ML\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107https://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9840statistical models, and so QSAR/QSPR e ﬀorts have been\\nturning toward ML models as well.490\\nWe have explained how data from diﬀerent CompChem\\nmethods, each containing diﬀerent degrees of physical rigor,can be used to train ML models. ML models in turn can be\\ncreated to approximate underlying high-dimensional functions\\nintrinsic to physical systems. For example, research eﬀorts are\\ntoward learning electron densities,491 density functionals,162and molecular polarizabilities.492\\nBesides these direct learning strategies, ML has been used to\\nenhance the performance and suitability of CompChem\\nmodels. As mentioned insection 1, the Δ-ML493 approach isnow a common technique for adapting an ML model that\\nimproves the quality of a theoretically insuﬃcient but\\ncomputationally aﬀordable method. This approach has been\\nused to learn many body corrections for water molecules toallow a relatively inexpensive KS-DFT approach like BLYP to\\nmore accurately reproduce CCSD(T) data.494 Along similar\\nlines, Shaw and co-workers used CompChem features along\\nwith an NN to reweight terms from an MP2 interaction energyto provide ML-enhanced methods with increased perform-\\nance.126 Miller and co-workers have developed ML-models\\nwhere molecular orbitals themselves are learned to generate a\\ndensity matrix functional that provides CCSD(T)-quality PESswith a single reference calculation.495 von Lilienfeld and co-\\nworkers have investigated how the choice of regressors and\\nmolecular representations for ML models impacts accuracy,\\nand theirﬁndings suggest ways that ML models may be trainedto be more accurate and less computationally expensive than\\nhybrid DFT methods.496 Burke and co-workers have studied\\nhow ML methods can result in improved understanding and\\nmore physical exact KS-DFT181,497−499 and OFDFT func-tionals.161 Brockherde et al. have presented an approach, where\\nML models can directly learn the Hohenberg−Kohn map from\\nthe one-body potential eﬃciently toﬁnd the functional and its\\nderivative.162,184 Akashi and co-workers have also reported theout-of-training transferabi lity of NNs that capture total\\nenergies, which shows a path forward to generalizable\\nmethods.500\\nToward predictive insights, there are many other approaches\\nthat are broadly useful. One can exploit the “universalapproximator” nature of ML architectures toﬁnd a function\\nthat gives the best solution in a variational setting. For\\ninstance, using restricted Boltzmann machines501 or deep NNs\\nas a basis representation of wavefunctions 105,106,502 inQuantum Monte Carlo calculations. Alternatively, the use of\\nactive learning might increase the eﬃciency, accuracy,\\nscalability, and transferability of ML models.503−505\\nTable 5. ML Databases for CompChem\\ndatabase description locationdatabase description location\\nAFLOWLIB databases containing calculated properties of over 625k materials 510 http://www.aﬂowlib.org\\nANI-1 large computational DFT database, which consists of more than 20 M o ﬀ equilibriumconformations for 57.5k small organic molecules511,512\\nhttps://github.com/isayev/ANI1_dataset\\nANI-1x/ANI-\\n1ccx\\nANI-1x contains multiple QM properties from 5 M DFT calculations, while ANI-1ccxcontains 500k data points obtained with an accurate CCSD(T)/CBS extrapolation513\\nhttps://github.com/aiqm/ANI1x_datasets\\nBindingDB measured binding a ﬃnities focusing on interactions of proteins considered to becandidates as drug-targets; 1 200 000 binding data for 5500 proteins and over 520 000\\ndrug-like molecules514\\nhttp://www.bindingdb.org\\nClean Energy\\nProject\\ncontains ∼10 000 000 molecular motifs of potential interest which cover small moleculeorganic photovoltaics and oligomer sequences for polymeric materials515\\nhttp://cepdb.molecularspace.org\\nCoRE MOF database containing over 4700 porous structures of metal −organic frameworks withpublicly available atomic coordinates; includes important physical and chemical\\nproperties516\\n10.11578/1118280publicly available atomic coordinates; includes important physical and chemical\\nproperties516\\n10.11578/1118280properties516\\n10.11578/1118280\\nFreeSolv experimental and calculated hydration free energies for neutral molecules in water 517 http://www.escholarship.org/uc/item/6sd403pzGDB GDB-11, GDB-13, and GDB-17; together these databases contain billions of small organic\\nmolecules following simple chemical stability and synthetic feasibility rules518\\nhttp://gdb.unibe.ch/downloads/\\nHypothetical\\nZeolitesHypothetical\\nZeolites\\ncontains approximately 1 M zeolite structures519 http://www.hypotheticalzeolites.net/\\nMaterials\\nProject\\ncontains computed structural, electronic, and energetic data for over 500k compounds520 https://www.materialsproject.orgMD17 data sets in this package range in size from 150k to nearly 1 M conformational geometries;\\nall trajectories are calculated at a temperature of 500 K and a resolution of 0.5 fs372\\nhttp://www.sgdml.orghttp://www.sgdml.org\\nMoleculeNet contains data on the properties of over 700k compounds 521 http://moleculenet.ai\\nOpen Catalyst\\nProject\\n1.2 M molecular relaxations with results from over 250 M DFT calculations relevant forrenewable energy storage522\\nhttps://opencatalystproject.org/index.html\\nOQMD consists of DFT predicted crystallographic parameters and formation energies for over\\n200k experimentally observed crystal structures523\\nhttp://oqmd.org\\nPubChemQC\\nPM6http://oqmd.org\\nPubChemQC\\nPM6\\nprovides 221 million molecular structures optimized with the PM6 method and several\\nelectronic properties computed at the same level of theory524\\nhttp://pubchemqc.riken.jp/pm6_datasets.htmlhttp://pubchemqc.riken.jp/pm6_datasets.html\\nPubChemQC provides ∼3 million molecular structures optimized by DFT and excited states for over 2\\nmillion molecules using TD-DFT525\\nhttp://pubchemqc.riken.jp/http://pubchemqc.riken.jp/\\nQM7-X comprehensive data set of 42 physicochemical properties for ∼4.2 M equilibrium and\\nnonequilibrium structures of small organic molecules with up to seven non-hydrogen (C,\\nN, O, S, Cl) atoms526N, O, S, Cl) atoms526\\nhttps://zenodo.org/record/4288677#.\\nX9jHNC2ZNTY\\nQM9 geometric, energetic, electronic, and thermodynamic properties for 134k stable small\\norganic molecules out of GDB-17527\\nhttps://ﬁgshare.com/collections/Quantum_https://ﬁgshare.com/collections/Quantum_\\nchemistry_structures_and_properties_of_134_\\nkilo_molecules/978904\\nSynthesis\\nProject\\ncollection of aggregated synthesis parameters computed using the text contained within\\nover 640 000 journal articles528over 640 000 journal articles528\\nwww.synthesisproject.org\\nquantum-\\nmachine.org\\na repository of diverse data sets, including valence electron densities, chemical reactions,\\nsolvated protein fragments, and molecular Hamiltonianshttp://quantum-machine.org/datasets/\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n98414.3. CompChem Data\\nWe have laid the general framework for CompChem+ML\\nstudies, but this direction would not be complete without more\\ndetails about training data (i.e., garbage in, garbage out). Wenow review the landscape of data sets in CompChem and how\\nthey will likely evolve over time. The past decade has seen\\ncontinually increasing usefulness and availabilty of“big data”\\nfrom CompChem that include community-wide data reposi-tories comprised of millions of atomistic structures along with\\ndiverse physical and chemical properties. 506 −509 Such\\nrepositories are becoming the norm, and it is more customary\\nfor diﬀerent users to deposit raw or processed simulation datathere for the beneﬁt of the research community. This brings\\nthe possibility of robust validation tests for ML models, but it\\nalso necessitates approaches that are well-equipped to handle\\nlarge and complex data sets. Typical data sets may come fromdiverse origins such as MD trajectories fromab initio\\nsimulations, data sets of small molecules and molecular\\nconformers, or other training sets used for developing ML\\nand non-ML FFs for speciﬁc applications. As the data setsgrow, so do the scope of publications that involve ML as\\nshown in Figure 1.\\n4.3.1. Benchmark Data Sets. ML models must be\\nvalidated before they can be trusted for predictions.\\nValidations of descriptors or model trainings are performedon benchmark data sets, and several popular ones are\\nsummarized inTable 5 . These allow ML models to be\\ncompared on the same ground and provide large amounts of\\ndata for robust training. Their availability to the public alsoensures that the data sets can evolve with time and be extended\\nas a part of community eﬀorts.529\\nAmong the entries inTable 5, the most often used one is the\\nQM9 set, which consists of approximately 134 000 of thesmallest organic molecules that contain up to 9 heavy atoms\\n(C, O, N, or F; excluding H) along with their CompChem-\\ncomputed molecular properties such as total energies, dipole\\nmoments, HOMO−LUMO gaps, etc. Several ML studies havealready been published using this data set (seeFigure 8, ref\\n496). A popular challenge associated with QM9 is to develop a\\nnext-generation ML model that learns the electronic energies\\nof random assortments of organic molecules with higheraccuracy and less required training data than other existing\\nmodels. Doing so tests next generation molecular representa-\\ntions and training algorithms.Figure 8 illustrates how the\\nchoice of architecture and descriptors can in ﬂuence thepredictive performance and data e ﬃciency of ML models\\nusing diﬀerent properties of the QM9 data set as examples.\\nThe next signi ﬁcant advance will potentially be due to a\\ncombination of supervised and unsupervised learning models.4.3.2. Visualization of Data Sets.As the structural data\\nsets grow it becomes infeasible to manually identify hidden\\npatterns or curate the data. Data-driven and automated\\nframeworks for visualizing these data sets become increasinglypopular.530−533 Dimensionality reduction eﬀectively translates\\nthe high dimensional data (i.e., the xyz-coordinates for\\nmolecules or materials in di ﬀerent atomic con ﬁgurations)\\ninto a low-dimensional space easily visualized on paper or acomputer screen. In this way, entries such as those in the QM9\\nset can be shown (seeFigure 9). The KPCA maps inFigure 9\\nare based on the dimensionality reduction of the global SOAP\\ndescriptors, which are constructed by combining all the atomicSOAP descriptors usingeq 30. Each dot represents a small\\nFigure 8.Learning curves of various properties contained in the QM9 database, reporting the predictive accuracy of various models as a function oftraining set sizes. Each curve represents an individual model based on a diﬀerent architecture and descriptor. Shown are learning curves for theinternal energy (U0), HOMO and LUMO energies (ϵHOMO, ϵLUMO), the HOMO−LUMO gap (Δϵ), the length of the dipole moment vector (μ),internal energy (U0), HOMO and LUMO energies (ϵHOMO, ϵLUMO), the HOMO−LUMO gap (Δϵ), the length of the dipole moment vector (μ),the isotropic polarizability (α), the zero point vibrational energy (ZPVE), heat capacity (CV), and the highest fundamental vibrational frequency\\n(ω1). Reprinted with permission from ref496. Copyright 2020 American Chemical Society.Chemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9842molecule in the QM9 set, and the maps, thus, illustrate the\\nsimilarity between the molecules, instead of the relations\\nbetween the carbon atomic environments in Figure 7. The\\nmaps in Figure 9 are color-coded using diﬀerent molecularproperties, such as the atomization energies, composition, and\\noptical properties, and these properties are strongly correlated\\nwith the principal axes. These KPCA maps are, therefore, an\\nintuitive and condensed way to help navigate the QM9 set.Similarly, ref 321 used SOAP-sketchmaps in conjunction with\\nquasi-chemical theory to visualize similarities in local solvation\\nstructures and thus show an unsupervised learning procedure\\nto identify structures that signi ﬁcantly impact solvationenergies of small ions.\\nGenerally speaking, these data-driven maps are generated by\\nprocessing the design matrix (or kernel matrix) associated with\\na data set using dimensionality reduction techniquesintroduced in section 3.2. A simple option is to use the\\nASAP code, 374 a Python-based command line tool, that\\nautomates analysis and mapping. Figures 7 and 9 were\\ngenerated using ASAP using only two commands that aredisplayed in the ﬁgure. Data sets can also be explored in an\\nintuitive manner using interactive visualizers534 that run in a\\nweb browser and display 3D-structures corresponding to each\\natomistic structure in the data set.atomistic structure in the data set.\\n4.3.3. Text and Data Mining for Chemistry.Conven-\\ntional publications are an essential part of any CompChem\\nknowledge base, and ML is becoming useful at acceleratinginformation extraction from the scientiﬁc literature via text\\nmining.535−537 This topic was previously comprehensively\\nreviewed in the context of cheminformatics. 538,539 Natural\\nlanguage processing has already driven text-mining eﬀorts formaterials science discovery 538 and experimental synthesis\\nconditions of oxides.528,540 CompChem+ML can also amplify\\nexisting eﬀorts in chemometrics,541 the science of data-driven\\nextraction of chemical information. 542 This area has alsobranched into related disciplines of data mining for speciﬁc\\nclasses of materials 543 and catalysis informatics. 544 These\\napproaches have great promi se, especially for deriving\\ninformation and knowledge from data, but it remainschallenging to implement these in ways that achieve insight\\n(and true impact).\\nSome have shown paths forward for doing so. For example,\\nML models can obtain knowledge from failed experimentaldata more reliably than humans who are more susceptible to\\nsurvivor bias,545 and it can also be used to distill physical laws\\nand fundamental equations using experimental 363 and\\ncomputational data.546 ML models can also be used to reliablypredict SMILES representations (a string-based representation\\nof molecular graphs) that allow encoded information to be\\nderived from low-resolution images found in the literature.547\\nML models can interpret experimental X-ray absorption nearedge structure (XANES) data and predict real space\\ninformation about coordination environments. 548 Likewise,\\nscanning tunneling microscopy (STM) data can be used to\\nclassify structural and rotational states on surfaces, 549 andname indicators can be used to predict in tandem mass\\nFigure 9.KPCA maps of the QM9 database using a global SOAP kernel. The maps are color-coded according to atomization energy per atom (a),composition (b), number of carbon atoms in the molecule (c), total number of atoms in the molecule (d), HOMO−LUMO gapϵgap (e), HOMOenergies ϵHOMO (f), and the number of atoms in a ring (g). Examples of molecules along various“paths” in panel a are illustrated. Reprinted with\\npermission from ref374. Copyright 2020 American Chemical Society.Chemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9843spectrometry (MS/MS) properties. 550 In closing, we see\\nexciting opportunities for future applications that complement\\ndata and text mining to chemometrics through chemical space.\\n4.4. Transforming Atomistic Modeling4.4. Transforming Atomistic Modeling\\nWe previously mentioned that ML can handle large data sets\\nand extract insights while circumventing the high cost of\\nquantum-mechanical calculations by statistical learning.CompChem+ML also has great potential in developing\\nMLPs. Car and Parrinello proposed running MD using\\nelectronic-structure methods in 1985. 551 These are now\\nmainstream but also quite computationally demanding andnormally restricted to small system sizes (∼100 atoms) and\\nshort simulation times ( ∼10−12 s). Alternatively, accurate\\natomistic potentials introduced in section 2.2.6 have been\\ndeveloped to allow Monte Carlo and MD simulations, butsuﬃciently accurate potentials are sometimes not available.\\nMLPs have emerged as way to achieve as high accuracy as KS-\\nDFT or correlated wavefunction methods but with a fraction of\\nthe cost. MLPs have been constructed for far-reaching systemsfrom small organic molecules to bulk condensed materials and\\ninterfaces.433,552,553 Several of the coauthors of the current\\nreview have also written separate review focused more\\nnarrowly on this topic,554 and so, we only provide a briefoverview here.\\nTraining an MLP to reproduce a system ’s PES usually\\nrequires generating diverse and high quality CompChem data\\npoints that cover the relevant temperature and pressure\\nconditions, reaction pathways, polymorphs, defects, composi-tions, etc.555−562 After data points comprised of atomic\\nconﬁgurations, system energies, and forces are obtained,\\ndiﬀerent methods for constructing MLPs employ either\\ndiﬀerent descriptors (see a list of examples in Table 4)o rdiﬀerent ML architectures to perform interpolations of the full\\nPES. Again, smoothness is an essential feature for any PES, so\\nspecial considerations are needed to avoid numerical noise thatwould result in discontinuities. 563,564 Kernel method-based\\nMLPs, such as GAP 276,565 and sGDML, 207,372,566 ensure\\nsmoothness by relying on smoothly varying basis functions,\\nbut the scaling of kernel-based methods with respect to thenumber of training points is challenged without reduction\\nmechanisms.396,567 As a much more eﬃcient but somewhat less\\naccurate alternative to GAP, SNAP568 uses the coeﬃcients of\\nthe SOAP descriptors and assumes a linear or quadraticrelation between energies and the SOAP bispectrum\\ncomponents.569 The most popular MLPs are currently NN-\\nbased due to their ﬂexibility and capacity to train based on\\nlarge amounts data. Among these, ANI 511 ,513 andlarge amounts data. Among these, ANI 511 ,513 and\\nBPNN274,433,570 potentials use ACSF descriptors as inputs,\\nwhile Deep NNs, such as SchNet422,434,571 and DeepMD572\\nuse the coordinates and nuclear charges of atoms. We nowfocus on a few example applications.\\n4.4.1. Predicting Thermodynamic Properties. Many\\nCompChem e ﬀorts focus on predicting thermodynamic\\nproperties atﬁnite temperatures, such as heat capacity, density,and chemical potential. Although many physical properties are\\nalready accessible from MD simulations, doing estimations of\\nfree energies that establish the relative stability of diﬀerent\\nstates using electronic structure methods remains diﬃcult. Theconﬁgurational part of the Gibbs free energy of a bulk system\\nthat hasN distinguishable particles with atomic coordinatesr =\\n{r1...N}, and the associated potential energy U(r) can be\\nexpressed as\\nÄ\\nÇ\\nÅÅÅÅÅÅÅÅÅÅ\\nÉ\\nÖ\\nÑÑÑÑÑÑÑÑÑÑ\\nGN P T kT UP VÄ\\nÇ\\nÅÅÅÅÅÅÅÅÅÅ\\nÉ\\nÖ\\nÑÑÑÑÑÑÑÑÑÑ\\nGN P T kT UP V\\nkTr r(,, ) l n d e x p ()\\nB\\nB\\n∫=− − +\\n(38)\\nintegrated over all possible coordinates r, where kB is the\\nBoltzmann constant. In order to rigorously determineG, onemust exhaustively sample the con ﬁguration space that has\\nrelatively high weight arising from the\\nÄ\\nÇ\\nÅÅÅÅÅÅÅ\\nÉ\\nÖ\\nÑÑÑÑÑÑÑexp UP V\\nkT\\nr()\\nB\\n− + . This\\nnormally requires thermodynamic integration or enhancedsampling methods (e.g., umbrella sampling, 573 metadynam-Ä\\nÇ\\nÅÅÅÅÅÅÅ\\nÉ\\nÖ\\nÑÑÑÑÑÑÑexp UP V\\nkT\\nr()\\nB\\n− + . This\\nnormally requires thermodynamic integration or enhanced\\nsampling methods (e.g., umbrella sampling, 573 metadynam-\\nics,574 or transition path sampling575), that require simulationtimes and scales far beyond what is accessible with MD\\nsimulations based on KS-DFT or correlated wavefunction\\nmethods.\\nHowever, MLPs have unleashed both limits on the time\\nscale and system size. An early example,576 used an MLP withumbrella sampling 573 and the free energy perturbation\\nmethod577 to reveal the inﬂuence of van der Waals corrections\\non the thermodynamic properties of liquid water. Later, the\\ncombination of an MLP trained from hybrid DFT data andfree energy methods reproduced several thermodynamic\\nproperties of water from quantum mechanics, including the\\ndensity of ice and water, the diﬀerence in melting temperature\\nfor normal and heavy water, and the stability of diﬀerent formsof ice.578,579 Ref 580 employed the DeepMD approach to\\nstudy the relatively long time-scale nucleation of gallium.\\nMLPs for high-pressure hydrogen provided evidence on how\\nhydrogen gradually turns into a metal in giant planets.581 In allthese examples, high accuracy and long time scales were\\nrequired to model the speciﬁc phenomena and reveal physical\\ninsights, and it is precisely the combination of CompChem\\n+ML that enables both.\\n4.4.2. Nuclear Quantum E ﬀects. As mentioned in4.4.2. Nuclear Quantum E ﬀects. As mentioned in\\nsection 2.2.5, NQEs of chemical systems having light elements\\nbring challenges for atomistic modeling because the added\\nmobility of lighter atoms in dynamics simulations requireshigher computational cost to treat. To make the matter even\\nmore complicated, many atomistic potentials (seesection\\n2.2.6), particularly the ones for water or organic molecules,\\ncannot be used to model NQEs, because they often describecolavent bonds as rigid and thus cannot describe the\\nﬂuctuations of the bond lengths and angles. As a remedy,\\nseveral studies have been performed by training an MLP using\\nhigher rungs of KS-DFT (e.g., hybrid-DFT or meta-GGA) andthen using this potential in PIMD simulations.578,582−584 The\\nstudy of water mentioned in the previous section, which used\\nMLPs trained from hybrid DFT, revealed that NQEs were\\ncritical for promoting the hexagonal packing of moleculesinside ice that ultimately lead to the 6-fold symmetry of\\nsnowﬂakes.578 Highly data eﬃcient ML potentials can even be\\ntrained on reference data at the computationally very expensive\\nquantum-chemical CCSD(T) level of accuracy. For example,the sGDML206,207,585 approach has been shown to faithfully\\nreproduce such FFs for small molecules, which were then used\\nto perform simulations with eﬀectively fully quantized\\nelectrons and nuclei.electrons and nuclei.\\n4.5. ML for Structure Search, Sampling, and Generation\\nLocating stationary points on the PES is a frequent task in\\nCompChem, since these are needed for explaining reactionkinetics. Explorations for stationary points normally require\\nmany energy and force evaluations. ML approaches are being\\nimplemented to dramatically accelerate minimum energy as\\nChemical Reviews pubs.acs.org/CR ReviewChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9844well as saddle-point optimizations.293−295,565,586−588 Bernstein\\net al. proposed an automated protocol that iteratively explores\\nstructural space using a GAP potential.565 Bisbo and Hammer\\nemployed an actively learned surrogate model of the PES toperform local relaxations while only performing single-point\\nquantum-mechanical calculations for selected structures with\\nhigh values of acquisition.586 Work in refs293 and 295−297\\naccelerated nudged elastic band (NEB) calculations byincorporating a surrogate ML models.\\nML can also dramatically accelerate the challenge of\\neﬃciently sampling equilibri um or transition states by\\naccelerating enhanced sampling methods such as umbrellasampling573 and metadynamics.574 These procedures make use\\nof collective variables (CVs) that deﬁne a reaction coordinate,\\nand computing the associated free energy surface (FES)\\namounts to generating the marginal probability distribution inthese CVs. Unfortunately, the choice of the CVs is not always\\nclear for speciﬁc systems, and ML has shown some promise in\\nguiding their determination.589−591 Another direction is to\\nexploit that ML models can be considered as universalapproximators of FESs.592 For example, there are reports of\\nadaptive enhanced sampling methods using a Gaussian\\nMixture model,593 using an NN architecture to represent the\\nFES594 or the bias function in variational sampling\\nsimulations.595simulations.595\\nML methods also oﬀer fundamentally new ways to explore\\nchemical compound and con ﬁguration space. Generative\\nmodels can learn the structural and elemental distribution\\nunderlying chemical systems, and once trained, these modelscan then be used to directly sample from this distribution. It is\\nfurthermore possible to bias the generated structures toward\\nexhibiting desired properties, for example, drug activity or\\nthermal conductivity. As a consequence, generative modelsoﬀer exciting new avenues in drug and materials design.596,597\\nGenerative methods in CompChem include recurrent neural\\nnetworks (RNNs), which can be used for the sequential\\ngeneration of molecules encoded as SMILES strings.598−600Segler et al. demonstrated how such a recurrent model canﬁrst\\nlearn general molecular motifs and then be ﬁne-tuned to\\nsample molecules exhibiting activity against a variety of\\nmedical targets.599 Autoencoders (AE) are another frequentlyused ML method for molecular generation. AEs learn to\\ntransform molecular graphs or SMILES into a low-dimensional\\nfeature space and backward. The resulting feature vector\\nrepresents a smooth encoding of the molecular distributionand can be used to eﬀectively sample chemical space.601−606 By\\napplying a variational AE to the QM9 and ZINC databases,\\nGomez-Bombarelli et al. could generate several optimized\\nfunctional compounds.607 An interesting extension to AEs areconditional AEs, which not only capture the distribution of\\nmolecular structures but also dependencies on various\\nproperties.426,608 This makes it possible to directly generate\\nstructures exhibiting certain property ranges or combinationswithout the need for biasing or additional optimization steps.\\nAEs can also form the basis of another approach for exploring\\nchemical space called generative adversarial networks\\n(GANs).609,610 In a GAN, a generator model (often an AE)attempts to create samples that closely match the underlying\\ndata, while a discriminator tries to distinguish true from\\ngenerated samples. These architectures can be enhanced by\\nusing RL objectives. RL learns an optimal sequence of actions(e.g., placement of atoms) leading to a desired outcome (e.g.,\\nmolecule with certain property). This makes it possible to\\ndrive generative processes toward certain objectives, allowing\\nfor the targeted generation of molecules with particularproperties.611−614 RL in general is a promising alternative\\nstrategy for generative models, 615,616 and they o ﬀer the\\npossibility for tight integration into drug design cycles. 617properties.611−614 RL in general is a promising alternative\\nstrategy for generative models, 615,616 and they o ﬀer the\\npossibility for tight integration into drug design cycles. 617\\nAlternative approaches combine autoregressive models withgraph convolution networks.618,619\\nWhile these methods use SMILES or graphs to encode\\nmolecular structures, generative models have recently been\\nextended to operate on 3D coordinates of molecules andmaterials.620,621 Gebauer et al. proposed an autoregressive\\ngenerative model based on the SchNet architecture, called g-\\nSchNet.622 Once trained on the QM9 data set, g-SchNet was\\nable to generate equilibrium structures without the need foroptimization procedures. It was further found, that the model\\ncould be biased toward certain properties. In another\\npromising approach, Noéet al. used an invertible NN based\\non normalizing ﬂows to learn the distribution of atomicpositions (e.g., sampled from an MD trajectory). This network\\ncan then be used to directly sample molecular conﬁgurations\\nby sampling from this distribution without performing costly\\nsimulations.298\\n4.6. Multiscale Modelingsimulations.298\\n4.6. Multiscale Modeling\\nMultiscale modeling is a term for including simulation or\\ninformation from diﬀerent scales (seeFigure 3). ML has been\\nintroduced into QM/MM-like schemes that enable improvedmultiscale simulations,300,325,623 and on the side of coarse-\\ngraining.624 Diﬀerent coarse-graining potentials have been\\ndeveloped,625 but the inherent functional form for these\\npotentials relies on CPI as well as trial-and-error procedures.Several works used ML for constructing coarse-grained\\npotentials by matching mean forces.449,450,626,627 In closing,\\nwe see promise for incorporating experimental priors into ML\\nmodels, for instance, using experimental measurements toimprove an ML PES by complementing them with\\nexperimental data. We are not aware of such eﬀorts for\\ndeveloping highly accurate MLPs beyond the atomic scale,\\nalthough much work has been done along this line to reﬁneFFs of RNAs and proteins, often incorporating methods from\\nML, including the maximum entropy approach.628\\n5. SELECTED APPLICATIONS AND PATHS TOWARD\\nINSIGHTS\\nThe central challenge posed at the beginning of this review washow to identify and make chemical compounds or materials\\nhaving optimal properties for a given purpose. To do so would\\nhelp address critical and broad issues from pollution to global\\nwarming to human diseases. Traditional developments areoften slow, expensive, and restricted by nontransferable\\nempirical optimizations, and so eﬀorts have turned to\\nCompChem+ML to alleviate this.515,525,629,630\\nCompChem+ML are enabling searches through larger areasof chemical space much faster than before. 20,631−634 This\\nsection is not to extensively review the large amount of work\\nusing CompChem+ML in these diﬀerent areas, but rather to\\nhighlight examples of applications that have resulted in notableinsights so that others might use these notable works as\\ntemplates for future eﬀorts.\\n5.1. Molecular and Material Design\\nMolecules and materials design is usually considered to be an\\noptimization problem.270,426,602,607,635 Thus, a comprehensiveunderstanding of chemical space is needed to identify\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9845compounds with desired properties that are subject to certain\\nrequired constraints (e.g., a speci ﬁc thermal stability or a\\nsuitable optical gap for absorbing sunlight). Those properties\\nwill also depend on many key variables (e.g., constitutiveelements, crystal forms, geometrical and electronic character-\\nistics, among others), which make the property prediction\\ncomplex.531 CompChem calculations as explained insection 2\\nshould provide a continuous description of properties across acontinuous representation (i.e., a descriptor orﬁngerprint) of\\nmolecules that is used to map molecular conﬁgurations to\\ntarget properties, and vice versa. ML methods then can be\\nimplemented to search large databases to extract structure−property relationships for designing compounds with speciﬁc\\ncharacteristics.531,635−637 Optimizations would then be per-\\nformed on the structure-based function learned from training\\nconﬁgurations, and the composition of the chemicalcompound would then be recovered back from the continuous\\nrepresentation.\\nAs a protoypical example of molecular design via high-\\nthroughput screening, Gomez-Bombarelli et al.632 showed a\\ncomputation-driven search for novel thermally activateddelayedﬂuorescence organic light-emitting diode (OLED)\\nemitters. That workﬁrst ﬁltered a search space of 1.6 million\\nmolecules down to approximately 400 000 candidates using\\nML to anticipate criteria for desirable OLEDs. For the purposeof evaluating candidates, they estimated an upper bound on the\\ndelayedﬂuorescence rate constant (kTADF). TD-DFT calcu-\\nlations were then used to provide reﬁned predictions of speciﬁc\\nproperties of thousands of promising novel OLED moleculesacross the visible spectrum so that synthetic chemists, device\\nscientists, and industry partners would be able to choose the\\nmost promising molecules for experimental validation and\\nimplementation. Notably, this example of CompChem+MLresulted in new devices that exhibited an external quantum\\neﬃciency of over 22%.Figure 10 shows the high accuracy of\\nML in predicting useful prop erties for high-throughput\\nscreening of molecules and materials based on kTADFcalculations. This work exempliﬁes how ML can accelerate\\nthe design of novel compounds in such a way that could not be\\npossible using traditional CompChem methods alone.\\nIntegrations of features relevant to learning tasks allow oneto improve the accuracy of ML predictions for a given target\\nproperty. Park and Wolverton638 improved the performance of\\nthe crystal graph convolution neural network (CGCNN)639 by\\nadding to the original framework information about theVoronoi tessellated crystal structures, which are explicit 3-body\\ncorrelations of neighboring constituent atoms, and an\\noptimized representation of interatomic bonds. The new\\napproach that was labeled as iCGCNN achieved a predictiveaccuracy 20% higher than that of the original CGCNN when\\ndetermining thermodynamic stabilities of compounds (i.e.,\\npredictions of hull distances). When used for high-throughput\\nsearches, iCGCNN exhibited a success rate higher than anundirected high-throughput search and higher than that of\\nCGCNN.Figure 11 shows the improvement in predictions of\\nnearly stable compounds afte r using more appropriate\\ndescriptors. This study showcases how descriptors can betailored to further enhance the success of ML-aided high-\\nthroughput screening.\\n5.2. Retrosynthetic Technologies\\nA grand challenge in chemistry is to understand synthetic\\npathways to desired molecules.640,641 Retrosynthesis involvesthe design of chemical steps to produce molecules and\\nmaterials that would be crucial to drug discovery, medicinal\\nchemistry, and materials science. As a diﬀerent kind of\\noptimization problem, the general tactic is to analyze atomicscale compounds recursively, map them onto synthetically\\nachievable building blocks, and then assemble those blocks\\ninto the desired compound.642−644\\nThree main issues make retrosynthesis a formidableachievable building blocks, and then assemble those blocks\\ninto the desired compound.642−644\\nThree main issues make retrosynthesis a formidable\\nintellectual challenge. 645 First, simple combinatorics makethe space of possible reactions greater than the space of\\npossible molecules. Second, reactants seldom contain only one\\nreactive functional group, and thus require predictions of\\nmultiple functional groups. Third, one failed step in the routecan invalidate the entire synthesis because organic synthesis is\\na multistep process.\\nGiven these challenges, ML is becoming more established in\\ndetermining reaction rules from CompChem data.641 Com-puter-aided synthesis planning was actuallyﬁrst attempted in\\nthe 1960s.646 Many have since attempted to formalize chemical\\nperception and synthetic th inking using computer pro-\\ngrams.647−649 These programs are typically based on one ofthree possible algorithms:649\\n1. Algorithms that use reaction rules (manually encoded or\\nautomatically derived from databases).\\n2. Algorithms that use principles of physical chemistry\\nbased on ab initio calculations to predict energy barriers.3. Algorithms based on ML techniques.\\nML approaches are used to try to overcome the general-\\nization issues of rule-based algorithms (that normally suﬀer\\nfrom incompleteness, infeasible suggestions, and human bias)while also avoiding the high cost of CompChem calculations.\\nIt is now possible to obtain purely data-driven approaches for\\nsynthesis planning, which are promoting a rapid advancement\\nin theﬁeld. For example, Coley and co-workers650 designed adata-driven metric, SCScore, for describing a real synthesis\\nmodeled after the idea that products are, on average, more\\nFigure 10.NN predictions compared to TD-DFT derived data of log\\nkTADF (R2 = 0.94). ML models computed molecular properties neededfor screening with an accuracy comparable to CompChem\\ncalculations, but at a fraction of the computational cost. Reprinted\\nby permission from Gómez-Bombarelli, R.; Aguilera-Iparraguirre, J.;Hirzel, T. D.; Duvenaud, D.; Maclaurin, D.; Blood-Forsythe, M. A.;\\nChae, H. S.; Einzinger, M.; Ha, D. G.; Wu, T., et al. Design of\\nEﬃcient Molecular Organic Light-Emitting Diodes by a High-Throughput Virtual Screening and Experimental Approach. Nat.\\nMater. 2016, 15, 1120−1127.632 Copyright 2016 Springer Nature,\\nNature Materials.\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107https://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9846synthetically complex than each of their reactants. The\\ndeﬁnition of a metric for selecting the most promising\\ndisconnections that produce easily synthesizable compounds\\nis crucial for avoiding combinatorial explosions.Figure 12shows that a data-driven metric, the SCScore, is more suitable\\nthan other heuristic metrics to perceive the complexity of each\\nstep in a given synthesis. This work o ﬀered a valuable\\ncontribution to the retrosynthesis working pipeline byproviding a method that implicitly learns what structures and\\nmotifs are more prevalent as reactants.\\nApart from isolated approaches or algorithms to deal with\\nspeciﬁc tasks within retrosynthesis, there is already softwareavailable to advance thisﬁeld. One example is the Chematica\\nprogram,651 which has implemented a new module that\\ncombines network theory, modern high-power computing, AI,\\nand expert chemical knowledge to design synthetic pathways.A scoring function is used to promote synthetic brevity and\\npenalize any reactivity conﬂicts or nonselectivities, thus\\nallowing it to ﬁnd solutions that might be hard for a human\\nto identify. Figure 13A shows the decision tree for one of thealmost 50 000 reaction rules used in Chematica. Reaction rules\\ncan be considered as the allowed moves from which the\\nsynthetic pathways are built, and such moves lead to an\\nenormous synthetic space (the number of possibilities withinnsteps scales as 100n) as the one shown by the graph inFigure\\n13B .C h e m a t i c ae x p l o r e st h i sl a r g es y n t h e t i cs p a c eb y\\ntruncating and reverting from unpromising connections anddrives its searches to the most eﬃcient sequences of steps.\\nMoreover, in the pathways presented to the user, each\\nsubstance can be further analyzed with molecular mechanics\\ntools. This software was used to obtain insights into thesynthetic pathways to eight targets (seven bioactive substances\\nand one natural product). All of the computer-planned routes\\nwere not only successfully carried out in the laboratory, but\\nthey also resulted in improved yields and cost savings overprevious known paths. This work opened an avenue for\\nchemists toﬁnally obtain reliable pathways from in silico\\nretrosynthesis. For further reading we recommend the two-part\\nreviews of Coley and co-workers.652,653\\n5.3. Catalysis5.3. Catalysis\\nCatalysis research involves understanding how to impact\\nchemical product yields and selectivities. 654 Traditional\\ncatalysis is normally discussed in textbooks in terms of\\nhomogeneous (i.e., within a solution phase), heterogeneous(occurring at a solid/liquid interface), and biological\\n(occurring within enzymes and riboenzymes), but it is best\\nnot to use these terms too strictly because actual reactionFigure 11. DFT vs ML predicted hull distances of nearly stable compounds (hull distances smaller than 50 meV/atom) for CGCNN andiCGCNN. The ﬂexibility of ML approaches enable constructions of robust models tailored for speciﬁc target properties. See ref638.\\nFigure 12. Use of diﬀerent metrics to analyze the synthesis of aprecursor to lenvatinib. Only the SCScore, a data-driven metric,\\ncorrectly perceives a monotonic increase in complexity. ML models\\ncan give insights into which compounds are either reactants orproducts. Reprinted with permission from ref647. Copyright 2018\\nAmerican Chemical Society.\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9847mechanisms can be quite complex and overall processes may\\nsometimes exhibit characteristics of two or more of these\\nclassical processes.655−657 Modern research in catalysis has\\nbeen interested in studying chemical reactivity and reactionselectivity arising from stimuli from solar −thermal en-\\nergy,658,659 electrochemical potentials, 660 photons, 661−664\\nplasmas,665,666 or other external resonances. 667 Catalysis\\nmakes up roughly 35% of the world ’s gross domesticproduct,668 and it is important to guide toward the end goal\\nof achieving greater sustainability with catalytic pro-\\ncesses.669−671\\nThese reasons help make catalysis a fertile training groundfor applying and developing theoretical models (e.g., refs\\n672−674) that can be used along with CompChem or\\nCompChem+ML. The research ﬁeld is also burgeoning with\\nmany reports and review articles 544,675−679 that discussperspectives and progress using ML methods for catalysis\\nscience. Here, we will mention notable examples. For example,\\nCompChem+ML methods are enabling more data generation\\nby allowing costly CompChem calculations to be run moreeﬃciently, and more information means more comprehensive\\npredictions of chemical and materials phase diagrams for\\ncatalysis680,681 as well as stability and reactivity descriptorsFigure 13.(A) Decision tree of one of the reaction rules within Chematica (double stereodiﬀerentiating condensation of esters with aldehydes).The diﬀerent conditions in the tree specify the range of admissible and possible substituents or atom types. (B) Reaction rules are used to explorethe graph of synthetic possibilities (similar to the one shown here). Each node corresponds to a set of substrates. The combination of expertchemical knowledge, CompChem calculations and ML enablesﬁnding synthesizable paths. See ref651. Reprinted fromChem, 4(3), Klucznik, T.,et al., Eﬃcient Syntheses of Diverse, Medicinally Relevant Targets Planned by Computer and Executed in the Laboratory, 522−532, Copyright\\n(2018), with permission from Elsevier.\\nChemical Reviews pubs.acs.org/CR ReviewChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9848identiﬁed on theﬂy.682−686 Figure 14 shows examples of the\\npalettes of insight available using state-of-the-art CompChem\\n+ML modeling for identifying activity and selectivity maps, as\\nwell as visualizations of data using t-SNE.687well as visualizations of data using t-SNE.687\\nRegarding modeling of deeply complex chemical environ-\\nments, Artrith and Kolpak developed MLPs for investigating\\nthe relationships between solvent, surface composition andmorphology, surface electronic structure, and catalytic activity\\nFigure 14. CompChem+ML screening of hypothetical Cu and Cu-based catalyst sites. (a) Two-dimensional activity volcano plot for CO2reduction. TOF, turnover frequency. (b) Two-dimensional selectivity volcano plot for CO2 reduction. CO and H adsorption energies in panels aand b were calculated using DFT. Yellow data points are average adsorption energies of monometallics; green data points are average adsorptionenergies of copper alloys; and magenta data points are average, low-coverage adsorption energies of Cu−Al surfaces. (c) t-SNE687 representation ofapproximately 4000 adsorption sites on which DFT calculations were performed with Cu-containing alloys. The Cu−Al clusters are labelednumerically. (d) Representative coordination sites for each of the clusters labeled in the t-SNE diagram. Each site archetype is labeled by thestoichiometric balance of the surface, that is, Al-heavy, Cu-heavy or balanced, and the binding site of the surface. See ref688. Reprinted bypermission from Springer Nature Customer Service Centre GmbH: Springer Nature,Nature. Accelerated discovery of CO2 electrocatalysts using\\nactive machine learning, Zhong, M., et al., Copyright 2020.Figure 15.Estimated price (for one mmol in US dollars) of the catalysts in the selected range of−32.1/−23.0 kcal mol−1 (for ligand no. 72-90).The price is calculated as a summation of the commercial price of transition metal precursors (one mmol) and one mmol of each ligand. Thecheapest complex for each metal is shown on the right. The estimated price of all the 557 catalysts is detailed in ref694. Published by The Royal\\nSociety of Chemistry.\\nChemical Reviews pubs.acs.org/CR ReviewChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9849in systems composed of thousands of atoms interfaces.689 We\\nexpect such simulations for el ectro- and photocatalysis\\nelucidation will continue to improve in size, scale, and\\naccuracy. For other physical insights, new approaches byKulik, Getman, and co-workers have also focused on\\ndeveloping ML models appropriate for elucidating complex\\nd-orbital participation in homogeneous catalysis.690 Rappe and\\nco-workers have used regularized random forests to analyzehow local chemical pressure eﬀects adsorbate states on surface\\nsites for the hydrogen evolution reaction.691 Almost trivially\\nsimple ML approaches can be used in catalysis studies to\\ndeduce insights into interaction trends between single metalatoms and oxide supports,692 to identify the signiﬁcance of\\nfeatures (e.g., adsorbate type or coverage), where CompChem\\ntheories break down,693 or they can be used to identify trends\\nthat result in optimal catalysis across multiple objectives, suchas activity and cost (Figure 15).694\\nML is also opening opportunities for CompChem+ML\\ns t u d i e so nh i g h l yd e t a i l e da n dc o m p l e xn e t w o r k so f\\nreactions.695−700 Such models in principle can then signiﬁ-cantly extend the range of utility of microkinetics modeling for\\npredictions of products from catalysis.701,702 ML also enables\\nstudies of complicated reaction networks that can allow\\npredictions of regioselective products based on CompChemdata,703 asymmetric catalysis important for natural product\\nsynthesis,704,705 and biochemical reactions.706 Eﬀorts to better\\nunderstand “above-the-arrow ” optimizations of reaction\\nconditions relate back to the challenge of retrosyntheticchallenges.707,708 Ideally, these e ﬀorts will continue while\\nmaking use of rapid advances in CompChem+ML that enable\\npredictive atomistic simulations to be run faster and more\\naccurately. We see reason for excitement for di ﬀerentapproaches, but we again stress the importance of ensuring\\nthat models will provide unique and physical results (see\\nsection 3 w h e r ew ed i s c u s st h er i s ko f“clever Hans ”\\npredictors360).\\n5.4. Drug Designpredictors360).\\n5.4. Drug Design\\nThe central objective for drug discovery is toﬁnd structurally\\nnovel molecules with precise selectivity for a medicinal\\nfunction. This involves identifying new chemical entities andobtaining structures with di ﬀerent physicochemical and\\npolypharmacological properties (i.e., combinations of beneﬁ-\\ncial pharmacological eﬀects or adverse side-eﬀects).709,710 Drug\\ndiscovery involves the identi ﬁcation of targets (a propertyoptimization task, as in material design) and the determination\\nof compounds with good on-target eﬀects and minimal oﬀ-\\ntarget eﬀects.711 Traditionally, a drug discovery program may\\ntake around six years before a drug candidate can be used inclinical trials, and six or seven more years are required for three\\nFigure 16. (A) Workﬂow and timeline for the design of candidates employing GENTRL. (B) Representative examples of the initial 30,000structures compared to the parent DDR1 kinase inhibitor. (C) Compounds found to have the highest inhibition activity against human DDR1kinase. CompChem+ML methods can considerably accelerate the discovery of drugs that are eﬀective against a desired target. See ref617.Reprinted by permission from Springer Nature Customer Service Centre GmbH: Springer Nature,Nature Biotechnology. Deep learning enables\\nrapid identiﬁcation of potent DDR1 kinase inhibitors, Zhavoronkov, A., et al., Copyright 2019.Chemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9850clinical phases. Thus, it is important to identify adverse eﬀects\\nas soon as possible to minimize time and monetary costs.712\\nAccelerating drug discovery relies on predicting how and\\nwhere a certain drug binds to more than one protein, aphenomenon that sometimes results in polypharmacology.\\nResearchers are developing ready-to-use tools aimed to\\nfacilitate research for drug discovery,713 but CompChem+ML\\nis expected to continue providing even more beneﬁts to thedrug development pipeline.714\\nIn a recent study, Zhavoronkov et al.617 developed a deep\\ngenerative model for de novo small-molecule design: the\\ngenerative tensorial reinforcement learning (GENTRL) modelthat was used to discover potent inhibitors of discoidin domain\\nreceptor 1 (DDR1), a kinase target implicated inﬁbrosis and\\nother diseases. The drug discovery process was carried out in\\nonly 46 days, beginning with the recollection of appropriatedata for training andﬁnishing with the synthesis and\\nexperimental test of some compounds (Figure 16A). GENTRL\\nwas used to screen a total of 30 000 structures (some examples\\ncompared to the parent DDR1 kinase inhibitor are shown inFigure 16B) down to only 40 structures that were randomly\\nselected ensuring a coverage of the resulting chemical space\\nand distribution of root-mean squared deviation values. Six of\\nthese molecules were then selected for experimental validation(seeFigure 16C), with one of them demonstrating favorable\\npharmacokinetics in mice. The predicted conformation of the\\nsuccessful compound according to pharmacophore modeling\\nwas very similar to the one predicted to be preferred and stableby CompChem methods. This work illustrates the utility of\\nCompChem+ML approaches to give insights into drug design\\nby rapidly giving compound candidates that are synthetically\\nfeasible and active against a desired target.feasible and active against a desired target.\\nBesides generating new chemical structures with favorable\\npharmacokinetics, ML methods are also used in pharmaceut-\\nical research and development for peptide design, compoundactivity prediction and for assisting scoring protein−ligand\\ninteraction (docking).709,715−717 An example of the latter was\\nproposed by Batra et al.718 for eﬃciently identifying ligands\\nthat can potentially limit the host−virus interactions of SARS-CoV-2. Those authors designed a high-throughput strategy\\nbased on CompChem+ML that involved high-ﬁdelity docking\\nstudies to ﬁnd candidates displaying high-binding aﬃnities.\\nThe ML model was used to search through thousands ofapproved ligands by the Food and Drug Administration (FDA)\\nand a million biomolecules in the BindingDB database. 514\\nFrom these, insights were obtained for more than 19 000\\nmolecules satisfying the Vina score (i.e., an importantphysicochemical measure of the therapeutic process of a\\nmolecule that is used to rank molecular conformations and\\npredict free energy of binding).Figure 17shows the Vina score\\npredictions that led to the selection of the best candidates,some of which are also illustrated in theﬁgure. The Vina scores\\nfor the top ligands were further conﬁrmed using expensive\\ndocking approaches, resulting in the identiﬁcation of 75 FDA-\\napproved and 100 other ligands potentially useful to treatSARS-CoV-2. This study highlights a reasonable CompChem\\n+ML strategy for making useful suggestions to aid expert\\nbiologists and medical professionals to focus in fewer\\ncandidates when performing either robust CompChem eﬀortsor synthesis and trial experiments.\\n6. CONCLUSIONS AND OUTLOOK\\nRecent CompChem methods, algorithms, and codes have\\nempowered new studies for a wealth of physical and chemical\\ninsights into molecules and materials. Today, the combinationof CompChem+ML can be equipped to address new and more\\nchallenging questions in diﬀerent domains of physics, materials\\nscience, chemistry, biology, and medicine. Productive research\\neﬀorts in this direction necessitate interdisciplinary teams andincreasing availability of high-quality data across appropriatescience, chemistry, biology, and medicine. Productive research\\neﬀorts in this direction necessitate interdisciplinary teams and\\nincreasing availability of high-quality data across appropriate\\nregions of chemical compound space. Discovering newchemicals and materials requires thorough investigations.\\nOne needs to predict reaction pathways and interactions\\nbetween molecules, optimize environmental conditions for\\ncatalytic reactions, enhanceselectivities that eliminateundesired side reactions or side eﬀects, and navigate other\\nsystem-speciﬁc degrees of freedom. Addressing this complexity\\ncalls for a statistical view on chemical design and discovery,\\nand CompChem+ML provides a natural synergy for obtainingpredictive insights to lead to wisdom and impact.\\nThis Review provided a bird’s-eye view of CompChem and\\nML and how they can be used together to make transformative\\nimpacts in the chemical sciences. The successes of CompChem+ML are particularly visible in physical chemistry and include\\ndrastic acceleration of molecular and materials modeling,\\ndiscovery and prediction of chemicals with desired properties,Figure 17. Vina scores predictions for the isolated protein (S-protein) and the protein-receptor complex (interface) for all the molecules in theBindingDB data sets and some exemplary top cases that satisfy the screening criteria. ML models trained on accurate CompChem databases are ofupmost importance to eﬃciently gain insights into possible treatments, even for newly discovered diseases. Figure taken from ref718. Copyright\\n2020 American Chemical Society.\\nChemical Reviews pubs.acs.org/CR ReviewChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9851prediction of reaction pathways, and design of new catalysts\\nand drug candidates. Nevertheless, we have only begun to\\nscratch the surface of how successful applications of ML in\\nchemistry can bring impact. There are many conceptual,theoretical, and practical challenges waiting to be solved to\\nenable further synergies within thetroika of CompChem, ML,\\nand CPI. Here we enumerate some of the challenges that we\\nconsider to be the most pressing and interesting at this\\nmoment:moment:\\n1. Reliance on ML in CompChem algorithms must be\\nincreased : ML algorithms can be integrated into\\nCompChem algorithms at almost any simulation level\\n(Figure 3 ). ML algorithms are already available toaccelerate calculations of CompChem energies, navi-\\ngations along reaction pathways, and sampling of larger\\nregions of the PES, but the reluctance of their use\\nimpedes progress. In general, these algorithms must bemade more eﬀective, eﬃcient, accessible, user-friendly,\\nand reproducible to beneﬁt fundamental and applied\\nresearch (see for example, ref719.).\\n2. More general ML approaches are needed: ML methods\\nmust continue to evolve beyond now-common applica-tions of learning a narrow region of a PES or identifying\\nstraightforward structure/property relationships. New\\nML methods should have the capacity to predict\\nenergetic and electronic properties and their moreconvoluted relationships across chemical space. Such\\napproaches should grow toward uniformly describing\\ncompositional (chemical arrangement of atoms in a\\nmolecule) and conﬁgurational (physical arrangement ofatoms in space) degrees of freedom on equal footing.\\nFurther progress in thisﬁeld requires developing new\\nuniversal ML models suitable for insights across diverse\\nsystems and physicochemical properties.systems and physicochemical properties.\\n3. ML representations must include the right physics :M L\\nmethods that are claimed to be accurate but incorrectly\\ndescribe the true physics of a system will eventually failto achieve meaningful insights while lowering the\\nreputation of other work in theﬁeld. Current ML\\nrepresentations (descriptors) can successfully describe\\nlocal chemical bonding, but few if any are treating long-range electrostatics, polarization, and van der Waals\\ndispersion interactions that are critical for rationalizing\\nphysical systems, both large and small. Combining\\nintermolecular interaction theory (a key focus ofintermolecular interaction theory (a key focus of\\nadvanced CompChem methods) with ML is an\\nimportant direction for future progress toward studying\\ncomplex molecular systems.\\n4. CompChem + ML applications need to strive towardachieving realistic complexity: Investigations using highly\\naccurate CompChem methods normally require overly\\nsimpliﬁed model systems while more realistic model\\nsystems necessitate less accurate but computationallyeﬃcient CompChem methods. This compromise should\\nno longer be necessary. We are due for a paradigm shift\\nin how thermodynamics, kinetics, and dynamics of\\nsystems in complex chemical environments (e.g., formultiscale biological processes like drug design and/or\\ncatalytic processes at solid−liquid interfaces under\\nphotochemical excitations, etc.) can be treated more\\nfaithfully with less corner-cutting. An emerging idea is todispatch ML approaches into computationally eﬃcient\\nmodel Hamiltonians for electronic interactions based on\\ncorrelated wavefunction, KS-DFT, tight-binding, molec-\\nular orbital techniques, and/or the many-body dis-persion method. ML can predict Hamiltonian parame-\\nters and the quantum-mechanical observables would be\\ncalculated via diagonalization of the corresponding\\nHamiltonian. The challenge is toﬁnd an appropriatebalance between prediction accuracy and computational\\neﬃciency to dramatically enhance larger scale simu-\\nlations.\\n5. Much more experimental data is needed: Validations of\\nML predictions require extensive comparisons withML predictions require extensive comparisons with\\nexperimental observables such as reaction rates,\\nspectroscopic observations, solvation energies, and\\nmelting temperatures. Such experiments may haveML predictions require extensive comparisons with\\nexperimental observables such as reaction rates,\\nspectroscopic observations, solvation energies, and\\nmelting temperatures. Such experiments may havemelting temperatures. Such experiments may have\\npreviously been considered too routine, too mundane,\\nor not insightful enough alone, but all high quality brings\\ngreat value for future CompChem+ML eﬀorts thatgreat value for future CompChem+ML eﬀorts that\\ntightly integrate quantum mechanics, statistical simu-\\nlations, and fast ML predictions, all within a\\ncomprehensive molecular simulation framework.720comprehensive molecular simulation framework.720\\n6. Much more comprehensive data sets need to be assembled\\nand curated : Current CompChem+ML e ﬀorts have\\nproﬁted heavily by the availability of benchmark datasets for relatively small molecules that allow a\\ncomparison of existing models.413,527 While e ﬀorts\\nﬁxated on boosting prediction accuracies and shrinking\\ndown requisite training set sizes for ML models have hadtheir merits, it is time to move on as further\\nimprovements are meaningless if the ML models are\\nnot making useful and insightful predictions themselves.\\nMore useful predictions will require knowledge fromlarger data sets, and these will inevitably contain\\nheterogeneous combinations of diﬀerent levels of theory\\nor experiments that must be analyzed, “cleaned”, and\\nuncertainties adequately quanti ﬁed for models touncertainties adequately quanti ﬁed for models to\\nproductively learn. Such hybrid data sets may be the\\nkey to arrive at novel hypotheses in chemistry that could\\nthen be experimentally tested.\\n7. Bolder and deeper explorations of chemical space areneeded: So far most eﬀorts to generate chemical data\\nhave focused on exploring parts of chemical space for\\nnew compounds for a targeted purpose. This should\\nchange. Combining ML model uncertainty estimateschange. Combining ML model uncertainty estimates\\nacross broader swaths of chemical space could open\\npathways for fruitful statistical explorations, say, in an\\nactive learning framework. This could lead to discover-ing new synergies between data that otherwise would\\nnot have been possible to enable advances in scientiﬁc\\nunderstanding and improve ML models. Generative\\nmodels can bridge the gap between sampling andmodels can bridge the gap between sampling and\\ntargeted structure generation imposing optimal com-\\npound properties, for example, for inverse chemical\\ndesign.125,621,622\\nThis and other reviews20,554,634,720−724 have stated how MLhas become instrumental for recent progress in CompChem.\\nWe would like to also mention inspirations that ML has drawn\\nfrom being applied to physical and chemical problems.\\nML methods generally assume that data is subject tomeasurement noise while CompChem data is generally\\napproximate but also noise-free from a statistical perspective.\\nML modeling still requires regularization, but regularizers\\nshould reﬂect the underlying physics of molecular andChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9852materials systems. ML models used in applications of vision\\ncontain discrete convolution ﬁlters that are suboptimal for\\nchemical modeling, but recognition of this shortcoming has led\\nto novel continuous convolutionﬁlters that are well suited forchemistry and have also become a popular novel architecture\\nfor core ML methods.434\\nFurthermore, invariances, symmetries, and conservation laws\\nare key ingredients to physical and chemical systems.Incorporating them into ML has led to novel and useful\\nmodels for chemistry since they can learn from signiﬁcantly\\nless data, which then makes it possible to build forceﬁelds at\\nunprecedentedly high levels of theory.206,207,372 Using thesepowerful ML techniques for computer vision, natural language\\nprocessing, and other applications is currently being explored.\\nStructural information from molecular graphs provide the basis\\nfor novel tensor NNs or message passing architectures,352,421as well as graph explanation methods.725\\nMany further challenges exist that have led or will lead to\\nmutual bidirectional cross-fertilization between ML and\\nchemistry. These interdisciplinary eﬀorts also initiate progressin respective application domains. The power of this path is\\nthat solving a burning problem in chemistry with a novel\\ncrafted ML model may also result in unforeseen insights in\\nhow to better design core ML methods. Interestingly, theexploratory usage of ML for knowledge discovery in chemistry\\ntypically requires novel ML models and unforeseen scientiﬁc\\ninnovations, and this can lead to interesting insight that is notnecessary limited to chemistry alone, rather it is likely to go\\nbeyond.\\nTo conclude, the past decade has shown that it has not been\\nenough to just apply existing ML algorithms, but break-\\nthroughs are happening by a handshaking of innovationsresulting in novel ML algorithms and architectures driven by\\nthe pursuit of novel insights in chemistrywhile retaining a deep\\nunderstanding about the underlying physical and chemical\\nprinciples. Research programs that foster interdisciplinaryexchange, such as IPAM (www.ipam.ucla.edu), have seeded\\nthis progress, and these should be continued. Mixed teams\\nwith members educated in di ﬀerent aspects of physics,\\nchemistry and ML have been instrumental. This also bringsthe need to solve the new educational challenge of developing\\nnew generations of researchers with an academic curriculum\\nthat interweaves chemistry, physics and computer science to\\nenable a meaningful (multilingual) research contribution tothis exciting emergingﬁeld.\\nAUTHOR INFORMATION\\nCorresponding Authors\\nJohn A. Keith− Department of Chemical and Petroleum\\nEngineering Swanson School of Engineering, University of\\nPittsburgh, Pittsburgh, Pennsylvania 15261, United States;orcid.org/0000-0002-6583-6322; Email: jakeith@\\npitt.edu\\nKlaus-Robert Müller − Machine Learning Group, Technische\\nUniversität Berlin, 10587 Berlin, Germany; Department of\\nArtiﬁcial Intelligence, Korea University, Seoul 02841, Korea;Max-Planck-Institut für Informatik, 66123 Saarbrücken,\\nGermany; Google Research, Brain Team, Berlin, Germany;\\norcid.org/0000-0002-3861-7685; Email: klaus-\\nrobert.mueller@tu-berlin.de\\nAlexandre Tkatchenko − Department of Physics andAlexandre Tkatchenko − Department of Physics and\\nMaterials Science, University of Luxembourg, L-1511\\nLuxembourg City, Luxembourg; orcid.org/0000-0002-\\n1012-4854; Email: alexandre.tkatchenko@uni.lu\\nAuthorsAuthors\\nValentin Vassilev-Galindo − Department of Physics and\\nMaterials Science, University of Luxembourg, L-1511\\nLuxembourg City, Luxembourg;orcid.org/0000-0001-\\n7532-3590\\nBingqing Cheng − Accelerate Programme for ScientiﬁcDiscovery, Department of Computer Science and Technology,\\nCambridge CB3 0FD, United Kingdom;orcid.org/0000-\\n0002-3584-9632\\nStefan Chmiela − Department of Software Engineering and\\nTheoretical Computer Science, Technische Universität Berlin,10587 Berlin, Germany\\nMichael Gastegger− Department of Software Engineering and\\nTheoretical Computer Science, Technische Universität Berlin,10587 Berlin, Germany\\nMichael Gastegger− Department of Software Engineering and\\nTheoretical Computer Science, Technische Universität Berlin,\\n10587 Berlin, Germany\\nComplete contact information is available at:Complete contact information is available at:\\nhttps://pubs.acs.org/10.1021/acs.chemrev.1c00107\\nNotes\\nThe authors declare no competingﬁnancial interest.\\nBiographies\\nJohn A. Keith is an associate professor and R.K. Mellon FacultyFellow in Energy at the University of Pittsburgh in the department of\\nchemical and petroleum engineering. He obtained his bachelors’ in\\nchemistry at Wesleyan University and a Ph.D. degree in computa-tional chemistry at Caltech in 2007. After an Alexander von\\nHumboldt postdoctoral fellowship at the Universität Ulm, he was\\nan Associate Research Scholar at Princeton University. He was arecipient of an NSF-CAREER award in 2017. His research interests lie\\nin the applications and development of computational chemistry for\\nengineering chemical reactions and materials for electrocatalysis,anticorrosion coatings, and the development of chemicals having less\\nof an environmental footprint. He was a recipient of a Luxembourg\\nScience Foundation INTER Mobility award in 2019−2020 to do aresearch sabbatical in Prof. Alexandre Tkatchenko ’s group at the\\nUniversity of Luxembourg. This Review is a primary product of that\\nvisit.\\nValentin Vassilev-Galindo graduated with honors from University ofVeracruz (Mexico) with a Bachelor’s degree in Chemical Engineering\\nin 2014. Then, he enrolled to the Master program in Physical\\nChemistry at Cinvestav-Mérida (Mexico), where he worked under thesupervision of Professor Gabriel Merino until receiving the MSc.\\ndegree in 2017. He is currently pursuing a PhD degree at the\\nUniversity of Luxembourg in the research group of Professor\\nAlexandre Tkatchenko. His research is mainly related to machinelearning potentials.\\nBingqing Cheng is a Departmental Early Career Fellow at the\\nComputer Laboratory, University of Cambridge, and a Junior\\nResearch Fellow at Trinity College. She received her Ph.D. fromthe École Polytechnique Fédérale de Lausanne (EPFL) in 2019. Her\\nwork focuses on theoretical predictions of material properties.\\nStefan Chmiela is a senior researcher at the Berlin Institute for theFoundations of Learning and Data (BIFOLD). He received his Ph.D.\\nfrom Technische Universität Berlin in 2019. His research interests\\ninclude Hilbert space learning methods for applications in quantumchemistry, with particular focus on data eﬃciency and robustness.\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9853Michael Gastegger is a postdoctoral researcher in the BASLEARN\\nproject of the Machine Learning Group at Technische Universität\\nBerlin. He received his Ph.D. in Chemistry from the University ofVienna in Austria in 2017. His research interests include the\\ndevelopment of machine learning methods for quantum chemistry\\nand their application in simulations.\\nKlaus-Robert Müller has been a professor of computer science atTechnische Universität Berlin since 2006; at the same time he is\\ndirecting and codirecting the Berlin Machine Learning Center and the\\nBerlin Big Data Center, respectively. He studied physics in Karlsruhefrom 1984 to 1989 and obtained his Ph.D. degree in computer science\\nat Technische Universität Karlsruhe in 1992. After completing a\\npostdoctoral position at GMD FIRST in Berlin, he was a researchfellow at the University of Tokyo from 1994 to 1995. In 1995, he\\nfounded the Intelligent Data Analysis group at GMD-FIRST (later\\nFraunhofer FIRST) and directed it until 2008. From 1999 to 2006, hewas a professor at the University of Potsdam. He was awarded the\\nOlympus Prize for Pattern Recognition (1999), the SEL Alcatel\\nCommunication Award (2006), the Science Prize of Berlin by theGoverning Mayor of Berlin (2014), and the Vodafone Innovations\\nAward (2017). In 2012, he was elected member of the German\\nNational Academy of Sciences-Leopoldina; in 2017, a member of the\\nBerlin Brandenburg Academy of Sciences; and also in 2017, anexternal scientiﬁc member of the Max Planck Society. In 2019 and\\n2020, he became a Highly Cited researcher in the cross-disciplinary\\narea. His research interests are intelligent data analysis and MachineLearning in the sciences (Neuroscience (speciﬁcally Brain-Computer\\nInterfaces), Physics, Chemistry) and in industry.\\nAlexandre Tkatchenko is a Professor of Theoretical Chemical Physicsat the University of Luxembourg and Visiting Professor at Technische\\nUniversität Berlin. He obtained his bachelor degree in Computer\\nScience and a Ph.D. in Physical Chemistry at the UniversidadAutonoma Metropolitana in Mexico City. Between 2008 and 2010, he\\nwas an Alexander von Humboldt Fellow at the Fritz Haber Institute of\\nthe Max Planck Society in Berlin. Between 2011 and 2016, he led anindependent research group at the same institute. Tkatchenko serves\\non editorial boards of two society journals:Physical Review Letters\\n(APS) and Science Advances (AAAS). He received a number ofawards, including elected Fellow of the American Physical Society, the\\n2020 Dirac Medal from WATOC, the Gerhard Ertl Young\\nInvestigator Award of the German Physical Society, and twoﬂagshipgrants from the European Research Council: a Starting Grant in 2011\\nand a Consolidator Grant in 2017. His group pushes the boundaries\\nof quantum mechanics, statistical mechanics, and machine learning todevelop eﬃcient methods to enable accurate modeling and obtain\\nnew insights into complex materials.\\nACKNOWLEDGMENTS\\nJ.A.K. was supported by the Luxembourg National Research\\nFund (INTER/MOBILITY/19/13511646) and the U.S.Fund (INTER/MOBILITY/19/13511646) and the U.S.\\nNational Science Foundation (CBET-1653392 and CBET-\\n1705592). V.V.G. acknowledgesﬁnancial support from the\\nLuxembourg National Research Fund (FNR) under theLuxembourg National Research Fund (FNR) under the\\nprogram DTU PRIDE MASSENA (PRIDE/15/10935404).\\nB.C. acknowledges funding from the Swiss National Science\\nFoundation (Project P2ELP2-184408). KRM was supported inpart by Institute of Information & Communications Technol-\\nogy Planning & Evaluation (IITP) grants funded by the Korea\\nGovernment (No. 2017-0-00451, Development of BCI-based\\nBrain and Cognitive Computing Technology for RecognizingUser’s Intentions using Deep Learning) and funded by the\\nKorea Government (No. 2019-0-00079, Artiﬁcial Intelligence\\nGraduate School Program, Korea University) and was partly\\nsupported by the German Ministry for Education and Research(BMBF) under Grants 01IS14013A-E, 01GQ1115,\\n01GQ0850, 01IS18025A, 031L0207D, and 01IS18037A; the\\nGerman Research Foundation (DFG) under Grant Math+,(BMBF) under Grants 01IS14013A-E, 01GQ1115,\\n01GQ0850, 01IS18025A, 031L0207D, and 01IS18037A; the\\nGerman Research Foundation (DFG) under Grant Math+,\\nEXC 2046/1, Project ID 390685689. A.T. acknowledgesﬁnancial support from the European Research Council (ERC\\nConsolidator Grant BeStMo and ERC-POC Grant DISCOV-\\nERER). We gratefully acknowledge helpful comments on the\\nmanuscript by Hartmut Maennel.\\nACRONYMS\\nACE atomic cluster expansionACRONYMS\\nACE atomic cluster expansion\\nACS American Chemical Society\\nACSF atom-centered symmetry function\\nAE autoencoders\\nAI artiﬁcial intelligence\\nAPI application programming interfaces\\nBoB bag of bonds\\nBOP bond order potential\\nBP back-propagationBOP bond order potential\\nBP back-propagation\\nCGCNN crystal graph convolution neural network\\nCNN convolutional neural network\\nCOSMO conductor-like screening model\\nC-PCM conductor polarizable continuum solvent\\nmodelmodel\\nCASPT2 complete active space perturbation theory\\nCASSCF complete active space self-consistentﬁeld\\nCBS complete basis set\\nCI conﬁguration interaction\\nCMD centroid molecular dynamics\\nCompChem computational chemistryCompChem computational chemistry\\nCPI chemical and physical intuition\\nCV collective variable\\nDDR1 discoidin domain receptor 1\\nDeepPot-SE smooth edition version of the DeepMD\\npotential\\nD-PCM dielectric polarizable continuum solvent modelDFT density-functional theory\\nDFTB density functional tight binding\\nDLPNO domain-based local pair natural orbital\\nDMRG density matrix renormalization group theory\\nDTNN deep tensor neural network\\nEAM embedded atom methodEAM embedded atom method\\nEANN embedded atom neural network\\nECP eﬀective core potential\\nFCHL Faber −Christensen−Huang−Lilienfeld\\nFCI full con ﬁguration interaction\\nFDA Food and Drug Administration\\nFES free energy surface\\nFF forceﬁeldFES free energy surface\\nFF forceﬁeld\\nFPS farthest point sampling\\nGAN generative adversarial network\\nGENTRL generative tensorial reinforcement learning\\nGGA generalized gradient approximation\\nGP Gaussian processes\\nGPU graphical processing unitsGPU graphical processing units\\nGVB generalized valence bond\\nHEAT high accuracy extrapolated ab initio thermo-\\nchemistry\\nHF Hartree −Fock\\nHIP-NN hierarchical interacting particle neural network\\nICA independent component analysisICA independent component analysis\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9854IEFPCM integral equation formulation of polarizable\\ncontinuum solvent model\\nKE kinetic energy\\nKPCA kernel principal component analysis\\nKRR kernel ridge regression\\nKS Kohn−Sham\\nLDA local density approximation\\nLJ Lennard-JonesLDA local density approximation\\nLJ Lennard-Jones\\nMBTR many-body tensor representation\\nMD molecular dynamics\\nMEAM modiﬁed embedded atom method\\nML machine learning\\nMLP machine learning potential\\nMPNN message-passing neural networkMPNN message-passing neural network\\nMRCC multireference coupled cluster\\nMRCI multireference con ﬁguration interaction\\nMS/MS tandem mass spectroscopy\\nNDDO neglect of diatomic diﬀerential overlap\\nNEB nudged elastic band\\nNMR nuclear magnetic resonanceNMR nuclear magnetic resonance\\nNN neural network\\nNQE nuclear quantum eﬀect\\nOF orbital-free\\nOLED organic light-emitting diode\\nPCA principal component analysis\\nPCM polarizable continuum solvent model\\nPES potential energy surfacePES potential energy surface\\nPIMD path integral molecular dynamics\\nQM quantum mechanics\\nQSAR/QSPR quantitative structure activity/property rela-\\ntionship\\nRE-Match regularized entropy match\\nRI resolution of the identityRI resolution of the identity\\nRISM reference interaction site model\\nRL reinforcement learning\\nRMSD root mean squared displacement\\nRNN recurrent neural network\\nSCRF self-consistent reactionﬁeld\\nSOAP smooth overlap of atomic positionsSOAP smooth overlap of atomic positions\\nSTM scanning tunneling microscopy\\nSVM support vector machine\\nt-SNE t-distributed stochastic neighbor embedding\\nTD time-dependent\\nUMAP uniform manifold approximation and projec-\\ntiontion\\nXAI explainable arti ﬁcial intelligence\\nXANES X-ray absorption near edge structure\\nREFERENCES\\n(1) LeCun, Y.; Bengio, Y.; Hinton, G. Deep Learning.Nature 2015,\\n521, 436−444.\\n(2) Schmidhuber, J. Deep Learning in Neural Networks: AnOverview. Neural Netw. 2015, 61,8 5−117.\\n(3) Goodfellow, I.; Bengio, Y.; Courville, A.Deep Learning; MIT\\nPress: Cambridge, MA, 2016;http://www.deeplearningbook.org.\\n(4) Capper, D.; Jones, D. T.; Sill, M.; Hovestadt, V.; Schrimpf, D.;Sturm, D.; Koelsche, C.; Sahm, F.; Chavez, L.; Reuss, D. E.; et al.\\nDNA Methylation-Based Classification of Central Nervous System\\nTumours. Nature 2018, 555, 469−474.\\n(5) Klauschen, F.; Müller, K.-R.; Binder, A.; Bockmayr, M.; Hägele,M.; Seegerer, P.; Wienert, S.; Pruneri, G.; de Maria, S.; Badve, S.; et al.\\nScoring of Tumor-Infiltrating Lymphocytes: From Visual Estimation\\nto Machine Learning.Semin. Cancer Biol.2018, 52, 151−157.(6) Jurmeister, P.; Bockmayr, M.; Seegerer, P.; Bockmayr, T.; Treue,\\nD.; Montavon, G.; Vollbrecht, C.; Arnold, A.; Teichmann, D.;\\nBressem, K.; et al. Machine Learning Analysis of DNA MethylationProfiles Distinguishes Primary Lung Squamous Cell Carcinomas\\nFrom Head and Neck Metastases.Sci. Transl. Med. 2019, 11,\\nNo. eaaw8513.\\n(7) Ardila, D.; Kiraly, A. P.; Bharadwaj, S.; Choi, B.; Reicher, J. J.;Peng, L.; Tse, D.; Etemadi, M.; Ye, W.; Corrado, G.; et al. End-to-End\\nLung Cancer Screening With Three-Dimensional Deep Learning on\\nLow-Dose Chest Computed Tomography.Nat. Med.2019, 25, 954−\\n961.961.\\n(8) Binder, A.; Bockmayr, M.; Hagele, M.; Wienert, S.; Heim, D.;\\nHellweg, K.; Ishii, M.; Stenzinger, A.; Hocke, A.; Denkert, C.; et al.\\nMorphological and molecular bre ast cancer profiling throughexplainable machine learning.Nat. Mach. Intel.2021, 3, 355−366.\\n(9) Baldi, P.; Sadowski, P.; Whiteson, D. Searching for Exotic\\nParticles in High-Energy Physics With Deep Learning.Nat. Commun.\\n2014, 5, 4308.2014, 5, 4308.\\n(10) Leinen, P.; Esders, M.; Schütt, K. T.; Wagner, C.; Müller, K.-R.;\\nTautz, F. S. Autonomous Robotic Nanofabrication With Reinforce-\\nment Learning. Sci. Adv. 2020, 6, No. eabb6987.ment Learning. Sci. Adv. 2020, 6, No. eabb6987.\\n(11) Lengauer, T.; Sander, O.; Sierra, S.; Thielen, A.; Kaiser, R.\\nBioinformatics Prediction of HIV Coreceptor Usage.Nat. Biotechnol.\\n2007, 25, 1407−1410.2007, 25, 1407−1410.\\n(12) Senior, A. W.; Evans, R.; Jumper, J.; Kirkpatrick, J.; Sifre, L.;\\nGreen, T.; Qin, C.; Žídek, A.; Nelson, A. W.; Bridgland, A.; et al.\\nImproved Protein Structure Prediction Using Potentials From DeepLearning. Nature 2020, 577, 706−710.\\n(13) Blankertz, B.; Tomioka, R.; Lemm, S.; Kawanabe, M.; Muller,Improved Protein Structure Prediction Using Potentials From Deep\\nLearning. Nature 2020, 577, 706−710.\\n(13) Blankertz, B.; Tomioka, R.; Lemm, S.; Kawanabe, M.; Muller,\\nK.-R. Optimizing Spatial Filters for Robust EEG Single-Trial Analysis.IEEE Signal Process. Mag.2008, 25,4 1−56.\\n(14) Perozzi, B.; Al-Rfou, R.; Skiena, S. DeepWalk: Online Learning\\nof Social Representations. Proceedings of the 20th ACM SIGKDD\\nInternational Conference on Knowledge Discovery and Data Mining; NewYork, NY, USA, 2014; pp 701−710.\\n(15) Thrun, S.; Burgard, W.; Fox, D. Probabilistic Robotics; MIT\\nPress: Cambridge, MA, 2005.\\n(16) Won, D.-O.; Müller, K.-R.; Lee, S.-W. An Adaptive Deep\\nReinforcement Learning Framework Enables Curling Robots WithHuman-Like Performance in Real World Conditions.Sci. Robot.2020,\\n5, No. eabb9764.\\n(17) Lewis, M. M.Moneyball: The Art of Winning an Unfair Game;\\nW. W. Norton: New York, N.Y., 2003.\\n(18) Ferrucci, D.; Levas, A.; Bagchi, S.; Gondek, D.; Mueller, E. T.Watson: Beyond Jeopardy!Artif. Intell. 2013, 199,9 3−105.\\n(19) Silver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.; Van\\nDen Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershelvam,V.; Lanctot, M.; et al. Mastering the Game of Go With Deep Neural\\nNetworks and Tree Search.Nature 2016, 529, 484−489.\\n(20) Tkatchenko, A. Machine Learning for Chemical Discovery.Nat.\\nCommun. 2020, 11, 4125.Commun. 2020, 11, 4125.\\n(21) Rowley, J. The Wisdom Hierarchy: Representations of the\\nDIKW Hierarchy. J. Inf. Sci.2007, 33, 163−180.\\n(22) Box, G. E. Science and Statistics.J. Am. Stat. Assoc.1976, 71,\\n791−799.791−799.\\n(23) McQuarrie, D.; Simon, J. Physical Chemistry: A Molecular\\nApproach; University Science Books: Sausalito, CA, 1997.\\n(24) Cramer, C. J. Essentials of Computational Chemistry: Theoriesand Models, 2nd ed.; John Wiley & Sons, Inc.: Hoboken, NJ, 2004.\\n(25) Frenkel, D.; Smit, B.Understanding Molecular Simulation: From\\nAlgorithms to Applications; Academic Press: New York, NY, 2002.(26) Foresman, J.; Frisch, A.; Gaussian, I.Exploring Chemistry With\\nElectronic Structure Methods; Gaussian, Inc.: Pittsburgh, PA, 1996.\\n(27) Eastman, P.; Swails, J.; Chodera, J. D.; McGibbon, R. T.; Zhao,Y.; Beauchamp, K. A.; Wang, L.-P.; Simmonett, A. C.; Harrigan, M.\\nP.; Stern, C. D.; et al. OpenMM 7: Rapid Development of High\\nPerformance Algorithms for Molecular Dynamics.PLoS Comput. Biol.\\n2017, 13, No. e1005659.2017, 13, No. e1005659.\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9855(28) Oyeyemi, V. B.; Keith, J. A.; Carter, E. A. Accurate Bond\\nEnergies of Biodiesel Methyl Esters From Multireference Averaged\\nCoupled-Pair Functional Calculations.J. Phys. Chem. A 2014, 118,\\n7392−7403.7392−7403.\\n(29) Anslyn, E.; Dougherty, D.Modern Physical Organic Chemistry;\\nUniversity Science Books: Sausalito, CA, 2006.\\n(30) Glazer, A. The Classification of Tilted Octahedra inPerovskites. Acta Crystallogr., Sect. B: Struct. Crystallogr. Cryst. Chem.\\n1972, 28, 3384−3392.\\n(31) Giessibl, F. J. Atomic Resolution of the Silicon (111)-(7× 7)\\nSurface by Atomic Force Microscopy.Science 1995, 267,6 8−71.(32) Curtiss, L. A.; Raghavachari, K.; Redfern, P. C.; Pople, J. A.\\nAssessment of Gaussian-3 and Density Functional Theories for a\\nLarger Experimental Test Set.J. Chem. Phys.2000, 112, 7374.(33) Haunschild, R.; Klopper, W. New Accurate Reference Energies\\nfor the G2/97 Test Set.J. Chem. Phys.2012, 136, 164102.\\n(34) Taylor, P. R.European Summer School in Quantum Chemistry;\\nSpringer, Berlin, 1994; Vol.125; pp 125−202.Springer, Berlin, 1994; Vol.125; pp 125−202.\\n(35) Bartlett, R. J. Many-Body Perturbation Theory and Coupled\\nCluster Theory for Electron Correlation in Molecules. Annu. Rev.\\nPhys. Chem. 1981, 32, 359−401.Phys. Chem. 1981, 32, 359−401.\\n(36) Stöhr, M.; Van Voorhis, T.; Tkatchenko, A. Theory and\\nPractice of Modeling Van Der Waals Interactions in Electronic-\\nStructure Calculations.Chem. Soc. Rev.2019, 48, 4118−4154.(37) Lundberg, M.; Siegbahn, P. E. Quantifying the Effects of the\\nSelf-Interaction Error in DFT: When Do the Delocalized States\\nAppear?J. Chem. Phys.2005, 122, 224103.\\n(38) Morgante, P.; Peverati, R. The Devil in the Details: A TutorialReview on Some Undervalued Aspects of Density Functional Theory\\nCalculations. Int. J. Quantum Chem.2020, 120, No. 26332.\\n(39) Becke, A. D. Density-Functional Thermochemistry. III. The\\nRole of Exact Exchange.J. Chem. Phys.1993, 98, 5648.(40) Perdew, J. P.; Burke, K.; Ernzerhof, M. Generalized Gradient\\nApproximation Made Simple.Phys. Rev. Lett.1996, 77, 3865.\\n(41) Goerigk, L.; Hansen, A.; Bauer, C.; Ehrlich, S.; Najibi, A.;Grimme, S. A Look at the Density Functional Theory Zoo With the\\nAdvanced GMTKN55 Database for General Main Group Thermo-\\nchemistry, Kinetics and Noncovalent Interactions.Phys. Chem. Chem.\\nPhys. 2017, 19, 32184−32215.Phys. 2017, 19, 32184−32215.\\n(42) Zhao, Y.; González-García, N.; Truhlar, D. G. Benchmark\\nDatabase of Barrier Heights for Heavy Atom Transfer, Nucleophilic\\nSubstitution, Association, and Unimolecular Reactions and Its Use toTest Theoretical Methods.J. Phys. Chem. A2005, 109, 2012−2018.\\n(43) Sim, E.; Song, S.; Burke, K. Quantifying Density Errors in DFT.\\nJ. Phys. Chem. Lett.2018, 9, 6385−6392.\\n(44) Riley, K. E.; Op’t Holt, B. T.; Merz, K. M. Critical Assessmentof the Performance of Density Functional Methods for Several Atomic\\nand Molecular Properties.J. Chem. Theory Comput.2007, 3, 407−433.\\n(45) Maldonado, A. M.; Hagiwara, S.; Choi, T. H.; Eckert, F.;Schwarz, K.; Sundararaman, R.; Otani, M.; Keith, J. A. Quantifying\\nUncertainties in Solvation Procedures for Modeling Aqueous Phase\\nReaction Mechanisms. J. Phys. Chem. A2021, 125, 154−164.(46) Abraham, M. J.; Apostolov, R. P.; Barnoud, J.; Bauer, P.; Blau,\\nC.; Bonvin, A. M.; Chavent, M.; Chodera, J. D.; Čondić-Jurkić, K.;\\nDelemotte, L.; et al. Sharing Data From Molecular Simulations.J.\\nChem. Inf. Model.2019, 59, 4093−4099.Chem. Inf. Model.2019, 59, 4093−4099.\\n(47) Wheeler, S. E.; Houk, K. N. Integration Grid Errors for Meta-\\nGga-Predicted Reaction Energies: Origin of Grid Errors for the M06\\nSuite of Functionals.J. Chem. Theory Comput.2010, 6, 395−404.(48) Bonomi, M.; Bussi, G.; Camilloni, C.; Tribello, G. A.; Banáš, P.;\\nBarducci, A.; Bernetti, M.; Bolhuis, P. G.; Bottaro, S.; Branduardi, D.;\\net al. Promoting Transparency and Reproducibility in EnhancedMolecular Simulations. Nat. Methods 2019, 16, 670−673.\\n(49) Lejaeghere, K.; Bihlmayer, G.; Björkman, T.; Blaha, P.; Blügel,\\nS.; Blum, V.; Caliste, D.; Castelli, I. E.; Clark, S. J.; Dal Corso, A.;et al. Reproducibility in Density Functional Theory Calculations ofS.; Blum, V.; Caliste, D.; Castelli, I. E.; Clark, S. J.; Dal Corso, A.;\\net al. Reproducibility in Density Functional Theory Calculations of\\nSolids. Science 2016, 351, No. aad3000.\\n(50) Sonnenburg, S.; Braun, M. L.; Ong, C. S.; Bengio, S.; Bottou,L.; Holmes, G.; LeCun, Y.; Müller, K.- R.; Pereira, F.; Rasmussen, C.\\nE.; et al. The Need for Open Source Software in Machine Learning.J.\\nMach. Learn. Res.2007, 8, 2443−2466.\\n(51) Durrani, J. Computational Chemistry Faces a Coding Crisis.Chemistry World , 2020. https://www.chemistryworld.com/news/\\nchemistrys--reproducibility--crisi s--that--youve--probably--never--\\nheard--of/4011693.article#/.\\n(52) Perkel, J. M. Challenge to Scientists: Does Your Ten-Year-OldCode Still Run?Nature 2020, 584, 656−658.\\n(53) Govoni, M.; Munakami, M.; Tanikanti, A.; Skone, J. H.;\\nRunesha, H. B.; Giberti, F.; de Pablo, J.; Galli, G. Qresp, a Tool for\\nCurating, Discovering and Exploring Reproducible Scientific Papers.Sci. Data2019, 6, 190002.\\n(54) Kitchin, J. R. Examples of Effective Data Sharing in Scientific\\nPublishing. ACS Catal. 2015, 5, 3894−3899.\\n(55) Álvarez-Moreno, M.; De Graaf, C.; López, N.; Maseras, F.;Poblet, J. M.; Bo, C. Managing the Computational Chemistry Big\\nData Problem: The ioChem-BD Platform.J. Chem. Inf. Model.2015,\\n55,9 5−103.\\n(56) Huber, S. P.; Zoupanos, S.; Uhrin, M.; Talirz, L.; Kahle, L.;Häuselmann, R.; Gresch, D.; Müller, T.; Yakutovich, A. V.; Andersen,\\nC. W.; et al. AiiDA 1.0, a Scalable Computational Infrastructure for\\nAutomated Reproducible Workflows and Data Provenance.Sci. Data\\n2020, 7, 300.2020, 7, 300.\\n(57) Heidrich, D.; Quapp, W. Saddle Points of Index 2 on Potential\\nEnergy Surfaces and Their Role in Theoretical Reactivity Inves-\\ntigations.Theor. Chim. Acta1986, 70,8 9−98.tigations.Theor. Chim. Acta1986, 70,8 9−98.\\n(58) Ess, D. H.; Wheeler, S. E.; Iafe, R. G.; Xu, L.; Çelebi-Ölçüm, N.;\\nHouk, K. N. Bifurcations on Potential Energy Surfaces of Organic\\nReactions.Angew. Chem., Int. Ed.2008, 47, 7592−7601.(59) Tarczay, G.; Császár, A. G.; Klopper, W.; Quiney, H. M.\\nAnatomy of Relativistic Energy Corrections in Light Molecular\\nSystems.Mol. Phys. 2001, 99, 1769−1794.\\n(60) Perdew, J. P.; Schmidt, K. Jacob’s Ladder of Density FunctionalApproximations for the Exchange-Correlation Energy.AIP Conference\\nProceedings. 2000,1 −20.\\n(61) Schrödinger, E. Quantisierung Als Eigenwertproblem (Erste\\nMitteilung). Ann. Phys. 1926, 384, 361−376.Mitteilung). Ann. Phys. 1926, 384, 361−376.\\n(62) Schrödinger, E. Quantisierung Als Eigenwertproblem (Zweite\\nMitteilung). Ann. Phys. 1926, 384, 489−527.\\n(63) Schrödinger, E. Quantisierung Als Eigenwertproblem (VierteMitteilung). Ann. Phys. 1926, 386, 109−139.\\n(64) Born, M.; Oppenheimer, R. Zur Quantentheorie Der Molekeln.\\nAnn. Phys. 1927, 389, 457−484.\\n(65) Curchod, B. F.; Martínez, T. J. Ab Initio NonadiabaticQuantum Molecular Dynamics.Chem. Rev. 2018, 118, 3305−3336.\\n(66) Pavos ̌ević, F.; Culpitt, T.; Hammes-Schiffer, S. Multi-\\ncomponent Quantum Chemistry: Integrating Electronic and NuclearQuantum Effects via the Nuclear-Electronic Orbital Method.Chem.\\nRev. 2020, 120, 4222−4253.\\n( 6 7 )P e t e r s o n ,K .A . ;D u n n i n g ,T .H .A c c u r a t eC o r r e l a t i o nConsistent Basis Sets for Molecular Core-Valence Correlation Effects:\\nThe Second Row Atoms Al-Ar, and the First Row Atoms B-Ne\\nRevisited.J. Chem. Phys.2002, 117, 10548.\\n(68) Hehre, W. J.; Stewart, R. F.; Pople, J. A. Self-ConsistentMolecular-Orbital Methods. I. Use of Gaussian Expansions of Slater-\\nType Atomic Orbitals.J. Chem. Phys.1969, 51, 2657.\\n(69) Schäfer, A.; Horn, H.; Ahlrichs, R. Fully Optimized ContractedGaussian Basis Sets for Atoms Li to Kr.J. Chem. Phys.1992, 97, 2571.\\n(70) Van Lenthe, E.; Baerends, E. J. Optimized Slater-Type Basis\\nSets for the Elements 1−118. J. Comput. Chem.2003, 24, 1142−1156.(71) Slater, J. C. Energy Band Calculations by the Augmented Plane\\nWave Method. Adv. Quantum Chem.1964, 1,3 5−58.\\n(72) MacDonald, A. H.; Picket, W. E.; Koelling, D. D. A Linearised\\nRelativistic Augmented-Plane-Wave Method Utilising ApproximateWave Method. Adv. Quantum Chem.1964, 1,3 5−58.\\n(72) MacDonald, A. H.; Picket, W. E.; Koelling, D. D. A Linearised\\nRelativistic Augmented-Plane-Wave Method Utilising Approximate\\nPure Spin Basis Functions.J. Phys. C: Solid State Phys.1980, 13, 2675.(73) Louie, S. G.; Ho, K. M.; Cohen, M. L. Self-Consistent Mixed-\\nBasis Approach to the Electronic Structure of Solids.Phys. Rev. B:\\nCondens. Matter Mater. Phys.1979, 19, 1774.\\nChemical Reviews pubs.acs.org/CR ReviewChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9856(74) Goedecker, S.; Teter, M.; Hutter, J. Separable Dual-Space\\nGaussian Pseudopotentials.Phys. Rev. B: Condens. Matter Mater. Phys.\\n1996, 54, 1703.\\n(75) Melius, C. F.; Goddard, W. A. Ab Initio Effective Potentials forUse in Molecular Quantum Mechanics.Phys. Rev. A: At., Mol., Opt.\\nPhys. 1974, 10, 1528.\\n(76) Wadt, W. R.; Hay, P. J. Ab Initio Effective Core Potentials for\\nMolecular Calculations. Potentials for Main Group Elements Na to Bi.J. Chem. Phys.1985, 82, 284.\\n(77) Hay, P. J.; Wadt, W. R. Ab Initio Effective Core Potentials for\\nMolecular Calculations. Potentials for the Transition Metal Atoms Sc\\nto Hg.J. Chem. Phys.1985, 82, 270.to Hg.J. Chem. Phys.1985, 82, 270.\\n(78) Cao, X.; Dolg, M. Segmented Contraction Scheme for Small-\\nCore Lanthanide Pseudopotential Basis Sets. J. Mol. Struct.:\\nTHEOCHEM 2002, 581, 139−147.THEOCHEM 2002, 581, 139−147.\\n(79) Metz, B.; Stoll, H.; Dolg, M. Small-Core Multiconfiguration-\\nDirac-Hartree-Fock-Adjusted P seudopotentials for Post-D Main\\nGroup Elements: Application to PbH and PbO. J. Chem. Phys.\\n2000, 113, 2563.2000, 113, 2563.\\n(80) Dolg, M. InHandbook of Relativistic Quantum Chemistry; Liu,\\nW., Ed.; Springer: Berlin, 2016; pp 449−478.\\n(81) Shaw, R. W.; Harrison, W. A. Reformulation of the Screened\\nHeine-Abarenkov Model Potential.Phys. Rev. 1967, 163, 604.(82) Kahn, L. R.; Baybutt, P.; Truhlar, D. G. Ab Initio Effective Core\\nPotentials: Reduction of All-Electron Molecular Structure Calcu-\\nlations to Calculations Involving Only Valence Electrons. J. Chem.\\nPhys. 1976, 65, 3826.Phys. 1976, 65, 3826.\\n(83) Christiansen, P. A.; Lee, Y. S.; Pitzer, K. S. Improved Ab Initio\\nEffective Core Potentials for Molecular Calculations.J. Chem. Phys.\\n1979, 71, 4445.\\n(84) Hamann, D. R.; Schlüter, M.; Chiang, C. Norm-ConservingPseudopotentials. Phys. Rev. Lett.1979, 43, 1494.\\n(85) Vanderbilt, D. Optimally Smooth Norm-Conserving Pseudo-\\npotentials. Phys. Rev. B: Condens. Matter Mater. Phys.1985, 32, 8412.\\n(86) Garrity, K. F.; Bennett, J. W.; Rabe, K. M.; Vanderbilt, D.Pseudopotentials for High-Throughput DFT Calculations. Comput.\\nMater. Sci. 2014, 81, 446−452.\\n(87) Kresse, G.; Hafner, J. Norm-Conserving and Ultrasoft\\nPseudopotentials for First-Row and Transition Elements. J. Phys.:\\nCondens. Matter 1994, 6, 8245.Condens. Matter 1994, 6, 8245.\\n(88) Kresse, G.; Joubert, D. From Ultrasoft Pseudopotentials to the\\nProjector Augmented-Wave Method. Phys. Rev. B: Condens. Matter\\nMater. Phys. 1999, 59, 1758.Mater. Phys. 1999, 59, 1758.\\n(89) Troullier, N.; Martins, J. A Straightforward Method for\\nGenerating Soft Transferable Pseudopotentials.Solid State Commun.\\n1990, 74, 613−616.\\n(90) Peterson, K. A.; Figgen, D.; Goll, E.; Stoll, H.; Dolg, M.Systematically Convergent Basis Sets With Relativistic Pseudopoten-\\ntials. II. Small-Core Pseudopotentials and Correlation Consistent\\nBasis Sets for the Post-D Group 16−18 Elements. J. Chem. Phys.\\n2003, 119, 11113.2003, 119, 11113.\\n(91) Roy, L. E.; Hay, P. J.; Martin, R. L. Revised Basis Sets for the\\nLANL Effective Core Potentials. J. Chem. Theory Comput. 2008, 4,\\n1029−1031.\\n(92) Pyykkö, P. Relativistic Effects in Chemistry: More CommonThan You Thought.Annu. Rev. Phys. Chem.2012, 63,4 5−64.\\n(93) Dirac, P. A. M. The Quantum Theory of the Electron.Proc. R.\\nSoc. London A1928, 117, 610−624.\\n(94) Tecmer, P.; Boguslawski, K.; Kȩdziera, D. In Handbook ofComputational Chemistry; Leszczynski, J., Ed.; Springer: Dordrecht,\\n2016; pp 1−43.\\n(95) Feynman, R. Quantum Electrodynamics ; CRC Press: Boca\\nRaton, FL, 2018.\\n(96) Nakajima, T.; Hirao, K. The Douglas-Kroll-Hess Approach.Chem. Rev. 2012, 112, 385−402.\\n(97) Van Lenthe, J. H.; Faas, S.; Snijders, J. G. Gradients in the Ab\\nInitio Scalar Zeroth-Order Regular Approximation (ZORA) Ap-\\nproach.Chem. Phys. Lett.2000, 328, 107−112.proach.Chem. Phys. Lett.2000, 328, 107−112.\\n(98) Visscher, L. The Dirac Equation in Quantum Chemistry:\\nStrategies to Overcome the Current Computational Problems. J.\\nComput. Chem. 2002, 23, 759−766.Comput. Chem. 2002, 23, 759−766.\\n(99) Hartree, D. R.; Hartree, W. Self-Consistent Field, With\\nExchange, for Beryllium.Proc. R. Soc. London A1935, 150,9 −33.\\n(100) Slater, J. C. A Simplification of the Hartree-Fock Method.\\nPhys. Rev. 1951, 81, 385.Exchange, for Beryllium.Proc. R. Soc. London A1935, 150,9 −33.\\n(100) Slater, J. C. A Simplification of the Hartree-Fock Method.\\nPhys. Rev. 1951, 81, 385.\\n(101) Fock, V. Näherungsmethode Zur Lösung Des Quantenme-chanischen Mehrkörperproblems. Eur. Phys. J. A1930, 61, 126−148.\\n(102) Jensen, F. Introduction to Computational Chemistry, 2nd ed.;\\nJohn Wiley & Sons, Inc.: Hoboken, NJ, 2007.\\n(103) Roothaan, C. C. New Developments in Molecular OrbitalTheory. Rev. Mod. Phys.1951, 23,6 9−89.\\n(104) Hall, G. G. The Molecular Orbital Theory of Chemical\\nValency VIII. A Method of Calculating Ionization Potentials.Proc. R.\\nSoc. London A1951, 205, 541−552.Soc. London A1951, 205, 541−552.\\n(105) Hermann, J.; Schätzle, Z.; Noé, F. Deep-Neural-Network\\nSolution of the Electronic Schrödinger Equation. Nat. Chem. 2020,\\n12, 891−897.\\n(106) Pfau, D.; Spencer, J. S.; Matthews, A. G.; Foulkes, W. M. C.Ab Initio Solution of the Many-Electron Schrödinger Equation With\\nDeep Neural Networks.Phys. Rev. Res.2020, 2, 033429.\\n(107) Eriksen, J. J.; Anderson, T. A.; Deustua, J. E.; Ghanem, K.;Hait, D.; Hoffmann, M. R.; Lee, S.; Levine, D. S.; Magoulas, I.; Shen,\\nJ.; et al. The Ground State Electronic Energy of Benzene.J. Phys.\\nChem. Lett. 2020, 11, 8922−8929.\\n(108) Helgaker, T.; Jorgensen, P.; Olsen, J. Molecular Electronic-Structure Theory; John Wiley & Sons, Inc.: Hoboken, NJ, 2013.\\n(109) Bartlett, R. J.; Musia ł, M. Coupled-Cluster Theory in\\nQuantum Chemistry. Rev. Mod. Phys.2007, 79, 291−352.\\n(110) Řezáč, J.; Hobza, P. Describing Noncovalent InteractionsBeyond the Common Approximations: How Accurate Is the”Gold\\nStandard,” CCSD(T) at the Complete Basis Set Limit? J. Chem.\\nTheory Comput. 2013, 9, 2151−2155.\\n(111) Moran, D.; Simmonett, A. C.; Leach, F. E.; Allen, W. D.;Schleyer, P. V.; Schaefer, H. F. Popular Theoretical Methods Predict\\nBenzene and Arenes to Be Nonplanar.J. Am. Chem. Soc.2006, 128,\\n9342−9343.\\n(112) Samala, N. R.; Jordan, K. D. Comment on a SpuriousPrediction of a Non-Planar Geometry for Benzene at the MP2 Level\\nof Theory. Chem. Phys. Lett.2017, 669, 230−232.\\n(113) Titov, A. V.; Ufimtsev, I. S.; Luehr, N.; Martinez, T. J.\\nGenerating Efficient Quantum Chemistry Codes for NovelArchitectures.J. Chem. Theory Comput.2013, 9, 213−221.\\n(114) Seritan, S.; Bannwarth, C.; Fales, B. S.; Hohenstein, E. G.;\\nIsborn, C. M.; Kokkila-Schumacher, S. I.; Li, X.; Liu, F.; Luehr, N.;Snyder, J. W.; et al. TeraChem: A Graphical Processing Unit-\\nAccelerated Electronic Structure Package for Large-Scale Ab Initio\\nMolecular Dynamics.Wiley Interdiscip. Rev. Comput. Mol. Sci.2020,\\nNo. e1494.No. e1494.\\n(115) Anderson, A. G.; Goddard, W. A.; Schröder, P. Quantum\\nMonte Carlo on Graphical Processing Units.Comput. Phys. Commun.\\n2007, 177, 298−306.\\n(116) Andrade, X.; Aspuru-Guzik, A. Real-Space Density FunctionalTheory on Graphical Processing Units: Computational Approach and\\nComparison to Gaussian Basis Set Methods.J. Chem. Theory Comput.\\n2013, 9, 4360−4373.\\n(117) Friesner, R. A. Solution of the Hartree-Fock Equations by aPseudospectral Method: Application to Diatomic Molecules.J. Chem.\\nPhys. 1986, 85, 1462.\\n(118) Martinez, T. J.; Mehta, A.; Carter, E. A. Pseudospectral Full\\nConfiguration Interaction. J. Chem. Phys.1992, 97, 1876.(119) Friesner, R. A.; Murphy, R. B.; Ringnalda, M. N. In\\nEncyclopedia of Computational Chemistry; Schleyer, P. v. R., Ed.; 2002.\\n(120) Sierka, M.; Hogekamp, A.; Ahlrichs, R. Fast Evaluation of theCoulomb Potential for Electron Densities Using Multipole Accel-\\nerated Resolution of Identity Approximation. J. Chem. Phys. 2003,\\n118, 9136−9148.\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107https://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9857(121) Riplinger, C.; Neese, F. An Efficient and Near Linear Scaling\\nPair Natural Orbital Based Local Coupled Cluster Method.J. Chem.\\nPhys. 2013, 138, 034106.\\n(122) Kong, L.; Bischoff, F. A.; Valeev, E. F. Explicitly CorrelatedR12/F12 Methods for Electronic Structure. Chem. Rev. 2012, 112,\\n75−107.\\n(123) Austin, B. M.; Zubarev, D. Y.; Lester Jr, W. A. Quantum\\nMonte Carlo and Related Approaches.Chem. Rev. 2012, 112, 263−\\n288.288.\\n(124) Marti, K. H.; Reiher, M. The Density Matrix Renormalization\\nGroup Algorithm in Quantum Chemistry.Z. Phys. Chem.2010, 224,\\n583−599.\\n(125) Schütt, K.; Gastegger, M.; Tkatchenko, A.; Müller, K.-R.;Maurer, R. Unifying Machine Learning and Quantum Chemistry With\\na Deep Neural Network for Molecular Wavefunctions.Nat. Commun.\\n2019, 10, 5024.\\n(126) McGibbon, R. T.; Taube, A. G.; Donchev, A. G.; Siva, K.;Hernández, F.; Hargus, C.; Law, K. H.; Klepeis, J. L.; Shaw, D. E.\\nImproving the accuracy of Møller-Plesset perturbation theory with\\nneural networks.J. Chem. Phys.2017, 147, 161725.\\n(127) Townsend, J.; Vogiatzis, K. D. Transferable MP2-BasedMachine Learning for Accurate Coupled-Cluster Energies.J. Chem.\\nTheory Comput. 2020, 16, 7453−7461.\\n(128) Coe, J. P. Machine Learning Configuration Interaction. J.\\nChem. Theory Comput.2018, 14, 5739−5749.Chem. Theory Comput.2018, 14, 5739−5749.\\n(129) Jeong, W. S.; Stoneburner, S. J.; King, D.; Li, R.; Walker, A.;\\nLindh, R.; Gagliardi, L. Automation of Active Space Selection for\\nMultireference Methods via Machine Learning on Chemical BondDissociation.J. Chem. Theory Comput.2020, 16, 2389−2399.\\n(130) Montgomery, J. A.; Frisch, M. J.; Ochterski, J. W.; Petersson,\\nG. A. A Complete Basis Set Model Chemistry. VII. Use of the\\nMinimum Population Localization Method.J. Chem. Phys.2000, 112,6532−6542.\\n(131) Curtiss, L. A.; Raghavachari, K.; Redfern, P. C.; Rassolov, V.;\\nPople, J. A. Gaussian-3 (G3) Theory for Molecules Containing First\\nand Second-Row Atoms.J. Chem. Phys.1998, 109, 7764−7776.(132) Karton, A.; Rabinovich, E.; Martin, J. M.; Ruscic, B. W4\\nTheory for Computational Thermochemistry: In Pursuit of Confident\\nSub-kJ/Mol Predictions. J. Chem. Phys.2006, 125, 144108.(133) Tajti, A.; Szalay, P. G.; Császár, A. G.; Kállay, M.; Gauss, J.;\\nValeev, E. F.; Flowers, B. A.; Vázquez, J.; Stanton, J. F. HEAT: High\\nAccuracy Extrapolated Ab Initio Thermochemistry.J. Chem. Phys.\\n2004, 121, 11599.2004, 121, 11599.\\n(134) Karton, A. A Computational Chemist’s Guide to Accurate\\nThermochemistry for Organic Molecules. Wiley Interdiscip. Rev.\\nComput. Mol. Sci.2016, 6, 292−310.\\n(135) Zaspel, P.; Huang, B.; Harbrecht, H.; von Lilienfeld, O. A.Boosting Quantum Machine Learning Models With a Multilevel\\nCombination Technique: Pople Diagrams Revisited.J. Chem. Theory\\nComput. 2019, 15, 1546−1559.\\n(136) Park, J. W.; Al-Saadon, R.; Macleod, M. K.; Shiozaki, T.;Vlaisavljevich, B. Multireference Electron Correlation Methods:\\nJourneys Along Potential Energy Surfaces. Chem. Rev. 2020, 120,\\n5878−5909.\\n(137) Hirao, K. Multireference Møller-Plesset Method.Chem. Phys.\\nLett. 1992, 190, 374−380.Lett. 1992, 190, 374−380.\\n(138) Buenker, R. J.; Peyerimhoff, S. D.; Butscher, W. Applicability\\nof the Multi-Reference Double-Excitation CI (MRD-CI) Method to\\nthe Calculation of Electronic Wavefunctions and Comparison WithRelated Techniques.Mol. Phys. 1978, 35, 771−791.\\n(139) Levine, B. G.; Coe, J. D.; Martínez, T. J. Optimizing Conical\\nIntersections Without Derivative Coupling Vectors: Application to\\nMultistate Multireference Second-Order Perturbation Theory (MS-CASPT2).J. Phys. Chem. B2008, 112, 405−413.\\n(140) Jiang, W.; Deyonker, N. J.; Wilson, A. K. Multireference\\nCharacter for 3d Transition-Metal-Containing Molecules. J. Chem.\\nTheory Comput. 2012, 8, 460−468.Theory Comput. 2012, 8, 460−468.\\n(141) Lee, T. J. Comparison of the T1 and D1 Diagnostics for\\nElectronic Structure Theory: A New Definition for the Open-Shell\\nD11 Diagnostic. Chem. Phys. Lett.2003, 372, 362−367.(142) Duan, C.; Liu, F.; Nandy, A.; Kulik, H. J. Data-Driven\\nApproaches Can Overcome the Cost-Accuracy Trade-Off in Multi-D11 Diagnostic. Chem. Phys. Lett.2003, 372, 362−367.\\n(142) Duan, C.; Liu, F.; Nandy, A.; Kulik, H. J. Data-Driven\\nApproaches Can Overcome the Cost-Accuracy Trade-Off in Multi-\\nreference Diagnostics.J. Chem. Theory Comput.2020, 16, 4373−4387.(143) Bobrowicz, F. W.; Goddard, W. A. InMethods of Electronic\\nStructure Theory; Schaefer, H. F., Ed.; Springer: Boston, MA, 1977; pp\\n79−127.\\n(144) Roos, B. O.; Taylor, P. R.; Sigbahn, P. E. A Complete ActiveSpace SCF Method (CASSCF) Using a Density Matrix Formulated\\nSuper-Ci Approach.Chem. Phys. 1980, 48, 157−173.\\n(145) Szalay, P. G.; Müller, T.; Gidofalvi, G.; Lischka, H.; Shepard,\\nR. Multiconfiguration Self-Consistent Field and MultireferenceConfiguration Interaction Methods and Applications. Chem. Rev.\\n2012, 112, 108−181.\\n(146) Pulay, P. A Perspective on the CASPT2Method. Int. J.\\nQuantum Chem. 2011, 111, 3273−3279.\\n(147) Lyakh, D. I.; Musia ł, M.; Lotrich, V. F.; Bartlett, R. J.Multireference Nature of Chemistry: The Coupled-Cluster View.\\nChem. Rev. 2012, 112, 182−243.\\n(148) Evangelista, F. A. Perspective: Multireference Coupled Cluster\\nTheories of Dynamical Electron Correlation.J. Chem. Phys.2018, 149,\\n030901.030901.\\n(149) Jensen, K. P.; Roos, B. O.; Ryde, U. Erratum: O2-Binding to\\nHeme: Electronic Structure and Spectrum of Oxyheme, Studied by\\nMulticonfigurational Methods.J. Inorg. Biochem.2005, 99 (1), 45−54;\\nJ. Inorg. Biochem.2005, 99, 978.J. Inorg. Biochem.2005, 99, 978.\\n(150) Pople, J. A. Two-Dimensional Chart of Quantum Chemistry.\\nJ. Chem. Phys.1965, 43, S229.\\n(151) Parr, R.; Weitao, Y.Density-Functional Theory of Atoms andMolecules; International Series of Monographs on Chemistry; Oxford\\nUniversity Press: New York, NY, 1994.\\n(152) Witt, W. C.; Del Rio, B. G.; Dieterich, J. M.; Carter, E. A.\\nOrbital-Free Density Functional Theory for Materials Research. J.Mater. Res. 2018, 33, 777−795.\\n(153) Hung, L.; Huang, C.; Shin, I.; Ho, G. S.; Lignères, V. L.;\\nCarter, E. A. Introducing PROFESS 2.0: A Parallelized, Fully Linear\\nScaling Program for Orbital-Free Density Functional TheoryCalculations. Comput. Phys. Commun.2010, 181, 2208−2209.\\n(154) Mi, W.; Shao, X.; Su, C.; Zhou, Y.; Zhang, S.; Li, Q.; Wang,\\nH.; Zhang, L.; Miao, M.; Wang, Y.; et al. ATLAS: A Real-Space Finite-Difference Implementation of Or bital-Free Density Functional\\nTheory. Comput. Phys. Commun.2016, 200,8 7−95.\\n(155) Mi, W.; Genova, A.; Pavanello, M. Nonlocal Kinetic Energy\\nFunctionals by Functional Integration. J. Chem. Phys. 2018, 148,\\n184107.184107.\\n(156) Ayers, P. W. Generalized Density Functional Theories Using\\nthe K -Electron Densities: Development of Kinetic Energy Func-\\ntionals.J. Math. Phys.2005, 46, 062107.\\n(157) Huang, C.; Carter, E. A. Nonlocal Orbital-Free Kinetic EnergyDensity Functional for Semiconductors.Phys. Rev. B: Condens. Matter\\nMater. Phys. 2010, 81, 045206.\\n(158) Burakovsky, L.; Ticknor, C.; Kress, J. D.; Collins, L. A.;\\nLambert, F. Transport Properties of Lithium Hydride at ExtremeConditions From Orbital-Free Molecular Dynamics. Phys. Rev. E\\n2013, 87, 023104.\\n(159) Sjostrom, T.; Daligault, J. Ionic and Electronic Transport\\nProperties in Dense Plasmas by Orbital-Free Density Functional\\nTheory. Phys. Rev. E2015, 92, 063304.Theory. Phys. Rev. E2015, 92, 063304.\\n(160) Kang, D.; Luo, K.; Runge, K.; Trickey, S. B. Two-Temperature\\nWarm Dense Hydrogen as a Test of Quantum Protons Driven by\\nOrbital-Free Density Functional Theory Electronic Forces. MatterRadiat. Extremes 2020, 5, 064403.\\n(161) Snyder, J. C.; Rupp, M.; Hansen, K.; Blooston, L.; Müller, K.-\\nR.; Burke, K. Orbital-Free Bond Breaking via Machine Learning.J.\\nChem. Phys. 2013, 139, 224104.\\nChemical Reviews pubs.acs.org/CR ReviewChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9858(162) Brockherde, F.; Vogt, L.; Li, L.; Tuckerman, M. E.; Burke, K.;\\nMüller, K. R. Bypassing the Kohn-Sham Equations With Machine\\nLearning. Nat. Commun. 2017, 8, 872.\\n(163) Kohn, W.; Sham, L. J. Self-Consistent Equations IncludingExchange and Correlation Effects. Phys. Rev. 1965, 140, A1133−\\nA1138.\\n(164) Maurer, R. J.; Freysoldt, C.; Reilly, A. M.; Brandenburg, J. G.;\\nHofmann, O. T.; Björkman, T.; Lebègue, S.; Tkatchenko, A. Advancesin Density-Functional Calculations for Materials Modeling.Annu. Rev.\\nMater. Res. 2019, 49,1 −30.\\n(165) Jacobsen, H.; Cavallo, L. In Handbook of Computational\\nChemistry; Leszczynski, J., Ed.; Springer: Dordrecht, 2012; pp 95−\\n133.133.\\n(166) Learn Density Functional Theory . https://dft.uci.edu/\\nlearnDFT.php (accessed 2020-11-30).\\n(167) Tran, F.; Stelzl, J.; Blaha, P. Rungs 1 to 4 of DFT Jacob’s\\nLadder: Extensive Test on the Lattice Constant, Bulk Modulus, andCohesive Energy of Solids.J. Chem. Phys.2016, 144, 204120.\\n(168) Kozuch, S.; Martin, J. M. Spin-Component-Scaled Double\\nHybrids: An Extensive Search for the Best Fifth-Rung Functionals\\nBlending DFT and Perturbation Theory.J. Comput. Chem.2013, 34,2327−2344.\\n(169) Janesko, B. G. Reducing Density-Driven Error Without Exact\\nExchange. Phys. Chem. Chem. Phys.2017, 19, 4793−4801.\\n(170) Gerber, I. C.; Ángyán, J. G.; Marsman, M.; Kresse, G. RangeSeparated Hybrid Density Functional With Long-Range Hartree-Fock\\nExchange Applied to Solids.J. Chem. Phys.2007, 127, 054101.\\n(171) Pisani, C.; Dovesi, R.; Roetti, C. Hartree-Fock Ab InitioTreatment of Crystalline Systems; Springer: Berlin Heidelberg, 2012;\\nVol. 48.\\n(172) Shishkin, M.; Sato, H. DFT+ U in Dudarev’s Formulation\\nWith Corrected Interactions Between the Electrons With OppositeSpins: The Form of Hamiltonian, Calculation of Forces, and Bandgap\\nAdjustments. J. Chem. Phys.2019, 151, 024102.\\n(173) Petukhov, A. G.; Mazin, I. I.; Chioncel, L.; Lichtenstein, A. I.\\nCorrelated Metals and the LDA + U Method.Phys. Rev. B: Condens.Matter Mater. Phys.2003, 67, 153106.\\n(174) Sun, Q.; Chan, G. K. L. Quantum Embedding Theories.Acc.\\nChem. Res. 2016, 49, 2705−2712.\\n(175) Cortona, P. Self-Consistently Determined Properties of SolidsWithout Band-Structure Calculations. Phys. Rev. B: Condens. Matter\\nMater. Phys. 1991, 44, 8454.\\n(176) Huang, P.; Carter, E. A. Advances in Correlated Electronic\\nStructure Methods for Solids, Surfaces, and Nanostructures. Annu.Rev. Phys. Chem.2008, 59, 261−290.\\n(177) Manby, F. R.; Stella, M.; Goodpaster, J. D.; Miller, T. F. A\\nSimple, Exact Density-Functional-Theory Embedding Scheme. J.\\nChem. Theory Comput.2012, 8, 2564−2568.Chem. Theory Comput.2012, 8, 2564−2568.\\n(178) Libisch, F.; Huang, C.; Carter, E. A. Embedded Correlated\\nWavefunction Schemes: Theory and Applications. Acc. Chem. Res.\\n2014, 47, 2768−2775.2014, 47, 2768−2775.\\n(179) Casida, M.; Huix-Rotllant, M. Progress in Time-Dependent\\nDensity-Functional Theory. Annu. Rev. Phys. Chem.2012, 63, 287−\\n323.\\n(180) Casida, M. E.; Casida, K. C.; Salahub, D. R. Excited-StatePotential Energy Curves From Time-Dependent Density-Functional\\nTheory: A Cross Section of Formaldehyde ’s 1A1Manifold. Int. J.\\nQuantum Chem. 1998, 70, 933−941.\\n(181) Snyder, J. C.; Rupp, M.; Hansen, K.; Müller, K. R.; Burke, K.Finding Density Functionals With Machine Learning.Phys. Rev. Lett.\\n2012, 108, 253002.\\n(182) Schmidt, J.; Benavides-Riveros, C. L.; Marques, M. A.\\nMachine Learning the Physical Nonlocal Exchange-CorrelationFunctional of Density-Functional Theory.J. Phys. Chem. Lett.2019,\\n10, 6425−6431.\\n(183) Meyer, R.; Weichselbaum, M.; Hauser, A. W. Machine\\nLearning Approaches Toward Orbital-Free Density FunctionalTheory: Simultaneous Training on the Kinetic Energy Density\\nFunctional and Its Functional Derivative. J. Chem. Theory Comput.\\n2020, 16, 5685−5694.\\n(184) Bogojeski, M.; Vogt-Maranto, L.; Tuckerman, M. E.; Müller,K.-R.; Burke, K. Quantum Che mical Accuracy From Density\\nFunctional Approximations via Machine Learning. Nat. Commun.\\n2020, 11, 5223.\\n(185) Nagai, R.; Akashi, R.; Sugino, O. Completing DensityK.-R.; Burke, K. Quantum Che mical Accuracy From Density\\nFunctional Approximations via Machine Learning. Nat. Commun.\\n2020, 11, 5223.\\n(185) Nagai, R.; Akashi, R.; Sugino, O. Completing DensityFunctional Theory by Machine Learning Hidden Messages From\\nMolecules. Npj Comput. Mater.2020, 6, 43.\\n(186) Thiel, W. Semiempirical Quantum-Chemical Methods.Wiley\\nInterdiscip. Rev.: Comput. Mol. Sci.2014, 4, 145−157.(187) Pople, J.; Beveridge, D.Approximate Molecular Orbital Theory;\\nMcGraw-Hill: United Kingdom, 1970.\\n(188) Dewar, M. J.; Zoebisch, E. G.; Healy, E. F.; Stewart, J. J.\\nDevelopment and Use of Quantum Mechanical Molecular Models.76. AM1: A New General Purpose Quantum Mechanical Molecular\\nModel. J. Am. Chem. Soc.1985, 107, 3902−3909.\\n(189) Stewart, J. J. Optimization of Parameters for Semiempirical\\nMethods VI: More Modifications to the NDDO Approximations andRe-Optimization of Parameters.J. Mol. Model.2013, 19,1 −32.\\n(190) Dewar, M. J.; Thiel, W. Ground States of Molecules. 38. The\\nMNDO Method. Approximations and Parameters.J. Am. Chem. Soc.\\n1977, 99, 4899−4907.1977, 99, 4899−4907.\\n(191) Dral, P. O.; Wu, X.; Spörkel, L.; Koslowski, A.; Thiel, W.\\nSemiempirical Quantum-Chemical Orthogonalization-Corrected\\nMethods: Benchmarks for Ground-State Properties.J. Chem. Theory\\nComput. 2016, 12, 1097−1120.Comput. 2016, 12, 1097−1120.\\n(192) Koskinen, P.; Mäkinen, V. Density-Functional Tight-Binding\\nfor Beginners. Comput. Mater. Sci.2009, 47, 237−253.\\n(193) Elstner, M.; Porezag, D.; Jungnickel, G.; Elsner, J.; Haugk, M.;Frauenheim, T.; et al. Self-Consistent-Charge Density-Functional\\nTight-Binding Method for Simulations of Complex Materials\\nProperties. Phys. Rev. B: Condens. Matter Mater. Phys.1998, 58, 7260.(194) Bannwarth, C.; Ehlert, S.; Grimme, S. GFN2-xTB - An\\nAccurate and Broadly Parametrized Self-Consistent Tight-Binding\\nQuantum Chemical Method With Multipole Electrostatics and\\nDensity-Dependent Dispersion Contributions.J. Chem. TheoryComput. 2019, 15, 1652−1671.\\n(195) Dral, P. O.; von Lilienfeld, O. A.; Thiel, W. Machine Learning\\nof Parameters for Accurate Semiempirical Quantum Chemical\\nCalculations. J. Chem. Theory Comput.2015, 11, 2120−2125.(196) Hegde, G.; Bowen, R. C. Machine-Learned Approximations to\\nDensity Functional Theory Hamiltonians.Sci. Rep. 2017, 7, 42669.\\n(197) Stöhr, M.; Medrano Sandonas, L.; Tkatchenko, A. AccurateMany-Body Repulsive Potentials for Density-Functional Tight\\nBinding From Deep Tensor Neural Networks.J. Phys. Chem. Lett.\\n2020, 11, 6835−6843.\\n(198) Li, H.; Collins, C.; Tanha, M.; Gordon, G. J.; Yaron, D. J. ADensity Functional Tight Binding Layer for Deep Learning of\\nChemical Hamiltonians. J. Chem. Theory Comput. 2018, 14, 5764−\\n5776.\\n(199) Poltavsky, I.; Zheng, L.; Mortazavi, M.; Tkatchenko, A.Quantum Tunneling of Thermal Protons Through Pristine Graphene.\\nJ. Chem. Phys.2018, 148, 204707.\\n(200) Ceriotti, M.; Fang, W.; Kusalik, P. G.; McKenzie, R. H.;\\nMichaelides, A.; Morales, M. A.; Markland, T. E. Nuclear QuantumEffects in Water and Aqueous Systems: Experiment, Theory, and\\nCurrent Challenges.Chem. Rev. 2016, 116, 7529−7550.\\n(201) Marx, D.; Parrinello, M. Ab Initio Path Integral Molecular\\nDynamics: Basic Ideas.J. Chem. Phys.1996, 104, 4077−4082.(202) Chandler, D.; Wolynes, P. G. Exploiting the Isomorphism\\nBetween Quantum Theory and Classical Statistical Mechanics of\\nPolyatomic Fluids. J. Chem. Phys.1981, 74, 4078−4095.\\n(203) Cao, J.; Voth, G. A. The Formulation of Quantum StatisticalMechanics Based on the Feynman Path Centroid Density. IV.\\nAlgorithms for Centroid Molecular Dynamics.J. Chem. Phys. 1994,\\n101, 6168−6183.\\n(204) Hele, T. J. H.; Willatt, M. J.; Muolo, A.; Althorpe, S. C.Communication: Relation of Centroid Molecular Dynamics and Ring-\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9859Polymer Molecular Dynamics to Exact Quantum Dynamics.J. Chem.\\nPhys. 2015, 142, 191101.\\n(205) Wang, L.; Ceriotti, M.; Markland, T. E. Quantum Fluctuations\\nand Isotope Effects in Ab Initio Descriptions of Water.J. Chem. Phys.\\n2014, 141, 104502.2014, 141, 104502.\\n(206) Sauceda, H. E.; Vassilev-Galindo, V.; Chmiela, S.; Müller, K.-\\nR.; Tkatchenko, A. Dynamical Strengthening of Covalent and Non-\\nCovalent Molecular Interactions by Nuclear Quantum Effects atFinite Temperature. Nat. Commun. 2021, 12, 442.\\n(207) Chmiela, S.; Sauceda, H. E.; Müller, K. R.; Tkatchenko, A.\\nTowards Exact Molecular Dynamics Simulations With Machine-\\nLearned Force Fields.Nat. Commun. 2018, 9, 3887.Learned Force Fields.Nat. Commun. 2018, 9, 3887.\\n(208) Wang, X.; Ramírez-Hinestrosa, S.; Dobnikar, J.; Frenkel, D.\\nThe Lennard-Jones Potential: When (Not) to Use It.Phys. Chem.\\nChem. Phys. 2020, 22, 10624−10633.Chem. Phys. 2020, 22, 10624−10633.\\n(209) Li, P.; Song, L. F.; Merz, K. M. Parameterization of Highly\\nCharged Metal Ions Using the 12−6-4 LJ-type Nonbonded Model in\\nExplicit Water. J. Phys. Chem. B2015, 119, 883−895.(210) Girifalco, L. A.; Weizer, V. G. Application of the Morse\\nPotential Function to Cubic Metals.Phys. Rev. 1959, 114, 687.\\n(211) Buckingham, R. A. The Classical Equation of State of GaseousHelium, Neon and Argon.Proc. R. Soc. London A1938, 168, 264−\\n283.\\n(212) Jorgensen, W. L.; Chandrasekhar, J.; Madura, J. D.; Impey, R.\\nW.; Klein, M. L. Comparison of Simple Potential Functions for\\nSimulating Liquid Water.J. Chem. Phys.1983, 79, 926.(213) Stillinger, F. H.; Weber, T. A. Computer Simulation of Local\\nOrder in Condensed Phases of Silicon.Phys. Rev. B: Condens. Matter\\nMater. Phys. 1985, 31, 5262.\\n(214) Weiner, P. K.; Kollman, P. A. AMBER: Assisted ModelBuilding With Energy Refinement. A General Program for Modeling\\nMolecules and Their Interactions.J. Comput. Chem. 1981, 2, 287−\\n303.\\n(215) Salomon-Ferrer, R.; Case, D. A.; Walker, R. C. An Overviewof the Amber Biomolecular Simulation Package.Wiley Interdiscip. Rev.\\nComput. Mol. Sci.2013, 3, 198−210.\\n(216) Wang, J.; Wolf, R. M.; Caldwell, J. W.; Kollman, P. A.; Case,\\nD. A. Development and Testing of a General Amber Force Field.J.Comput. Chem. 2004, 25, 1157−1174.\\n(217) Brooks, B. R.; Brooks III, C. L.; Mackerell Jr, A. D.; Nilsson,\\nL.; Petrella, R. J.; Roux, B.; Won, Y.; Archontis, G.; Bartels, C.;\\nBoresch, S.; et al. CHARMM: The Biomolecular Simulation Program.J. Comput. Chem.2009, 30, 1545−1614.\\n(218) Oostenbrink, C.; Villa, A.; Mark, A. E.; van Gunsteren, W. F.\\nA Biomolecular Force Field Based on the Free Enthalpy of Hydration\\nand Solvation: The GROMOS Force-Field Parameter Sets 53A5 and53A6. J. Comput. Chem.2004, 25, 1656−1676.\\n(219) Schmid, N.; Eichenberger, A. P.; Choutko, A.; Riniker, S.;\\nWinger, M.; Mark, A. E.; Van Gunsteren, W. F. Definition and\\nTesting of the GROMOS Force-Field Versions 54A7 and 54B7.Eur.Biophys. J. 2011, 40, 843.\\n(220) Daura, X.; Mark, A. E.; Van Gunsteren, W. F. Parametrization\\nof Aliphatic CHn United Atoms of GROMOS96 Force Field. J.\\nComput. Chem. 1998, 19, 535−547.Comput. Chem. 1998, 19, 535−547.\\n(221) Jorgensen, W. L.; Maxwell, D. S.; Tirado-Rives, J. Develop-\\nment and Testing of the OPLS All-Atom Force Field on Conforma-\\ntional Energetics and Properties of Organic Liquids.J. Am. Chem. Soc.1996, 118, 11225−11236.\\n(222) Jorgensen, W. L.; Madura, J. D.; Swenson, C. J. Optimized\\nIntermolecular Potential Functions for Liquid Hydrocarbons.J. Am.\\nChem. Soc. 1984, 106, 6638−6646.\\n(223) Mayo, S. L.; Olafson, B. D.; Goddard, W. A. DREIDING: AGeneric Force Field for Molecular Simulations.J. Phys. Chem.1990,\\n94, 8897−8909.\\n(224) Halgren, T. A. Merck Molecular Force Field. I. Basis, Form,\\nScope, Parameterization, and Performance of MMFF94.J. Comput.\\nChem. 1996, 17, 490−519.Chem. 1996, 17, 490−519.\\n(225) Rappé, A. K.; Casewit, C. J.; Colwell, K. S.; Goddard, W. A.;\\nSkiff, W. M. UFF, a Full Periodic Table Force Field for Molecular\\nMechanics and Molecular Dynamics Simulations.J. Am. Chem. Soc.\\n1992, 114, 10024−10035.1992, 114, 10024−10035.\\n(226) Sun, H. Compass: An Ab Initio Force-Field Optimized forMechanics and Molecular Dynamics Simulations.J. Am. Chem. Soc.\\n1992, 114, 10024−10035.\\n(226) Sun, H. Compass: An Ab Initio Force-Field Optimized for\\nCondensed-Phase Applications - Overview With Details on Alkaneand Benzene Compounds.J. Phys. Chem. B1998, 102, 7338−7364.\\n(227) Heinz, H.; Lin, T. J.; Kishore Mishra, R.; Emami, F. S.\\nThermodynamically Consistent Force Fields for the Assembly of\\nInorganic, Organic, and Biological Nanostructures: The INTERFACEForce Field. Langmuir 2013, 29, 1754−1765.\\n(228) Gale, J. D. Empirical Potential Derivation for Ionic Materials.\\nPhilos. Mag. B1996, 73,3 −19.\\n(229) Ponder, J. W.; Wu, C.; Ren, P.; Pande, V. S.; Chodera, J. D.;Schnieders, M. J.; Haque, I.; Mobley, D. L.; Lambrecht, D. S.;\\nDiStasio Jr, R. A.; et al. Current Status of the AMOEBA Polarizable\\nForce Field. J. Phys. Chem. B2010, 114, 2549−2564.\\n(230) Lemkul, J. A.; Huang, J.; Roux, B.; Mackerell, A. D. AnEmpirical Polarizable Force Field Based on the Classical Drude\\nOscillator Model: Development History and Recent Applications.\\nChem. Rev. 2016, 116, 4983−5013.\\n(231) Banks, J. L.; Kaminski, G. A.; Zhou, R.; Mainz, D. T.; Berne,B. J.; Friesner, R. A. Parametrizing a Polarizable Force Field From Ab\\nInitio Data. I. The Fluctuating Point Charge Model.J. Chem. Phys.\\n1999, 110, 741.\\n(232) Babin, V.; Leforestier, C.; Paesani, F. Development of a”FirstPrinciples” Water Potential With Flexible Monomers: Dimer Potential\\nEnergy Surface, VRT Spectrum, and Second Virial Coefficient. J.\\nChem. Theory Comput.2013, 9, 5395−5403.\\n(233) Kumar, R.; Wang, F. F.; Jenness, G. R.; Jordan, K. D. ASecond Generation Distributed Point Polarizable Water Model. J.\\nChem. Phys. 2010, 132, 014309.\\n(234) Xu, P.; Guidez, E. B.; Bertoni, C.; Gordon, M. S. Perspective:\\nAb Initio Force Field Methods Derived From Quantum Mechanics.J.Chem. Phys. 2018, 148, 090901.\\n(235) Daw, M. S.; Foiles, S. M.; Baskes, M. I. The Embedded-Atom\\nMethod: A Review of Theory and Applications.Mater. Sci. Rep.1993,\\n9, 251−310.\\n(236) Baskes, M. I. Modified Embedded-Atom Potentials for CubicMaterials and Impurities. Phys. Rev. B: Condens. Matter Mater. Phys.\\n1992, 46, 2727.\\n(237) Finnis, M. W.; Sinclair, J. E. A Simple Empirical N-Body\\nPotential for Transition Metals.Philos. Mag. A1984, 50,4 5−55.(238) Sutton, A. P.; Chen, J. Long-Range Finnis-Sinclair Potentials.\\nPhilos. Mag. Lett.1990, 61, 139−146.\\n(239) Brenner, D. W. Empirical Potential for Hydrocarbons for Use\\nin Simulating the Chemical Vapor Deposition of Diamond Films.Phys. Rev. B: Condens. Matter Mater. Phys.1990, 42, 9458.\\n(240) Tersoff, J. New Empirical Approach for the Structure and\\nEnergy of Covalent Systems. Phys. Rev. B: Condens. Matter Mater.\\nPhys. 1988, 37, 6991.Phys. 1988, 37, 6991.\\n(241) Tersoff, J. Modeling Solid-State Chemistry: Interatomic\\nPotentials for Multicomponent Systems.Phys. Rev. B: Condens. Matter\\nMater. Phys. 1989, 39, 5566−5568.Mater. Phys. 1989, 39, 5566−5568.\\n(242) Brenner, D. W.; Shenderova, O. A.; Harrison, J. A.; Stuart, S.\\nJ.; Ni, B.; Sinnott, S. B. A Second-Generation Reactive Empirical\\nBond Order (REBO) Potential Energy Expression for Hydrocarbons.J. Phys.: Condens. Matter2002, 14, 783.\\n(243) Liang, T.; Shan, T. R.; Cheng, Y. T.; Devine, B. D.;\\nNoordhoek, M.; Li, Y.; Lu, Z.; Phillpot, S. R.; Sinnott, S. B. Classical\\nAtomistic Simulations of Surfaces and Heterogeneous Interfaces Withthe Charge-Optimized Many Body (COMB) Potentials.Mater. Sci.\\nEng., R 2013, 74, 255−279.\\n(244) Yu, J.; Sinnott, S. B.; Phillpot, S. R. Charge Optimized Many-\\nBody Potential for the Si/SiO2 System. Phys. Rev. B: Condens. MatterMater. Phys. 2007, 75, 085311.\\n(245) Senftle, T. P.; Hong, S.; Islam, M. M.; Kylasa, S. B.; Zheng, Y.;\\nShin, Y. K.; Junkermeier, C.; Engel-Herbert, R.; Janik, M. J.; Aktulga,\\nH. M.; et al. The ReaxFF Reactive Force-Field: Development,Applications and Future Directions. Npj Comput. Mater. 2016, 2,\\n15011.\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9860(246) Van Duin, A. C.; Dasgupta, S.; Lorant, F.; Goddard, W. A.\\nReaxFF: A Reactive Force Field for Hydrocarbons.J. Phys. Chem. A\\n2001, 105, 9396−9409.\\n(247) Rappé, A. K.; Bormann-Rochotte, L. M.; Wiser, D. C.; Hart, J.R.; Pietsch, M. A.; Casewit, C. J.; Skiff, W. M. APT a Next Generation\\nQM-based Reactive Force Field Model.Mol. Phys. 2007, 105, 301−\\n324.\\n(248) Warshel, A.; Weiss, R. M. An Empirical Valence BondApproach for Comparing Reactions in Solutions and in Enzymes.J.\\nAm. Chem. Soc.1980, 102, 6218−6226.\\n(249) Wu, Y.; Chen, H.; Wang, F.; Paesani, F.; Voth, G. A. An\\nImproved Multistate Empirical Valence Bond Model for AqueousProton Solvation and Transport.J. Phys. Chem. B2008, 112, 467−\\n482.\\n(250) Hartke, B.; Grimme, S. Reactive Force Fields Made Simple.\\nPhys. Chem. Chem. Phys.2015, 17, 16715−16718.\\n(251) Singh, U. C.; Kollman, P. A. An Approach to ComputingElectrostatic Charges for Molecules.J. Comput. Chem.1984, 5, 129−\\n145.\\n(252) Storer, J. W.; Giesen, D. J.; Cramer, C. J.; Truhlar, D. G. Class\\nIV Charge Models: A New Semiempirical Approach in QuantumChemistry. J. Comput.-Aided Mol. Des.1995, 9,8 7−110.\\n(253) Mehler, E. L.; Solmajer, T. Electrostatic Effects in Proteins:\\nComparison of Dielectric and Charge Models.Protein Eng., Des. Sel.\\n1991, 4, 903−910.1991, 4, 903−910.\\n(254) Chen, J.; Martínez, T. J. QTPIE: Charge Transfer With\\nPolarization Current Equalization. A Fluctuating Charge Model With\\nCorrect Asymptotics. Chem. Phys. Lett.2007, 438, 315−320.(255) Poier, P. P.; Jensen, F. Describing Molecular Polarizability by\\na Bond Capacity Model.J. Chem. Theory Comput.2019, 15, 3093−\\n3107.\\n(256) Rappé, A. K.; Goddard, W. A. Charge Equilibration forMolecular Dynamics Simulations. J. Phys. Chem. 1991, 95, 3358−\\n3363.\\n(257) Akimov, A. V.; Prezhdo, O. V. Large-Scale Computations in\\nChemistry: A Bird’s Eye View of a Vibrant Field.Chem. Rev. 2015,\\n115, 5797−5890.115, 5797−5890.\\n(258) Harrison, J. A.; Schall, J. D.; Maskey, S.; Mikulski, P. T.;\\nKnippenberg, M. T.; Morrow, B. H. Review of Force Fields and\\nIntermolecular Potentials Used in Atomistic Computational MaterialsResearch. Appl. Phys. Rev.2018, 5, 031104.\\n(259) Piquemal, J. P.; Jordan, K. D. Preface: Special Topic: From\\nQuantum Mechanics to Force Fields. J. Chem. Phys. 2017, 147,\\n161401.\\n(260) Lennard-Jones, J. E. Cohesion.Proc. Phys. Soc.1931, 43, 461−\\n482.482.\\n(261) Tersoff, J. Empirical Interatomic Potential for Silicon With\\nImproved Elastic Properties.Phys. Rev. B: Condens. Matter Mater. Phys.\\n1988, 38, 9902.\\n(262) Vanommeslaeghe, K.; Hatcher, E.; Acharya, C.; Kundu, S.;Zhong, S.; Shim, J.; Darian, E.; Guvench, O.; Lopes, P.; Vorobyov, I.;\\net al. CHARMM General Force Field: A Force Field for Drug-Like\\nMolecules Compatible With the CHARMM All-Atom Additive\\nBiological Force Fields.J. Comput. Chem.2010, 31, 671−690.(263) Zhang, C.; Lu, C.; Jing, Z.; Wu, C.; Piquemal, J.-P.; Ponder, J.\\nW.; Ren, P. AMOEBA Polarizable Atomic Multipole Force Field for\\nNucleic Acids. J. Chem. Theory Comput.2018, 14, 2084−2108.(264) Götz, A. W.; Williamson, M. J.; Xu, D.; Poole, D.; Le Grand,\\nS.; Walker, R. C. Routine Microsecond Molecular Dynamics\\nSimulations With AMBER on GPUs. 1. Generalized Born.J. Chem.\\nTheory Comput. 2012, 8, 1542−1555.Theory Comput. 2012, 8, 1542−1555.\\n(265) Salomon-Ferrer, R.; Götz, A. W.; Poole, D.; Le Grand, S.;\\nWalker, R. C. Routine Microsecond Molecular Dynamics Simulations\\nWith AMBER on GPUs. 2. Explicit Solvent Particle Mesh Ewald.J.Chem. Theory Comput.2013, 9, 3878−3888.\\n(266) Stone, J. E.; Hardy, D. J.; Ufimtsev, I. S.; Schulten, K. GPU-\\naccelerated Molecular Modeling Coming of Age. J. Mol. Graphics\\nModell. 2010, 29, 116−125.Modell. 2010, 29, 116−125.\\n(267) Glaser, J.; Nguyen, T. D.; Anderson, J. A.; Lui, P.; Spiga, F.;\\nMillan, J. A.; Morse, D. C.; Glotzer, S. C. Strong Scaling of General-\\nPurpose Molecular Dynamics Simulations on GPUs.Comput. Phys.Commun. 2015, 192,9 7−107.\\n(268) Lagardère, L.; Jolly, L. H.; Lipparini, F.; Aviat, F.; Stamm, B.;\\nJing, Z. F.; Harger, M.; Torabifard, H.; Cisneros, G. A.; Schnieders,Commun. 2015, 192,9 7−107.\\n(268) Lagardère, L.; Jolly, L. H.; Lipparini, F.; Aviat, F.; Stamm, B.;\\nJing, Z. F.; Harger, M.; Torabifard, H.; Cisneros, G. A.; Schnieders,\\nM. J.; et al. Tinker-Hp: A Massively Parallel Molecular DynamicsPackage for Multiscale Simulations of Large Complex Systems With\\nAdvanced Point Dipole Polarizable Force Fields.Chem. Sci. 2018, 9,\\n956−972.\\n(269) Zhang, Y.; Hu, C.; Jiang, B. Embedded Atom Neural NetworkPotentials: Efficient and Accurate Machine Learning With a Physically\\nInspired Representation. J. Phys. Chem. Lett.2019, 10, 4962−4967.\\n(270) Agrawal, A.; Choudhary, A. Perspective: Materials Informaticsand Big Data: Realization of the ”Fourth Paradigm” of Science in\\nMaterials Science. APL Mater. 2016, 4, 053208.\\n(271) Pun, G. P.; Batra, R.; Ramprasad, R.; Mishin, Y. Physically\\nInformed Artificial Neural Networks for Atomistic Modeling ofMaterials.Nat. Commun. 2019, 10, 2339.\\n(272) Guo, F.; Wen, Y.-S.; Feng, S.-Q.; Li, X.-D.; Li, H.-S.; Cui, S.-\\nX.; Zhang, Z.-R.; Hu, H.-Q.; Zhang, G.-Q.; Cheng, X.-L. Intelligent-\\nReaxFF: Evaluating the Reactive Force Field Parameters WithMachine Learning. Comput. Mater. Sci.2020, 172, 109393.\\n(273) Narayanan, B.; Chan, H.; Kinaci, A.; Sen, F. G.; Gray, S. K.;\\nChan, M. K.; Sankaranarayanan, S. K. Machine Learnt Bond Order\\nPotential to Model Metal-Organic (Co-C) Heterostructures.Nano-scale 2017, 9, 18229−18239.\\n(274) Behler, J.; Parrinello, M . Generalized Neural-Network\\nRepresentation of High-Dimensional Potential-Energy Surfaces.\\nPhys. Rev. Lett.2007, 98, 146401.\\n(275) Wood, M. A.; Thompson, A. P. Extending the Accuracy of theSNAP Interatomic Potential Form.J. Chem. Phys.2018, 148, 241721.\\n(276) Bartók, A. P.; Payne, M. C.; Kondor, R.; Csányi, G. Gaussian\\nApproximation Potentials: The Accuracy of Quantum Mechanics,\\nWithout the Electrons.Phys. Rev. Lett.2010, 104, 136403.(277) Boes, J. R.; Groenenboom, M. C.; Keith, J. A.; Kitchin, J. R.\\nNeural Network and ReaxFF Comparison for Au Properties.Int. J.\\nQuantum Chem. 2016, 116, 979−987.\\n(278) Ingólfsson, H. I.; Lopez, C. A.; Uusitalo, J. J.; de Jong, D. H.;Gopal, S. M.; Periole, X.; Marrink, S. J. The Power of Coarse Graining\\nin Biomolecular Simulations.Wiley Interdiscip. Rev. Comput. Mol. Sci.\\n2014, 4, 225−248.\\n(279) Mennucci, B. Polarizable Continuum Model.Wiley Interdiscip.Rev.: Comput. Mol. Sci.2012, 2, 386−404.\\n(280) Jäger, M.; Schäfer, R.; Johnston, R. L. First Principles Global\\nOptimization of Metal Clusters and Nanoalloys.Adv. Phys. X2018, 3,\\nS100009.\\n(281) Dieterich, J. M.; Hartke, B. OGOLEM: Global ClusterStructure Optimisation for Arbitrary Mixtures of Flexible Molecules.\\nA Multiscaling, Object-Oriented Approach. Mol. Phys. 2010, 108,\\n279−291.\\n(282) Wales, D. J.; Doye, J. P. Global Optimization by Basin-Hopping and the Lowest Energy Structures of Lennard-Jones Clusters\\nContaining Up to 110 Atoms.J. Phys. Chem. A 1997, 101, 5111−\\n5116.\\n(283) Zhang, J.; Dolg, M. ABCluster: The Artificial Bee ColonyAlgorithm for Cluster Global Optimization.Phys. Chem. Chem. Phys.\\n2015, 17, 24173−24181.\\n(284) Goedecker, S. Minima Hopping: An Efficient Search Method\\nfor the Global Minimum of the Potential Energy Surface of ComplexMolecular Systems. J. Chem. Phys.2004, 120, 9911.\\n(285) Schlegel, H. B. Geometry Optimization.Wiley Interdiscip. Rev.:\\nComput. Mol. Sci.2011, 1, 790−809.\\n(286) Sheppard, D.; Terrell, R.; Henkelman, G. OptimizationMethods for Finding Minimum Energy Paths.J. Chem. Phys. 2008,\\n128, 134106.\\n(287) Schlegel, B. H. Estimating the Hessian for Gradient-Type\\nGeometry Optimizations. Theor. Chim. Acta1984, 66, 333−340.\\nChemical Reviews pubs.acs.org/CR ReviewChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9861(288) Henkelman, G.; Uberuaga, B. P.; Jónsson, H. A Climbing\\nImage Nudged Elastic Band Method for Finding Saddle Points and\\nMinimum Energy Paths.J. Chem. Phys.2000, 113, 9901.\\n(289) Sheppard, D.; Xiao, P.; Chemelewski, W.; Johnson, D. D.;Henkelman, G. A Generalized Solid-State Nudged Elastic Band\\nMethod. J. Chem. Phys.2012, 136, 074103.\\n(290) Zimmerman, P. M. Growing String Method With\\nInterpolation and Optimization in Internal Coordinates: Methodand Examples. J. Chem. Phys.2013, 138, 184102.\\n(291) Samanta, A.; Weinan, E. Optimization-Based String Method\\nfor Finding Minimum Energy Path.Commun. Comput. Phys.2013, 14,\\n265−275.\\n(292) Burger, S. K.; Yang, W. Quadratic String Method forDetermining the Minimum-Energy Path Based on Multiobjective\\nOptimization. J. Chem. Phys.2006, 124, 054109.\\n(293) Peterson, A. A. Acceleration of Saddle-Point Searches With\\nMachine Learning. J. Chem. Phys.2016, 145, 074106.(294) Garijo del Río, E.; Mortensen, J. J.; Jacobsen, K. W. Local\\nBayesian Optimizer for Atomic Structures. Phys. Rev. B: Condens.\\nMatter Mater. Phys.2019, 100, 104103.\\n(295) Garrido Torres, J. A.; Jennings, P. C.; Hansen, M. H.; Boes, J.R.; Bligaard, T. Low-Scaling Algorithm for Nudged Elastic Band\\nCalculations Using a Surrogate Machine Learning Model.Phys. Rev.\\nLett. 2019, 122, 156001.\\n(296) Meyer, R.; Schmuck, K. S.; Hauser, A. W. Machine Learningin Computational Chemistry: An Evaluation of Method Performance\\nfor Nudged Elastic Band Calculations.J. Chem. Theory Comput.2019,\\n15, 6513−6523.\\n(297) Koistinen, O. P.; Ásgeirsson, V.; Vehtari, A.; Jónsson, H.Nudged Elastic Band Calculations Accelerated With Gaussian Process\\nRegression Based on Inverse Interatomic Distances.J. Chem. Theory\\nComput. 2019, 15, 6738−6751.\\n(298) Noé, F.; Olsson, S.; Köhler, J.; Wu, H. Boltzmann Generators:Sampling Equilibrium States of Many-Body Systems With Deep\\nLearning.Science 2019, 365, No. eaaw1147.\\n(299) Christensen, A. S.; Faber, F. A.; von Lilienfeld, O. A.\\nOperators in Quantum Machine Learning: Response Properties inChemical Space. J. Chem. Phys.2019, 150, 064105.\\n(300) Gastegger, M.; Schütt, K. T.; Müller, K.-R. Machine Learning\\nof Solvent Eﬀects on Molecular Spectra and Reactions.arXiv, 2020,\\n2010.14942. https://arxiv.org/abs/2010.14942.2010.14942. https://arxiv.org/abs/2010.14942.\\n(301) Varghese, J. J.; Mushrif, S. H. Origins of Complex Solvent\\nEffects on Chemical Reactivity and Computational Tools to\\nInvestigate Them: A Review.React. Chem. Eng.2019, 4, 165−206.(302) Basdogan, Y.; Maldonado, A. M.; Keith, J. A. Advances and\\nChallenges in Modeling Solvated Reaction Mechanisms for Renew-\\nable Fuels and Chemicals. Wiley Interdiscip. Rev.: Comput. Mol. Sci.\\n2020, 10, No. 1446.2020, 10, No. 1446.\\n(303) Tomasi, J.; Mennucci, B.; Cammi, R. Quantum Mechanical\\nContinuum Solvation Models.Chem. Rev. 2005, 105, 2999−3094.\\n(304) Cramer, C. J.; Truhlar, D. G. A Universal Approach toSolvation Modeling. Acc. Chem. Res.2008, 41, 760−768.\\n(305) Klamt, A. The COSMO and COSMO-RS Solvation Models.\\nWiley Interdiscip. Rev.: Comput. Mol. Sci.2011, 1, 699−709.\\n(306) Hirata, F., Ed.Molecular Theory of Solvation; Kluwer AcademicPublishers: Norwell, MA, 2003; Vol.24.\\n(307) Miertuš, S.; Scrocco, E.; Tomasi, J. Electrostatic Interaction of\\na Solute With a Continuum. A Direct Utilizaion of Ab Initio\\nMolecular Potentials for the Prevision of Solvent Effects.Chem. Phys.1981, 55, 117−129.\\n(308) Cances, E.; Mennucci, B.; Tomasi, J. A New Integral Equation\\nFormalism for the Polarizable Continuum Model: Theoretical\\nBackground and Applications to Isotropic and Anisotropic Dielectrics.J. Chem. Phys.1997, 107, 3032−3041.\\n(309) Marenich, A. V.; Cramer, C. J.; Truhlar, D. G. Universal\\nSolvation Model Based on Solute Electron Density and on a\\nContinuum Model of the Solvent Defined by the Bulk DielectricConstant and Atomic Surface Tensions.J. Phys. Chem. B2009, 113,\\n6378−6396.\\n(310) Barone, V.; Cossi, M. Quantum Calculation of Molecular\\nEnergies and Energy Gradients in Solution by a Conductor Solvent6378−6396.\\n(310) Barone, V.; Cossi, M. Quantum Calculation of Molecular\\nEnergies and Energy Gradients in Solution by a Conductor Solvent\\nModel. J. Phys. Chem. A1998, 102, 1995−2001.\\n(311) Klamt, A.; Schüürmann, G. COSMO: A New Approach toDielectric Screening in Solvents With Explicit Expressions for the\\nScreening Energy and Its Gradient. J. Chem. Soc., Perkin Trans. 2\\n1993, 799−805.\\n(312) Klamt, A. Conductor-Like Screening Model for Real Solvents:A New Approach to the Quantitative Calculation of Solvation\\nPhenomena.J. Phys. Chem.1995, 99, 2224−2235.\\n(313) Bernales, V. S.; Marenich, A. V.; Contreras, R.; Cramer, C. J.;\\nTruhlar, D. G. Quantum Mechanical Continuum Solvation Modelsfor Ionic Liquids.J. Phys. Chem. B2012, 116, 9122−9129.\\n(314) Truchon, J. F.; Pettitt, B. M.; Labute, P. A Cavity Corrected\\n3d-Rism Functional for Accurate Solvation Free Energies.J. Chem.\\nTheory Comput. 2014, 10, 934−941.Theory Comput. 2014, 10, 934−941.\\n(315) Nishihara, S.; Otani, M. Hybrid Solvation Models for Bulk,\\nInterface, and Membrane: Reference Interaction Site Methods\\nCoupled With Density Functional Theory. Phys. Rev. B: Condens.Matter Mater. Phys.2017, 96, 115429.\\n(316) Kamerlin, S. C.; Haranczyk, M.; Warshel, A. Progress in Ab\\nInitio QM/MM Free-Energy Simulations of Electrostatic Energies in\\nProteins: Accelerated QM/MM Studies of pKa, Redox Reactions andSolvation Free Energies.J. Phys. Chem. B2009, 113, 1253−1272.\\n(317) Boereboom, J. M.; Fleurat-Lessard, P.; Bulo, R. E. Explicit\\nSolvation Matters: Performance of QM/MM Solvation Models in\\nNucleophilic Addition.J. Chem. Theory Comput. 2018, 14, 1841−1852.\\n(318) Gregersen, B. A.; Lopez, X.; York, D. M. Hybrid QM/MM\\nStudy of Thio Effects in Transphosphorylation Reactions: The Role\\nof Solvation.J. Am. Chem. Soc.2004, 126, 7504−7513.\\n(319) Maldonado, A. M.; Basdogan, Y.; Berryman, J. T.; Rempe, S.B.; Keith, J. A. First-Principles Modeling of Chemistry in Mixed\\nSolvents: Where to Go From Here?J. Chem. Phys.2020, 152, 130902.\\n(320) Skyner, R.; McDonagh, J.; Groom, C.; Van Mourik, T.;Mitchell, J. A Review of Methods for the Calculation of Solution Free\\nEnergies and the Modelling of Systems in Solution.Phys. Chem. Chem.\\nPhys. 2015, 17, 6174−6191.\\n(321) Basdogan, Y.; Groenenboom, M. C.; Henderson, E.; De, S.;Rempe, S. B.; Keith, J. A. Machine Learning-Guided Approach for\\nStudying Solvation Environments.J. Chem. Theory Comput.2020, 16,\\n633−642.\\n(322) Pratt, L. R.; Laviolette, R. A. Quasi-Chemical Theories ofAssociated Liquids. Mol. Phys. 1998, 94, 909−915.\\n(323) Rempe, S. B.; Pratt, L. R.; Hummer, G.; Kress, J. D.; Martin,\\nR. L.; Redondo, A. The Hydration Number of Li+ in Liquid Water.J.\\nAm. Chem. Soc.2000, 122, 966−967.Am. Chem. Soc.2000, 122, 966−967.\\n(324) Chew, A. K.; Jiang, S.; Zhang, W.; Zavala, V. M.; Van Lehn, R.\\nC. Fast Predictions of Liquid-Phase Acid-Catalyzed Reaction Rates\\nUsing Molecular Dynamics Simulations and Convolutional NeuralNetworks.Chem. Sci. 2020, 11, 12464−12476.\\n(325) Zhang, P.; Shen, L.; Yang, W. Solvation Free Energy\\nCalculations With Quantum Mechanics/Molecular Mechanics and\\nMachine Learning Models.J. Phys. Chem. B2019, 123, 901−908.(326) Katritzky, A. R.; Kuanar, M.; Slavov, S.; Hall, C. D.; Karelson,\\nM.; Kahn, I.; Dobchev, D. A. Quantitative Correlation of Physical and\\nChemical Properties With Chemical Structure: Utility for Prediction.\\nChem. Rev. 2010, 110, 5714−5789.Chem. Rev. 2010, 110, 5714−5789.\\n(327) Muratov, E. N.; et al. QSAR Without Borders.Chem. Soc. Rev.\\n2020, 49, 3525−3564.\\n(328) Fourches, D.; Muratov, E.; Tropsha, A. Trust, but Verify: Onthe Importance of Chemical Structure Curation in Cheminformatics\\nand QSAR Modeling Research.J. Chem. Inf. Model.2010, 50, 1189−\\n1204.\\n(329) Geerlings, P.; Chamorro, E.; Chattaraj, P. K.; De Proft, F.;Gázquez, J. L.; Liu, S.; Morell, C.; Toro-Labbé, A.; Vela, A.; Ayers, P.\\nConceptual Density Functional Theory: Status, Prospects, Issues.\\nTheor. Chem. Acc.2020, 139, 36.\\nChemical Reviews pubs.acs.org/CR ReviewChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9862(330) von Rudorff, G. F.; von Lilienfeld, O. A. Alchemical\\nPerturbation Density Functional Theory. Phys. Rev. Res. 2020, 2,\\n023220.\\n(331) Hinton, G. E.; Srivastava, N.; Krizhevsky, A.; Sutskever, I.;Salakhutdinov, R. R. Improving Neural Networks by Preventing Co-\\nAdaptation of Feature Detectors. arXiv, 2012, 1207.0580. https://\\narxiv.org/abs/1207.0580.\\n(332) Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P.; Reed, S.; Anguelov,D.; Erhan, D.; Vanhoucke, V.; Rabinovich, A. Going Deeper With\\nConvolutions. Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition.2015,1 −9.\\n(333) Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.; Ma,S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.; et al. Imagenet\\nLarge Scale Visual Recognition Challenge.Int. J. Comput. Vis.2015,\\n115, 211−252.\\n(334) Krizhevsky, A.; Sutskever, I.; Hinton, G. E. ImagenetClassification With Deep Convolutional Neural Networks.Commun.\\nACM 2017, 60,8 4−90.\\n(335) Blei, D. M.; Ng, A. Y.; Jordan, M. I. Latent Dirichlet\\nAllocation. J. Mach. Learn. Res.2003, 3, 993−1022.(336) Bengio, Y.; Ducharme, R.; Vincent, P.; Jauvin, C. A Neural\\nProbabilistic Language Model. J. Mach. Learn. Res. 2003, 3, 1137−\\n1155.\\n(337) Wu, Y.; Schuster, M.; Chen, Z.; Le, Q. V.; Norouzi, M.;Macherey, W.; Krikun, M.; Cao, Y.; Gao, Q.; Macherey, K., et al.\\nGoogle’s Neural Machine Translation System: Bridging the Gap\\nBetween Human and Machine Translation.arXiv, 2016, 1609.08144.\\nhttps://arxiv.org/abs/1609.08144.https://arxiv.org/abs/1609.08144.\\n(338) Mikolov, T.; Chen, K.; Corrado, G.; Dean, J. E ﬃcient\\nEstimation of Word Representations in Vector Space.arXiv, 2013,\\n1301.3781. https://arxiv.org/abs/1301.3781.1301.3781. https://arxiv.org/abs/1301.3781.\\n(339) Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\\nGomez, A. N.; Kaiser,Ł .; Polosukhin, I. Attention Is All You Need.\\nAdv. Neural Inf. Process. Syst.2017, 30, 5998−6008.(340) Hastie, T.; Tibshirani, R.; Friedman, J. The Elements of\\nStatistical Learning: Data Mining, Inference, and Prediction; Springer:\\nNew York, NY, 2009.\\n(341) Rasmussen, C. E. Gaussian Processes in Machine Learning.Advanced Lectures on Machine Learning. ML 2003. Lecture Notes in\\nComputer Science: Berlin, 2004; pp 63−71.\\n(342) Bishop, C. M. Pattern Recognition and Machine Learning ;\\nSpringer: New York, NY, 2006.Springer: New York, NY, 2006.\\n(343) Hornik, K.; Stinchcom be, M.; White, H. Multilayer\\nFeedforward Networks Are Universal Approximators. Neural Netw.\\n1989, 2, 359−366.\\n(344) Tran, D.; Ranganath, R.; Blei, D. M. The Variational GaussianProcess. arXiv preprint , 2015, 1511.06499. https://arxiv.org/abs/\\n1511.06499.\\n(345) Vapnik, V. N. The Nature of Statistical Learning Theory ;\\nSpringer: New York, NY, 1995.\\n(346) Poggio, T.; Girosi, F. Networks for Approximation andLearning. Proc. IEEE 1990, 78, 1481−1497.\\n(347) Smola, A. J.; Schölkopf, B.; Müller, K.-R. The Connection\\nBetween Regularization Operators and Support Vector Kernels.\\nNeural Netw. 1998, 11, 637−649.Neural Netw. 1998, 11, 637−649.\\n(348) Rasmussen, C. E.; Williams, C. K. I.Gaussian Processes for\\nMachine Learning (Adaptive Computation and Machine Learning); MIT\\nPress: Cambridge, MA, 2005.Press: Cambridge, MA, 2005.\\n(349) Caruana, R.; Lawrence, S.; Giles, C. L. Overfitting in Neural\\nNets: Backpropagation, Conjugate Gradient, and Early Stopping.\\nAdvances in Neural Information Processing Systems2001, 402−408.(350) Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.;\\nSalakhutdinov, R. Dropout: A Simple Way to Prevent Neural\\nNetworks From Overfitting. J. Mach. Learn. Res. 2014, 15, 1929−\\n1958.1958.\\n(351) Geman, S.; Bienenstock, E.; Doursat, R. Neural Networks and\\nthe Bias/Variance Dilemma.Neural Comput. 1992, 4,1 −58.\\n(352) Schütt, K. T.; Arbabzadah, F.; Chmiela, S.; Müller, K. R.;Tkatchenko, A. Quantum-Chemical Insights From Deep Tensor\\nNeural Networks.Nat. Commun. 2017, 8, 13890.\\n(353) Bietti, A.; Mairal, J. On the Inductive Bias of Neural Tangent\\nKernels. arXiv, 2019, 1905.12173.https://arxiv.org/abs/1905.12173(354) Montavon, G.; Lapuschkin, S.; Binder, A.; Samek, W.; Müller,Kernels. arXiv, 2019, 1905.12173.https://arxiv.org/abs/1905.12173\\n(354) Montavon, G.; Lapuschkin, S.; Binder, A.; Samek, W.; Müller,\\nK.-R. Explaining Nonlinear Classification Decisions With Deep TaylorDecomposition. Pattern Recognit. 2017, 65, 211−222.\\n(355) Samek, W., Montavon, G., Vedaldi, A., Hansen, L. K., Müller,\\nK.-R., Eds. Explainable AI: Interpreting, Explaining and VisualizingDeep Learning; Lecture Notes in Computer Science; Springer: New\\nYork, NY, 2019; Vol.11700.\\n(356) Baehrens, D.; Schroeter, T.; Harmeling, S.; Kawanabe, M.;\\nHansen, K.; Müller, K.-R. How to Explain Individual ClassificationDecisions. J. Mach. Learn. Res.2010, 11, 1803−1831.\\n(357) Bach, S.; Binder, A.; Montavon, G.; Klauschen, F.; Müller, K.-\\nR.; Samek, W. On Pixel-Wise Explanations for Non-Linear ClassifierDecisions by Layer-Wise Relevance Propagation.PLoS One2015, 10,\\nNo. e0130140.\\n(358) Montavon, G.; Samek, W.; Mu ̈ller, K.-R. Methods for\\nInterpreting and Understanding Deep Neural Networks.Digit. Signal\\nProcess. 2018, 73,1 −15.Process. 2018, 73,1 −15.\\n(359) Holzinger, A. From Machine Learning to Explainable AI.2018\\nWorld Symposium on Digital Intelligence for Systems and Machines\\n(DISA); 2018; pp 55−66.\\n(360) Lapuschkin, S.; Wäldchen, S.; Binder, A.; Montavon, G.;Samek, W.; Müller, K.-R. Unmasking Clever Hans Predictors and\\nAssessing What Machines Really Learn. Nat. Commun. 2019, 10,\\n1096.\\n(361) Samek, W.; Montavon, G.; Lapuschkin, S.; Anders, C. J.;Muller, K.-R. Explaining deep neural networks and beyond: A review\\nof methods and applications.Proc. IEEE 2021, 109, 247−278.\\n(362) Bongard, J.; Lipson, H. Automated Reverse Engineering ofNonlinear Dynamical Systems. Proc. Natl. Acad. Sci. U. S. A.2007,\\n104, 9943−9948.\\n(363) Schmidt, M.; Lipson, H. Distilling Free-Form Natural Laws\\nFrom Experimental Data.Science 2009, 324,8 1−85.From Experimental Data.Science 2009, 324,8 1−85.\\n(364) Brunton, S. L.; Proctor, J. L.; Kutz, J. N. Discovering\\nGoverning Equations From Data by Sparse Identification of\\nNonlinear Dynamical Systems. Proc. Natl. Acad. Sci. U. S. A.2016,113, 3932−3937.\\n(365) Boninsegna, L.; Nüske, F.; Clementi, C. Sparse Learning of\\nStochastic Dynamical Equations.J. Chem. Phys.2018, 148, 241723.\\n(366) Hoffmann, M.; Fröhner, C.; Noé, F. Reactive SINDy:Discovering Governing Reactions From Concentration Data. J.\\nChem. Phys. 2019, 150, 025101.\\n(367) Watters, N.; Zoran, D.; Weber, T.; Battaglia, P.; Pascanu, R.;\\nTacchetti, A. Visual Interaction Networks: Learning a PhysicsSimulator From Video. Adv. Neural Inf. Process. Syst. 2017, 4539−\\n4547.\\n(368) Raissi, M. Deep Hidden Physics Models: Deep Learning of\\nNonlinear Partial Differential Equations.J. Mach. Learn. Res.2018, 19,\\n932−955.932−955.\\n(369) Ahneman, D. T.; Estrada, J. G.; Lin, S.; Dreher, S. D.; Doyle,\\nA. G. Predicting Reaction Performance in C-N Cross-Coupling Using\\nMachine Learning. Science 2018, 360, 186−190.\\n(370) Chuang, K. V.; Keiser, M. J. Comment on “PredictingReaction Performance in C-N Cross-Coupling Using Machine\\nLearning. Science 2018, 362, No. eaat8603.\\n(371) Estrada, J. G.; Ahneman, D. T.; Sheridan, R. P.; Dreher, S. D.;\\nDoyle, A. G. Response to Comment on “Predicting ReactionPerformance in C-N Cross-Coupling Using Machine Learning.Science\\n2018, 362, No. eaat8763.\\n(372) Chmiela, S.; Tkatchenko, A.; Sauceda, H. E.; Poltavsky, I.;\\nSchütt, K. T.; Müller, K.-R. Machine Learning of Accurate Energy-Conserving Molecular Force Fields.Sci. Adv. 2017, 3, No. e1603015.\\n(373) Ghiringhelli, L. M.; Vybiral, J.; Levchenko, S. V.; Draxl, C.;\\nScheffler, M. Big Data of Materials Science: Critical Role of the\\nDescriptor.Phys. Rev. Lett.2015, 114, 105503.Descriptor.Phys. Rev. Lett.2015, 114, 105503.\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9863(374) Cheng, B.; Griffiths, R.-R.; Wengert, S.; Kunkel, C.; Stenczel,\\nT.; Zhu, B.; Deringer, V. L.; Bernstein, N.; Margraf, J. T.; Reuter, K.;\\net al. Mapping Materials and Molecules.Acc. Chem. Res. 2020, 53,\\n1981−1991.1981−1991.\\n(375) Reinhardt, A.; Pickard, C. J.; Cheng, B. Predicting the Phase\\nDiagram of Titanium Dioxide With Random Search and Pattern\\nRecognition.Phys. Chem. Chem. Phys.2020, 22, 12697−12705.(376) Meila, M.; Koelle, S.; Zhang, H. A Regression Approach for\\nExplaining Manifold Em bedding Coordinates. arXiv , 2018,\\n1811.11891. https://arxiv.org/abs/1811.11891.\\n(377) Cox, M. A.; Cox, T. F. Handbook of Data Visualization ;Springer, 2008; pp 315−347.\\n(378) Schölkopf, B.; Smola, A.; Mu ̈ller, K.-R. Kernel Principal\\nComponent Analysis. International Conference on Arti ﬁcial Neural\\nNetworks; 1997; pp 583−588.Networks; 1997; pp 583−588.\\n(379) Schölkopf, B.; Smola, A.; Müller, K.-R. Nonlinear Component\\nAnalysis as a Kernel Eigenvalue Problem.Neural Comput. 1998, 10,\\n1299−1319.\\n(380) Maaten, L. v. d.; Hinton, G. Visualizing Data Using T-Sne.J.Mach. Learn. Res.2008, 9, 2579−2605.\\n(381) Ceriotti, M.; Tribello, G. A.; Parrinello, M. Simplifying the\\nRepresentation of Complex Free-Energy Landscapes Using Sketch-\\nMap. Proc. Natl. Acad. Sci. U. S. A.2011, 108, 13023−13028.(382) McInnes, L.; Healy, J.; Melville, J. UMAP: Uniform Manifold\\nApproximation and Projection for Dimension Reduction.arXiv, 2018,\\n1802.03426. https://arxiv.org/abs/1802.03426.\\n(383) Ruff, L.; Kauffmann, J. R.; Vandermeulen, R. A.; Montavon,G.; Samek, W.; Kloft, M.; Dietterich, T. G.; Müller, K.-R. A Unifying\\nReview of Deep and Shallow Anomaly Detection.Proc. IEEE 2021,\\n109, 756−795.\\n(384) Kaelbling, L. P.; Littman, M. L.; Moore, A. W. ReinforcementLearning: A Survey.J. Artif. Intell. Res.1996, 4, 237−285.\\n(385) Rosenblatt, F. The Perceptron: A Probabilistic Model for\\nInformation Storage and Organization in the Brain. Psychol. Rev.\\n1958, 65, 386−408.1958, 65, 386−408.\\n(386) Rosenblatt, F. Principles of Neurodynamics.Perceptrons and\\nthe Theory of Brain Mechanisms ; Cornell Aeronautical Lab, Inc.:\\nBuﬀalo, NY, 1961.\\n(387) Minsky, M.; Papert, S. A. Perceptrons: An Introduction toComputational Geometry; MIT Press: Cambridge, MA, 2017.\\n(388) Rumelhart, D. E.; Hinton, G. E.; Williams, R. J. Learning\\nRepresentations by Back-Propagating Errors.Nature 1986, 323, 533−\\n536.536.\\n(389) Lecun, Y. Une procédure d’apprentissage pour réseau àseuil\\nasymétrique (A learning scheme for asymmetric threshold networks).\\nProceedings of Cognitiva 85; Paris, France, 1985; pp 599−604.(390) Bishop, C. M.Neural Networks for Pattern Recognition; Oxford\\nUniversity Press: New York, NY, 1995.\\n(391) Funahashi, K.-I. On the Approximate Realization of\\nContinuous Mappings by Neural Networks. Neural Netw. 1989, 2,\\n183−192.183−192.\\n(392) Cybenko, G. Approximation by Superpositions of a Sigmoidal\\nFunction. Math. Control. Signals Syst.1989, 2, 303−314.\\n(393) Cortes, C.; Vapnik, V. Support-Vector Networks.Mach. Learn.\\n1995, 20, 273−297.1995, 20, 273−297.\\n(394) Müller, K.-R.; Mika, S.; Rätsch, G.; Tsuda, K.; Schölkopf, B.\\nAn Introduction to Kernel-Based Learning Algorithms.IEEE Trans.\\nNeural Netw. 2001, 12, 181−201.\\n(395) Schölkopf, B.; Smola, A. J. Learning With Kernels: SupportVector Machines, Regularization, Optimization, and Beyond; MIT Press:\\nCambridge, MA, 2002.\\n(396) Schölkopf, B.; Mika, S.; Burges, C. J.; Knirsch, P.; Müller, K.-\\nR.; Rätsch, G.; Smola, A. J. Input Space Versus Feature Space inKernel-Based Methods. IEEE Trans. Neural Netw. 1999, 10, 1000−\\n1017.\\n(397) Mu ̈ller, K.-R.; Smola, A. J.; Rätsch, G.; Schölkopf, B.;\\nKohlmorgen, J.; Vapnik, V. Predicting Time Series With SupportVector Machines.International Conference on Arti ﬁcial Neural\\nNetworks; 1997; pp 999−1004.\\n(398) Harmeling, S.; Ziehe, A.; Kawanabe, M.; Müller, K.-R. Kernel-\\nBased Nonlinear Blind Source Separation.Neural Comput. 2003, 15,\\n1089−1124.1089−1124.\\n(399) Braun, M. L.; Buhmann, J. M.; Müller, K.-R. On Relevant\\nDimensions in Kernel Feature Spaces.J. Mach. Learn. Res.2008, 9,\\n1875−1908.1089−1124.\\n(399) Braun, M. L.; Buhmann, J. M.; Müller, K.-R. On Relevant\\nDimensions in Kernel Feature Spaces.J. Mach. Learn. Res.2008, 9,\\n1875−1908.\\n(400) Montavon, G.; Braun, M. L.; Müller, K.-R. Kernel Analysis ofDeep Networks. J. Mach. Learn. Res.2011, 12, 2563−2581.\\n(401) Montavon, G.; Braun, M. L.; Krueger, T.; Mu ̈ller, K.-R.\\nAnalyzing Local Structure in Kernel-Based Learning: Explanation,\\nComplexity, and Reliability Assessment.IEEE Signal Process. Mag.2013, 30,6 2−74.\\n(402) Mitchell, J. B. O. Machine Learning Methods in Chemo-\\ninformatics. Wiley Interdiscip. Rev.: Comput. Mol. Sci.2014, 4, 468−\\n481.\\n(403) Ballester, P. J.; Mitchell, J. B. O. A Machine LearningApproach to Predicting Protein-Ligand Binding Affinity With\\nApplications to Molecular Docking.Bioinformatics 2010, 26, 1169−\\n1175.\\n(404) Mizoguchi, T.; Kiyohara, S. Machine Learning Approaches for\\nELNES/XANES. Microscopy 2020, 69,9 2−109.ELNES/XANES. Microscopy 2020, 69,9 2−109.\\n(405) Carr, D. A.; Lach-Hab, M.; Yang, S.; Vaisman, I. I.; Blaisten-\\nBarojas, E. Machine Learning Approach for Structure-Based Zeolite\\nClassification.Microporous Mesoporous Mater.2009, 117, 339−349.(406) Legrain, F.; Carrete, J.; Van Roekeghem, A.; Madsen, G. K.;\\nMingo, N. Materials Screening for the Discovery of New Half-\\nHeuslers: Machine Learning Versus Ab Initio Methods.J. Phys. Chem.\\nB 2018, 122, 625−632.B 2018, 122, 625−632.\\n(407) Lam Pham, T.; Kino, H.; Terakura, K.; Miyake, T.; Tsuda, K.;\\nTakigawa, I.; Chi Dam, H. Machine Learning Reveals Orbital\\nInteraction in Materials.Sci. Technol. Adv. Mater.2017, 18, 756−765.(408) Sugiyama, M.; Krauledat, M.; Müller, K.-R. Covariate Shift\\nAdaptation by Importance Weighted Cross Validation.J. Mach. Learn.\\nRes. 2007, 8, 985−1005.\\n(409) Sugiyama, M.; Kawanabe, M. Machine Learning in Non-Stationary Environments: Introduction to Covariate Shift Adaptation;\\nMIT Press: Cambridge, MA, 2012.\\n(410) Zien, A.; Rätsch, G.; Mika, S.; Schölkopf, B.; Lengauer, T.;\\nMüller, K.-R. Engineering Support Vector Machine Kernels ThatRecognize Translation Initiation Sites.Bioinformatics 2000, 16, 799−\\n807.\\n(411) Behler, J. Atom-Centered Symmetry Functions for Construct-\\ning High-Dimensional Neural Network Potentials. J. Chem. Phys.\\n2011, 134, 074106.2011, 134, 074106.\\n(412) Bartók, A. P.; Kondor, R.; Csányi, G. On Representing\\nChemical Environments. Phys. Rev. B: Condens. Matter Mater. Phys.\\n2013, 87, 184115.\\n(413) Rupp, M.; Tkatchenko, A.; Müller, K.-R.; von Lilienfeld, O. A.Fast and Accurate Modeling of Molecular Atomization Energies With\\nMachine Learning. Phys. Rev. Lett.2012, 108, 058301.\\n(414) Faber, F.; Lindmaa, A.; von Lilienfeld, O. A.; Armiento, R.\\nCrystal Structure Representations for Machine Learning Models ofFormation Energies. Int. J. Quantum Chem.2015, 115, 1094−1101.\\n(415) Hansen, K.; Biegler, F.; Ramakrishnan, R.; Pronobis, W.; von\\nLilienfeld, O. A.; Müller, K. R.; Tkatchenko, A. Machine LearningPredictions of Molecular Properties: Accurate Many-Body Potentials\\nand Nonlocality in Chemical Space. J. Phys. Chem. Lett. 2015, 6,\\n2326−2331.\\n(416) Christensen, A. S.; Bratholm, L. A.; Faber, F. A.; Anatole vonLilienfeld, O. FCHL Revisited: Faster and More Accurate Quantum\\nMachine Learning. J. Chem. Phys.2020, 152, 044107.\\n(417) Faber, F. A.; Christensen, A. S.; Huang, B.; von Lilienfeld, O.A. Alchemical and Structural Distribution Based Representation for\\nUniversal Quantum Machine Learning. J. Chem. Phys. 2018, 148,\\n241717.\\n(418) Huo, H.; Rupp, M. Uniﬁed Representation of Molecules andCrystals for Machine Learning. arXiv, 2017, 1704.06439. https://\\narxiv.org/abs/1704.06439.\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9864(419) Schütt, K. T.; Glawe, H.; Brockherde, F.; Sanna, A.; Müller,\\nK.-R.; Gross, E. K. U. How to Represent Crystal Structures for\\nMachine Learning: Towards Fast Prediction of Electronic Properties.Phys. Rev. B: Condens. Matter Mater. Phys.2014, 89, 205118.\\n(420) Drautz, R. Atomic Cluster Expansion for Accurate and\\nTransferable Interatomic Potentials. Phys. Rev. B: Condens. Matter\\nMater. Phys. 2019, 99, 014104.Mater. Phys. 2019, 99, 014104.\\n(421) Gilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; Dahl, G.\\nE. Neural Message Passing for Quantum Chemistry.34th International\\nConference on Machine Learning ICML 2017; 2017; pp 2053−2070.(422) Schütt, K. T.; Kindermans, P. J.; Sauceda, H. E.; Chmiela, S.;\\nTkatchenko, A.; Mu ̈ller, K. R. SchNet: A Continuous-Filter\\nConvolutional Neural Network for Modeling Quantum Interactions.\\nAdv. Neural Inf. Process. Syst.2017, 30, 992−1002.(423) Kocer, E.; Mason, J. K.; Erturk, H. A Novel Approach to\\nDescribe Chemical Environments in High-Dimensional Neural\\nNetwork Potentials.J. Chem. Phys.2019, 150, 154102.\\n(424) Duvenaud, D.; Maclaurin, D.; Aguilera-Iparraguirre, J.;Gómez-Bombarelli, R.; Hirzel, T.; Aspuru-Guzik, A.; Adams, R. P.\\nConvolutional Networks on Graphs for Learning Molecular Finger-\\nprints. Adv. Neural Inf. Process. Syst.2015, 28, 2224−2232.(425) Li, Z.; Wang, S.; Chin, W. S.; Achenie, L. E.; Xin, H. High-\\nThroughput Screening of Bimetallic Catalysts Enabled by Machine\\nLearning. J. Mater. Chem. A2017, 5, 24131−24138.\\n(426) Lim, J.; Ryu, S.; Kim, J. W.; Kim, W. Y. Molecular GenerativeModel Based on Conditional Variational Autoencoder for De Novo\\nMolecular Design. J. Cheminf. 2018, 10, 31.\\n(427) Rogers, D.; Hahn, M. Extended-Connectivity Fingerprints.J.\\nChem. Inf. Model.2010, 50, 742−754.Chem. Inf. Model.2010, 50, 742−754.\\n(428) Ehmki, E. S. R.; Schmidt, R.; Ohm, F.; Rarey, M. Comparing\\nMolecular Patterns Using the Example of SMARTS: Applications and\\nFilter Collection Analysis.J. Chem. Inf. Model.2019, 59, 2572−2586.(429) Schmidt, R.; Ehmki, E. S.; Ohm, F.; Ehrlich, H. C.;\\nMashychev, A.; Rarey, M. Comparing Molecular Patterns Using the\\nExample of SMARTS: Theory and Algorithms.J. Chem. Inf. Model.\\n2019, 59, 2560−2571.2019, 59, 2560−2571.\\n(430) Weininger, D. SMILES, a Chemical Language and\\nInformation System: 1: Introduction to Methodology and Encoding\\nRules.J. Chem. Inf. Model.1988, 28,3 1−36.\\n(431) Weininger, D.; Weininger, A.; Weininger, J. L. SMILES. 2.Algorithm for Generation of Unique SMILES Notation.J. Chem. Inf.\\nComput. Sci. 1989, 29,9 7−101.\\n(432) Behler, J. Neural Network Potential-Energy Surfaces in\\nChemistry: A Tool for Large-Scale Simulations.Phys. Chem. Chem.\\nPhys. 2011, 13, 17930−17955.Phys. 2011, 13, 17930−17955.\\n(433) Behler, J. Perspective: Machine Learning Potentials for\\nAtomistic Simulations. J. Chem. Phys.2016, 145, 170901.\\n(434) Schütt, K. T.; Sauceda, H. E.; Kindermans, P.-J.; Tkatchenko,A.; Müller, K.-R. SchNet-A Deep Learning Architecture for Molecules\\nand Materials. J. Chem. Phys.2018, 148, 241722.\\n( 4 3 5 )U n k e ,O .T . ;M e u w l y ,M .AR e a c t i v e ,S c a l a b l e ,a n dTransferable Model for Molecular Energies From a Neural Network\\nApproach Based on Local Information.J. Chem. Phys. 2018, 148,\\n241708.\\n(436) Murray, I. Gaussian Processes and Fast Matrix-VectorMultiplies. NUMML 2009 Numerical Mathematics in Machine Learning\\nICML 2009 Workshop. 2009.\\n(437) Wilson, A. G.; Hu, Z.; Salakhutdinov, R.; Xing, E. P. Deep\\nKernel Learning. Proceedings of the 19th International Conference onArtificial Intelligence and Statistics2016, 51, 370−378.\\n(438) Gardner, J. R.; Pleiss, G.; Wu, R.; Weinberger, K. Q.; Wilson,\\nA. G. Product Kernel Interpolation for Scalable Gaussian Processes.arXiv, 2018, 1802.08903.https://arxiv.org/abs/1802.08903.\\n(439) Gardner, J.; Pleiss, G.; Weinberger, K. Q.; Bindel, D.; Wilson,\\nA. G. Gpytorch: Blackbox Matrix-Matrix Gaussian Process InferenceWith Gpu Acceleration.Adv. Neural Inf. Process. Syst.2018, 31, 7576−\\n7586.\\n(440) Wang, K.; Pleiss, G.; Gardner, J.; Tyree, S.; Weinberger, K. Q.;With Gpu Acceleration.Adv. Neural Inf. Process. Syst.2018, 31, 7576−\\n7586.\\n(440) Wang, K.; Pleiss, G.; Gardner, J.; Tyree, S.; Weinberger, K. Q.;\\nWilson, A. G. Exact Gaussian Processes on a Million Data Points.Adv.Neural Inf. Process. Syst.2019, 32, 14648−14659.\\n(441) LeCun, Y. A.; Bottou, L.; Orr, G. B.; Müller, K.-R. In Neural\\nNetworks: Tricks of the Trade; Lecture Notes in Computer Science;Montavon, G., Orr, G. B., Müller, K.-R., Eds.; Springer-Verlag: Berlin,\\n2012; Vol. 7700;p p9 −48.\\n(442) Hansen, K.; Montavon, G.; Biegler, F.; Fazli, S.; Rupp, M.;\\nScheffler, M.; von Lilienfeld, O. A.; Tkatchenko, A.; Müller, K.-R.Assessment and Validation of Machine Learning Methods for\\nPredicting Molecular Atomization Energies.J. Chem. Theory Comput.\\n2013, 9, 3404−3419.\\n(443) MacKay, D. J. The Evidence Framework Applied toClassification Networks. Neural Comput. 1992, 4, 720−736.\\n(444) MacKay, D. J. A Practical Bayesian Framework for\\nBackpropagation Networks. Neural Comput. 1992, 4, 448−472.\\n(445) Kwok, J. T.-Y. The Evidence Framework Applied to SupportVector Machines. IEEE Trans. Neural Netw.2000, 11, 1162−1173.\\n(446) Akaike, H. A New Look at the Statistical Model Identification.\\nIEEE Trans. Autom. Control1974, 19, 716−723.\\n(447) Schwarz, G. Estimating the Dimension of a Model.Ann. Statis.1978, 6, 461−464.\\n(448) Murata, N.; Yoshizawa, S.; Amari, S. Network Information\\nCriterion-Determining the Number of Hidden Units for an Artificial\\nNeural Network Model.IEEE Trans. Neural Netw.1994, 5, 865−872.(449) Wang, J.; Olsson, S.; Wehmeyer, C.; Pérez, A.; Charron, N. E.;\\nDe Fabritiis, G.; Noé, F.; Clementi, C. Machine Learning of Coarse-\\nGrained Molecular Dynamics Force Fields.ACS Cent. Sci. 2019, 5,\\n755−767.755−767.\\n(450) Wang, J.; Chmiela, S.; Müller, K.-R.; Noé, F.; Clementi, C.\\nEnsemble Learning of Coarse-Grained Molecular Dynamics Force\\nFields With a Kernel Approach.J. Chem. Phys.2020, 152, 194106.(451) Bonati, L.; Rizzi, V.; Parrinello, M. Data-Driven Collective\\nVariables for Enhanced Sampling. J. Phys. Chem. Lett. 2020, 11,\\n2998−3004.\\n(452) Willatt, M. J.; Musil, F.; Ceriotti, M. Atom-DensityRepresentations for Machine Learning. J. Chem. Phys. 2019, 150,\\n154110.\\n(453) Musil, F.; Grisaﬁ, A.; Bartók, A. P.; Ortner, C.; Csányi, G.;\\nCeriotti, M. Physics-Inspired Structural Representations for Moleculesand Materials. arXiv, 2021, 2101.04673. https://arxiv.org/abs/2101.\\n04673.\\n(454) Sadeghi, A.; Ghasemi, S. A.; Schaefer, B.; Mohr, S.; Lill, M. A.;\\nGoedecker, S. Metrics for Measuring Distances in ConfigurationSpaces. J. Chem. Phys.2013, 139, 184118.\\n(455) Barker, J.; Bulin, J.; Hamaekers, J.; Mathias, S. InScientiﬁc\\nComputing and Algorithms in Industrial Simulations ; Griebel, M.,\\nSchüller, A., Schweitzer, M. A., Eds.; Springer: Berlin, 2017; pp 25−\\n42.42.\\n( 4 5 6 )T s u z u k i ,H . ;B r a n i c i o ,P .S . ;R i n o ,J .P .S t r u c t u r a l\\nCharacterization of Deformed Crystals by Analysis of Common\\nAtomic Neighborhood.Comput. Phys. Commun.2007, 177, 518−523.(457) C ̧aylak, O.; Anatole von Lilienfeld, O.; Baumeier, B.\\nWasserstein Metric for Improved Quantum Machine Learning With\\nAdjacency Matrix Representations.Mach. Learn.: Sci. Technol.2020, 1,\\n03LT01.03LT01.\\n(458) Gastegger, M.; Schwiedrzik, L.; Bittermann, M.; Berzsenyi, F.;\\nMarquetand, P. WACSF - Weighted Atom-Centered Symmetry\\nFunctions as Descriptors in Machine Learning Potentials. J. Chem.\\nPhys. 2018, 148, 241709.Phys. 2018, 148, 241709.\\n(459) De, S.; Bartók, A. P.; Csányi, G.; Ceriotti, M. Comparing\\nMolecules and Solids Across Structural and Alchemical Space.Phys.\\nChem. Chem. Phys.2016, 18, 13754−13769.Chem. Chem. Phys.2016, 18, 13754−13769.\\n(460) Pronobis, W.; Tkatchenko, A.; Mu ̈ller, K.-R. Many-Body\\nDescriptors for Predicting Molecular Properties With Machine\\nLearning: Analysis of Pairwise and Three-Body Interactions inMolecules. J. Chem. Theory Comput.2018, 14, 2991−3003.\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9865(461) Zhang, L.; Han, J.; Wang, H.; Car, R.; Weinan, E. Deep\\nPotential Molecular Dynamics: A Scalable Model With the Accuracy\\nof Quantum Mechanics.Phys. Rev. Lett.2018, 120, 143001.(462) Zhang, L.; Han, J.; Wang, H.; Saidi, W.; Car, R.; E, W. End-to-\\nEnd Symmetry Preserving Inter-Atomic Potential Energy Model for\\nFinite and Extended Systems.Adv. Neural Inf. Process. Syst.2018, 31,\\n4436−4446.4436−4446.\\n(463) Anderson, B.; Hy, T. S.; Kondor, R. Cormorant: Covariant\\nMolecular Neural Networks.Adv. Neural Inf. Process. Syst.2019, 32,\\n14537−14546.\\n(464) Thomas, N.; Smidt, T.; Kearnes, S.; Yang, L.; Li, L.; Kohlhoﬀ,K.; Riley, P. Tensor Field Networks: Rotation-and Translation-\\nEquivariant Neural Networks for 3d Point Clouds. arXiv, 2018,\\n1802.08219. https://arxiv.org/abs/1802.08219.\\n(465) Imbalzano, G.; Anelli, A.; Giofré, D.; Klees, S.; Behler, J.;Ceriotti, M. Automatic Select ion of Atomic Fingerprints and\\nReference Configurations for Machine-Learning Potentials.J. Chem.\\nPhys. 2018, 148, 241730.\\n(466) Schlömer, T.; Heck, D.; Deussen, O. Farthest-PointOptimized Point Sets with Maximized Minimum Distance.Proceed-\\nings of the ACM SIGGRAPH Symposium on High Performance\\nGraphics; New York, NY, USA, 2011; p 135−142.\\n(467) Mahoney, M. W.; Drineas, P. CUR Matrix Decompositionsfor Improved Data Analysis.Proc. Natl. Acad. Sci. U. S. A.2009, 106,\\n697−702.\\n(468) Pozdnyakov, S. N.; Willatt, M. J.; Bartók, A. P.; Ortner, C.;\\nCsányi, G.; Ceriotti, M. Incompleteness of Atomic StructureRepresentations.Phys. Rev. Lett.2020, 125, 166001.\\n(469) von Lilienfeld, O. A.; Ramakrishnan, R.; Rupp, M.; Knoll, A.\\nFourier Series of Atomic Radial Distribution Functions: A Molecular\\nFingerprint for Machine Learning Models of Quantum ChemicalProperties. Int. J. Quantum Chem.2015, 115, 1084−1093.\\n(470) Bartok, A. P.; De, S.; Poelking, C.; Bernstein, N.; Kermode, J.;\\nCsanyi, G.; Ceriotti, M. Machine Learning Unifies the Modelling of\\nMaterials and Molecules.Sci. Adv. 2017, 3, No. e1701816.(471) Monserrat, B.; Brandenburg, J. G.; Engel, E. A.; Cheng, B.\\nExtracting Ice Phases From Liquid Water: Why a Machine-Learning\\nWater Model Generalizes So Well.arXiv, 2020, 2006.13316,https://\\narxiv.org/abs/2006.13316.arxiv.org/abs/2006.13316.\\n(472) Deringer, V. L.; Csányi, G. Machine Learning Based\\nInteratomic Potential for Amorphous Carbon.Phys. Rev. B: Condens.\\nMatter Mater. Phys.2017, 95, 094203.\\n(473) Rowe, P.; Deringer, V. L.; Gasparotto, P.; Csányi, G.;Michaelides, A. An Accurate and Transferable Machine Learning\\nPotential for Carbon.J. Chem. Phys.2020, 153, 034702.\\n(474) Yue, S.; Muniz, M. C.; Calegari Andrade, M. F.; Zhang, L.;\\nCar, R.; Panagiotopoulos, A. Z. When Do Short-Range AtomisticMachine-Learning Models Fall Short? J. Chem. Phys. 2021, 154,\\n034111.\\n(475) Ko, T. W.; Finkler, J. A.; Goedecker, S.; Behler, J. General-\\nPurpose Machine Learning Potentials Capturing Nonlocal Charge\\nTransfer. Acc. Chem. Res.2021, 54, 808−817.Transfer. Acc. Chem. Res.2021, 54, 808−817.\\n(476) Ko, T. W.; Finkler, J. A.; Goedecker, S.; Behler, J. A Fourth-\\nGeneration High-Dimensional Neural Network Potential With\\nAccurate Electrostatics Including Non-Local Charge Transfer. Nat.Commun. 2021, 12, 398.\\n(477) Bereau, T.; Andrienko, D.; von Lilienfeld, O. A. Transferable\\nAtomic Multipole Machine Learning Models for Small Organic\\nMolecules. J. Chem. Theory Comput.2015, 11, 3225−3233.(478) Ghasemi, S. A.; Hofstetter, A.; Saha, S.; Goedecker, S.\\nInteratomic Potentials for Ionic Systems With Density Functional\\nAccuracy Based on Charge Densities Obtained by a Neural Network.Phys. Rev. B: Condens. Matter Mater. Phys.2015, 92, 045131.\\n(479) Grisafi, A.; Ceriotti, M. Incorporating Long-Range Physics in\\nAtomic-Scale Machine Learning.J. Chem. Phys.2019, 151, 204105.(480) Grisafi, A.; Nigam, J.; Ceriotti, M. Multi-Scale Approach for\\nthe Prediction of Atomic Scale Properties.Chem. Sci.2021, 12, 2078−\\n2090.\\n(481) Glielmo, A.; Sollich, P.; De Vita, A. Accurate InteratomicForce Fields via Machine Learning With Covariant Kernels.Phys. Rev.2090.\\n(481) Glielmo, A.; Sollich, P.; De Vita, A. Accurate Interatomic\\nForce Fields via Machine Learning With Covariant Kernels.Phys. Rev.\\nB: Condens. Matter Mater. Phys.2017, 95, 214302.B: Condens. Matter Mater. Phys.2017, 95, 214302.\\n(482) Grisafi, A.; Wilkins, D. M.; Csányi, G.; Ceriotti, M. Symmetry-\\nAdapted Machine Learning for Tensorial Properties of Atomistic\\nSystems. Phys. Rev. Lett.2018, 120, 036002.Systems. Phys. Rev. Lett.2018, 120, 036002.\\n(483) Willatt, M. J.; Musil, F.; Ceriotti, M. Feature Optimization for\\nAtomistic Machine Learning Yields a Data-Driven Construction of the\\nPeriodic Table of the Elements.Phys. Chem. Chem. Phys. 2018, 20,29661−29668.\\n(484) Lubbers, N.; Smith, J. S.; Barros, K. Hierarchical Modeling of\\nMolecular Energies Using a Deep Neural Network. J. Chem. Phys.\\n2018, 148, 241715.\\n(485) Unke, O. T.; Meuwly, M. PhysNet: A Neural Network forPredicting Energies, Forces, Dipole Moments, and Partial Charges.J.\\nChem. Theory Comput.2019, 15, 3678−3693.\\n(486) Jørgensen, P. B.; Jacobsen, K. W.; Schmidt, M. N. Neural\\nMessage Passing With Edge Updates for Predicting Properties ofMolecules and Materials.arXiv, 2018, 1806.03146.https://arxiv.org/\\nabs/1806.03146.\\n(487) Klicpera, J.; Groß, J.; Günnemann, S. Directional Message\\nPassing for Molecular Graphs. International Conference on Learning\\nRepresentations; 2020.Representations; 2020.\\n(488) Shao, Y.; Hellstrom, M.; Mitev, P. D.; Knijff, L.; Zhang, C.\\nPiNN: A Python Library for Building Atomic Neural Networks of\\nMolecules and Materials.J. Chem. Inf. Model.2020, 60, 1184−1193.(489) Tropsha, A. Best Practices for QSAR Model Development,\\nValidation, and Exploitation.Mol. Inf. 2010, 29, 476−488.\\n(490) Ma, J.; Sheridan, R. P.; Liaw, A.; Dahl, G. E.; Svetnik, V. Deep\\nNeural Nets as a Method for Quantitative Structure-ActivityRelationships. J. Chem. Inf. Model.2015, 55, 263−274.\\n(491) Grisafi, A.; Fabrizio, A .; Meyer, B.; Wilkins, D. M.;\\nCorminboeuf, C.; Ceriotti, M. Transferable Machine-Learning\\nModel of the Electron Density.ACS Cent. Sci.2019, 5,5 7−64.(492) Wilkins, D. M.; Grisafi, A.; Yang, Y.; Lao, K. U.; DiStasio Jr, R.\\nA.; Ceriotti, M. Accurate Molecular Polarizabilities With Coupled\\nCluster Theory and Machine Learning.Proc. Natl. Acad. Sci. U. S. A.\\n2019, 116, 3401−3406.2019, 116, 3401−3406.\\n(493) Ramakrishnan, R.; Dral, P. O.; Rupp, M.; von Lilienfeld, O. A.\\nBig Data Meets Quantum Chemistry Approximations: The Δ-\\nMachine Learning Approach. J. Chem. Theory Comput. 2015, 11,\\n2087−2096.2087−2096.\\n(494) Bartók, A. P.; Gillan, M. J.; Manby, F. R.; Csányi, G. Machine-\\nLearning Approach for One- And Two-Body Corrections to Density\\nFunctional Theory: Applications to Molecular and Condensed Water.Phys. Rev. B: Condens. Matter Mater. Phys.2013, 88, 054104.\\n(495) Cheng, L.; Welborn, M.; Christensen, A. S.; Miller, T. F. A\\nUniversal Density Matrix Functional From Molecular Orbital-BasedMachine Learning: Transferability Across Organic Molecules.J. Chem.\\nPhys. 2019, 150, 131103.\\n(496) Faber, F. A.; Hutchison, L.; Huang, B.; Gilmer, J.; Schoenholz,\\nS. S.; Dahl, G. E.; Vinyals, O.; Kearnes, S.; Riley, P. F.; von Lilienfeld,O. A. Prediction Errors of Molecular Machine Learning Models\\nLower Than Hybrid DFT Error.J. Chem. Theory Comput.2017, 13,\\n5255−5264.\\n(497) Hollingsworth, J.; Baker, T. E.; Burke, K. Can ExactConditions Improve Machine-L earned Density Functionals? J.\\nChem. Phys. 2018, 148, 241743.\\n(498) Li, L.; Snyder, J. C.; Pelaschier, I. M.; Huang, J.; Niranjan, U.\\nN.; Duncan, P.; Rupp, M.; Müller, K. R.; Burke, K. UnderstandingMachine-Learned Density Functionals. Int. J. Quantum Chem.2016,\\n116, 819−833.\\n(499) Vu, K.; Snyder, J. C.; Li, L.; Rupp, M.; Chen, B. F.; Khelif, T.;\\nMüller, K. R.; Burke, K. Understanding Kernel Ridge Regression:Common Behaviors From Simple Functions to Density Functionals.\\nInt. J. Quantum Chem.2015, 115, 1115−1128.\\n(500) Nagai, R.; Akashi, R.; Sasaki, S.; Tsuneyuki, S. Neural-\\nNetwork Kohn-Sham Exchange-Correlation Potential and Its Out-of-Training Transferability.J. Chem. Phys.2018, 148, 241737.\\nChemical Reviews pubs.acs.org/CR ReviewNetwork Kohn-Sham Exchange-Correlation Potential and Its Out-of-\\nTraining Transferability.J. Chem. Phys.2018, 148, 241737.\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9866(501) Carleo, G.; Troyer, M. Solving the Quantum Many-Body\\nProblem With Artificial Neural Networks.Science 2017, 355, 602−\\n606.\\n(502) Manzhos, S.; Carrington, T. An Improved Neural NetworkMethod for Solving the Schrödinger Equation.Can. J. Chem.2009, 87,\\n864−871.\\n(503) Huang, B.; von Lilienfeld, O. A. Quantum Machine Learning\\nUsing Atom-in-Molecule-Based Fragments Selected on the Fly.Nat.\\nChem. 2020, 12, 945−951.Chem. 2020, 12, 945−951.\\n(504) Li, Z.; Kermode, J. R.; De Vita, A. Molecular Dynamics With\\non-the-Fly Machine Learning of Quantum-Mechanical Forces.Phys.\\nRev. Lett. 2015, 114, 096405.\\n(505) Podryabinkin, E. V.; Shapeev, A. V. Active Learning ofLinearly Parametrized Interatomic Potentials. Comput. Mater. Sci.\\n2017, 140, 171−180.\\n(506) Quantum-machine.org. http://quantum-machine.org/datasets/\\n.\\n(507) de Pablo, J. J.; Jackson, N. E.; Webb, M. A.; Chen, L.-Q.;Moore, J. E.; Morgan, D.; Jacobs, R.; Pollock, T.; Schlom, D. G.;\\nToberer, E. S.; et al. New Frontiers for the Materials Genome\\nInitiative. Npj Comput. Mater.2019, 5, 41.\\n(508) The Materials Project. https://materialsproject.org/.(509) The NOMAD Laboratory. https://nomad-repository.eu/.\\n(510) Calderon, C. E.; Plata, J. J.; Toher, C.; Oses, C.; Levy, O.;\\nFornari, M.; Natan, A.; Mehl, M. J.; Hart, G.; Buongiorno Nardelli,M.; et al. The AFLOW Standard for High-Throughput Materials\\nScience Calculations. Comput. Mater. Sci.2015, 108, 233−238.\\n(511) Smith, J. S.; Isayev, O.; Roitberg, A. E. ANI-1: An Extensible\\nNeural Network Potential With DFT Accuracy at Force FieldComputational Cost.Chem. Sci. 2017, 8, 3192−3203.\\n(512) Smith, J. S.; Isayev, O.; Roitberg, A. E. Data Descriptor: ANI-\\n1, a Data Set of 20 Million Calculated Off-Equilibrium Conformations\\nfor Organic Molecules.Sci. Data 2017, 4, 170193.for Organic Molecules.Sci. Data 2017, 4, 170193.\\n(513) Smith, J. S.; Zubatyuk, R.; Nebgen, B.; Lubbers, N.; Barros,\\nK.; Roitberg, A. E.; Isayev, O.; Tretiak, S. The ANI-1ccx and ANI-1xData Sets, Coupled-Cluster and Density Functional Theory Properties\\nfor Molecules. Sci. Data 2020, 7, 134.\\n(514) Liu, T.; Lin, Y.; Wen, X.; Jorissen, R. N.; Gilson, M. K.\\nBindingDB: A Web-Accessible Database of Experimentally Deter-mined Protein-Ligand Binding Affinities.Nucleic Acids Res.2007, 35,\\nD198−D201.\\n(515) Hachmann, J.; Olivares-Amaya, R.; Atahan-Evrenk, S.;\\nAmador-Bedolla, C.; Sánchez-Carrera, R. S.; Gold-Parker, A.; Vogt,L.; Brockway, A. M.; Aspuru-Guzik, A. The Harvard Clean Energy\\nProject: Large-Scale Computational Screening and Design of Organic\\nPhotovoltaics on the World Community Grid. J. Phys. Chem. Lett.\\n2011, 2, 2241−2251.2011, 2, 2241−2251.\\n(516) Chung, Y. G.; Camp, J.; Haranczyk, M.; Sikora, B. J.; Bury, W.;\\nKrungleviciute, V.; Yildirim, T.; Farha, O. K.; Sholl, D. S.; Snurr, R. Q.\\nComputation-Ready, Experimental Metal-Organic Frameworks: ATool to Enable High-Throughput Screening of Nanoporous Crystals.\\nChem. Mater.2014, 26, 6185−6192.\\n(517) Mobley, D. L.; Guthrie, J. P. FreeSolv: A Database of\\nExperimental and Calculated Hydration Free Energies, With InputFiles. J. Comput.-Aided Mol. Des.2014, 28, 711−720.\\n(518) Fink, T.; Reymond, J.-L. Virtual Exploration of the Chemical\\nUniverse Up to 11 Atoms of C, N, O, F: Assembly of 26.4 Million\\nStructures (110.9 Million Stereoisomers) and Analysis for New RingSystems, Stereochemistry, Physicochemical Properties, Compound\\nClasses, and Drug Discovery.J. Chem. Inf. Model.2007, 47, 342−353.\\n(519) Earl, D. J.; Deem, M. W. Toward a Database of HypotheticalZeolite Structures. Ind. Eng. Chem. Res.2006, 45, 5449−5454.\\n(520) Jain, A.; Ong, S. P.; Hautier, G.; Chen, W.; Richards, W. D.;\\nDacek, S.; Cholia, S.; Gunter, D.; Skinner, D.; Ceder, G.; et al.Commentary: The Materials Project: A Materials Genome Approach\\nto Accelerating Materials Innovation.APL Mater. 2013, 1, 011002.\\n(521) Wu, Z.; Ramsundar, B.; Feinberg, E. N.; Gomes, J.; Geniesse,C.; Pappu, A. S.; Leswing, K.; Pande, V. MoleculeNet: A Benchmark\\nfor Molecular Machine Learning.Chem. Sci. 2018, 9, 513−530.(521) Wu, Z.; Ramsundar, B.; Feinberg, E. N.; Gomes, J.; Geniesse,\\nC.; Pappu, A. S.; Leswing, K.; Pande, V. MoleculeNet: A Benchmark\\nfor Molecular Machine Learning.Chem. Sci. 2018, 9, 513−530.(522) Zitnick, C. L.; Chanussot, L.; Das, A.; Goyal, S.; Heras-\\nDomingo, J.; Ho, C.; Hu, W.; Lavril, T.; Palizhati, A.; Riviere, M.,\\net al. An Introduction to Electrocatalyst Design Using MachineLearning for Renewable Energy Storage.arXiv, 2020, 2010.09435.\\nhttps://arxiv.org/abs/2010.09435.\\n(523) Saal, J. E.; Kirklin, S.; Aykol, M.; Meredig, B.; Wolverton, C.\\nMaterials Design and Discovery With High-Throughput DensityFunctional Theory: The Open Quantum Materials Database\\n(OQMD).JOM 2013, 65, 1501−1509.\\n(524) Nakata, M.; Shimazaki, T.; Hashimoto, M.; Maeda, T.\\nPubChemQC PM6: Data Sets of 221 Million Molecules WithOptimized Molecular Geometries and Electronic Properties.J. Chem.\\nInf. Model. 2020, 60, 5891−5899.\\n(525) Nakata, M.; Shimazaki, T. PubChemQC Project: A Large-\\nScale First-Principles Electronic Structure Database for Data-DrivenChemistry.J. Chem. Inf. Model.2017, 57, 1300−1308.\\n(526) Hoja, J.; Medrano Sandonas, L.; Ernst, B. G.; Vazquez-\\nMayagoitia, A.; DiStasio Jr, R. A.; Tkatchenko, A. QM7-X: A\\nComprehensive Dataset of Quantum-Mechanical Properties Spanningthe Chemical Space of Small Organic Molecules.Sci. Data 2021, 8,\\n43.\\n(527) Ramakrishnan, R.; Dral, P. O.; Rupp, M.; von Lilienfeld, O. A.\\nQuantum Chemistry Structures and Properties of 134 Kilo Molecules.\\nSci. Data2014, 1, 140022.Sci. Data2014, 1, 140022.\\n(528) Kim, E.; Huang, K.; Tomala, A.; Matthews, S.; Strubell, E.;\\nSaunders, A.; McCallum, A.; Olivetti, E. Machine-Learned and\\nCodified Synthesis Parameters of Oxide Materials.Sci. Data 2017, 4,\\n170127.170127.\\n(529) Tetko, I. V.; Maran, U.; Tropsha, A. Public (Q)SAR Services,\\nIntegrated Modeling Environments, and Model Repositories on the\\nWeb: State of the Art and Perspectives for Future Development.Mol.\\nInf. 2017, 36, 1600082.Inf. 2017, 36, 1600082.\\n(530) Montavon, G.; Rupp, M.; Gobre, V.; Vazquez-Mayagoitia, A.;\\nHansen, K.; Tkatchenko, A.; Müller, K.-R.; Anatole von Lilienfeld, O.\\nMachine Learning of Molecular Electronic Properties in ChemicalCompound Space.New J. Phys.2013, 15, 095003.\\n(531) Isayev, O.; Fourches, D.; Muratov, E. N.; Oses, C.; Rasch, K.;\\nTropsha, A.; Curtarolo, S. Materials Cartography: Representing and\\nMining Materials Space Using Structural and Electronic Fingerprints.Chem. Mater.2015, 27, 735−743.\\n(532) Ceriotti, M. Unsupervised Machine Learning in Atomistic\\nSimulations, Between Predictions and Understanding.J. Chem. Phys.\\n2019, 150, 150901.\\n(533) Montavon, G.; Hansen, K.; Fazli, S.; Rupp, M.; Biegler, F.;Ziehe, A.; Tkatchenko, A.; Lillienfeld, A.; Muller, K.-R. Learning\\ninvariant representations of mo lecules for atomization energy\\nprediction. NeurIPS 2012, 25, 440−448.\\n(534) Fraux, G.; Cersonsky, R.; Ceriotti, M. Chemiscope: InteractiveStructure-Property Explorer for Materials and Molecules. J. Open\\nSource Softw. 2020, 5, 2117.\\n(535) Qiang, Y.; Xindong, W. 10 Challenging Problems in Data\\nMining Research. Int. J. Inf. Technol. Decis. Mak.2006, 5, 597−604.(536) Wu, X.; Kumar, V.; Ross Quinlan, J.; Ghosh, J.; Yang, Q.;\\nMotoda, H.; McLachlan, G. J.; Ng, A.; Liu, B.; Yu, P. S.; et al. Top 10\\nAlgorithms in Data Mining.Knowl. Inf. Syst.2008, 14,1 −37.(537) Li, H.; Zhang, Z.; Zhao, Z.-Z. Data-Mining for Processes in\\nChemistry, Materials, and Engineering.Processes 2019, 7, 151.\\n(538) Tshitoyan, V.; Dagdelen, J.; Weston, L.; Dunn, A.; Rong, Z.;Kononova, O.; Persson, K. A.; Ceder, G.; Jain, A. Unsupervised Word\\nEmbeddings Capture Latent Knowledge From Materials Science\\nLiterature.Nature 2019, 571,9 5−98.\\n(539) Lo, Y. C.; Rensi, S. E.; Torng, W.; Altman, R. B. MachineLearning in Chemoinformatics and Drug Discovery.Drug Discovery\\nToday 2018, 23, 1538−1546.\\n(540) Kim, E.; Huang, K.; Saunders, A.; McCallum, A.; Ceder, G.;\\nOlivetti, E. Materials Synthesis Insights From Scientific Literature viaText Extraction and Machine Learning.Chem. Mater. 2017, 29,(540) Kim, E.; Huang, K.; Saunders, A.; McCallum, A.; Ceder, G.;\\nOlivetti, E. Materials Synthesis Insights From Scientific Literature via\\nText Extraction and Machine Learning.Chem. Mater. 2017, 29,\\n9436−9444.\\nChemical Reviews pubs.acs.org/CR ReviewChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9867(541) Kowalski, B. R. Chemometrics: Views and Propositions. J.\\nChem. Inf. Comput. Sci.1975, 15, 201−203.\\n(542) Swain, M. C.; Cole, J. M. ChemDataExtractor: A Toolkit for\\nAutomated Extraction of Chemical Information From the ScientificLiterature.J. Chem. Inf. Model.2016, 56, 1894−1904.\\n(543) Sparks, T. D.; Gaultois, M. W.; Oliynyk, A.; Brgoch, J.;\\nMeredig, B. Data Mining Our Way to the Next Generation of\\nThermoelectrics. Scr. Mater. 2016, 111,1 0−15.Thermoelectrics. Scr. Mater. 2016, 111,1 0−15.\\n(544) Medford, A. J.; Kunz, M. R.; Ewing, S. M.; Borders, T.;\\nFushimi, R. Extracting Knowledge From Data Through Catalysis\\nInformatics. ACS Catal. 2018, 8, 7403−7429.Informatics. ACS Catal. 2018, 8, 7403−7429.\\n(545) Raccuglia, P.; Elbert, K. C.; Adler, P. D.; Falk, C.; Wenny, M.\\nB.; Mollo, A.; Zeller, M.; Friedler, S. A.; Schrier, J.; Norquist, A. J.Machine-Learning-Assisted Materials Discovery Using Failed Experi-\\nments. Nature 2016, 533,7 3−76.\\n(546) Xie, S.; Stewart, G.; Hamlin, J.; Hirschfeld, P.; Hennig, R.\\nFunctional Form of the Superconducting Critical Temperature FromMachine Learning. Phys. Rev. B: Condens. Matter Mater. Phys.2019,\\n100, 174513.\\n(547) Staker, J.; Marshall, K.; Abel, R.; McQuaw, C. M. Molecular\\nStructure Extraction From Documents Using Deep Learning.J. Chem.\\nInf. Model. 2019, 59, 1017−1029.Inf. Model. 2019, 59, 1017−1029.\\n(548) Timoshenko, J.; Lu, D.; Lin, Y.; Frenkel, A. I. Supervised\\nMachine-Learning-Based Determination of Three-Dimensional Struc-\\nture of Metallic Nanoparticles. J. Phys. Chem. Lett. 2017, 8, 5091−\\n5098.5098.\\n(549) Ziatdinov, M.; Maksov, A.; Kalinin, S. V. Learning Surface\\nMolecular Structures via Machine Vision.Npj Comput. Mater.2017, 3,\\n31.\\n(550) Zhou, X. X.; Zeng, W. F.; Chi, H.; Luo, C.; Liu, C.; Zhan, J.;He, S. M.; Zhang, Z. PDeep: Predicting MS/MS Spectra of Peptides\\nWith Deep Learning.Anal. Chem. 2017, 89, 12690−12697.\\n(551) Car, R.; Parrinello, M. Unified Approach for Molecular\\nDynamics and Density-Functional Theory.Phys. Rev. Lett.1985, 55,2471−2474.\\n(552) Butler, K. T.; Davies, D. W.; Cartwright, H.; Isayev, O.; Walsh,\\nA. Machine Learning for Molecular and Materials Science. Nature\\n2018, 559, 547−555.\\n(553) Deringer, V. L.; Caro, M. A.; Csányi, G. Machine LearningInteratomic Potentials as Emerging Tools for Materials Science.Adv.\\nMater. 2019, 31, 1902765.\\n(554) Unke, O. T.; Chmiela, S.; Sauceda, H. E.; Gastegger, M.;\\nPoltavsky, I.; Schütt, K. T.; Tkatchenko, A.; Müller, K.-R. MachineLearning Force Fields. Chem. Rev. 2021, DOI: 10.1021/acs.chem-\\nrev.0c01111.\\n(555) Dral, P. O.; Owens, A.; Yurchenko, S. N.; Thiel, W. Structure-\\nBased Sampling and Self-Correcting Machine Learning for AccurateCalculations of Potential Energy Surfaces and Vibrational Levels.J.\\nChem. Phys. 2017, 146, 244108.\\n(556) Handley, C. M.; Popelier, P. L. Potential Energy Surfaces\\nFitted by Artificial Neural Networks. J. Phys. Chem. A 2010, 114,\\n3371−3383.3371−3383.\\n(557) Jiang, B.; Li, J.; Guo, H. Potential Energy Surfaces From High\\nFidelity Fitting of Ab Initio Points: The Permutation Invariant\\nPolynomial - Neural Network Approach.Int. Rev. Phys. Chem.2016,\\n35, 479−506.35, 479−506.\\n(558) Kolb, B.; Zhao, B.; Li, J.; Jiang, B.; Guo, H. Permutation\\nInvariant Potential Energy Surfaces for Polyatomic Reactions Using\\nAtomistic Neural Networks.J. Chem. Phys.2016, 144, 224103.(559) Shao, K.; Chen, J.; Zhao, Z.; Zhang, D. H. Communication:\\nFitting Potential Energy Surfaces With Fundamental Invariant Neural\\nNetwork. J. Chem. Phys.2016, 145, 071101.\\n(560) Li, J.; Song, K.; Behler, J. A Critical Comparison of NeuralNetwork Potentials for Molecular Reaction Dynamics With Exact\\nPermutation Symmetry. Phys. Chem. Chem. Phys. 2019, 21, 9672−\\n9682.\\n(561) Fu, B.; Zhang, D. H. Ab Initio Potential Energy Surfaces andQuantum Dynamics for Polyatomic Bimolecular Reactions.J. Chem.\\nTheory Comput. 2018, 14, 2289−2303.\\n(562) Ballard, A. J.; Das, R.; Martiniani, S.; Mehta, D.; Sagun, L.;\\nStevenson, J. D.; Wales, D. J. Energy Landscapes for MachineLearning. Phys. Chem. Chem. Phys.2017, 19, 12585−12603.\\n(563) Chen, J.; Xu, X.; Xu, X.; Zhang, D. H. A Global PotentialStevenson, J. D.; Wales, D. J. Energy Landscapes for Machine\\nLearning. Phys. Chem. Chem. Phys.2017, 19, 12585−12603.\\n(563) Chen, J.; Xu, X.; Xu, X.; Zhang, D. H. A Global Potential\\nEnergy Surface for the H2 +O H↔ H2O + H Reaction Using NeuralNetworks. J. Chem. Phys.2013, 138, 154301.\\n(564) Brown, D. F.; Gibbs, M. N.; Clary, D. C. Combining Ab Initio\\nComputations, Neural Networks, and Diffusion Monte Carlo: An\\nEfficient Method to Treat Weakly Bound Molecules.J. Chem. Phys.1996, 105, 7597−7604.\\n(565) Bernstein, N.; Csányi, G.; Deringer, V. L. De Novo\\nExploration and Self-Guided Learning of Potential-Energy Surfaces.\\nNpj Comput. Mater.2019, 5, 99.\\n(566) Chmiela, S.; Sauceda, H. E.; Poltavsky, I.; Mu ̈ller, K.-R.;Tkatchenko, A. sGDML: Constructing Accurate and Data Efficient\\nMolecular Force Fields Using Machine Learning. Comput. Phys.\\nCommun. 2019, 240,3 8−45.\\n(567) Pártay, L. B.; Bartók, A. P.; Csányi, G. Efficient Sampling ofAtomic Configurational Spaces.J. Phys. Chem. B2010, 114, 10502−\\n10512.\\n(568) Thompson, A.; Swiler, L.; Trott, C.; Foiles, S.; Tucker, G.\\nSpectral Neighbor Analysis Method for Automated Generation ofQuantum-Accurate Interatomic Potentials. J. Comput. Phys. 2015,\\n285, 316−330.\\n(569) Zuo, Y.; Chen, C.; Li, X.; Deng, Z.; Chen, Y.; Behler, J.;\\nCśanyi, G.; Shapeev, A. V.; Thompson, A. P.; Wood, M. A.; et al.Performance and Cost Assessment of Machine Learning Interatomic\\nPotentials. J. Phys. Chem. A2020, 124, 731−745.\\n(570) Behler, J. Representing Potential Energy Surfaces by High-\\nDimensional Neural Network Potentials. J. Phys.: Condens. Matter2014, 26, 183001.\\n(571) Schütt, K. T.; Kessel, P.; Gastegger, M.; Nicoli, K. A.;\\nTkatchenko, A.; Müller, K. R. SchNetPack: A Deep Learning Toolbox\\nfor Atomistic Systems.J. Chem. Theory Comput.2019, 15, 448−455.(572) Han, J.; Zhang, L.; Car, R.; E, W. Deep Potential: A General\\nRepresentation of a Many-Body Potential Energy Surface.Commun.\\nComput. Phys. 2018, 23, 629−639.\\n(573) Torrie, G. M.; Valleau, J. P. Nonphysical SamplingDistributions in Monte Carlo Free-Energy Estimation: Umbrella\\nSampling. J. Comput. Phys.1977, 23, 187−199.\\n(574) Laio, A.; Parrinello, M. Escaping Free-Energy Minima.Proc.\\nNatl. Acad. Sci. U. S. A.2002, 99, 12562−12566.Natl. Acad. Sci. U. S. A.2002, 99, 12562−12566.\\n(575) Bolhuis, P. G.; Chandler, D.; Dellago, C.; Geissler, P. L.\\nTransition Path Sampling: Throwing Ropes Over Rough Mountain\\nPasses, in the Dark.Annu. Rev. Phys. Chem.2002, 53, 291−318.(576) Morawietz, T.; Singraber, A.; Dellago, C.; Behler, J. How Van\\nDer Waals Interactions Determine the Unique Properties of Water.\\nProc. Natl. Acad. Sci. U. S. A.2016, 113, 8368−8373.\\n(577) Tuckerman, M. Statistical Mechanics: Theory and MolecularSimulation; Oxford University Press: New York, NY, 2010.\\n(578) Cheng, B.; Engel, E. A.; Behler, J.; Dellago, C.; Ceriotti, M. Ab\\nInitio Thermodynamics of Liquid and Solid Water.Proc. Natl. Acad.\\nSci. U. S. A.2019, 116, 1110−1115.Sci. U. S. A.2019, 116, 1110−1115.\\n(579) Reinhardt, A.; Cheng, B. Quantum-Mechanical Exploration of\\nthe Phase Diagram of Water.Nat. Commun. 2021, 12, 588.\\n(580) Niu, H.; Bonati, L.; Piaggi, P. M.; Parrinello, M. Ab InitioPhase Diagram And Nucleation of Gallium.Nat. Commun. 2020, 11,\\n2654.\\n(581) Cheng, B.; Mazzola, G.; Pickard, C. J.; Ceriotti, M. Evidence\\nfor Supercritical Behaviour of High-Pressure Liquid Hydrogen.Nature\\n2020, 585, 217−220.2020, 585, 217−220.\\n(582) Cheng, B.; Behler, J.; Ceriotti, M. Nuclear Quantum Effects in\\nWater at the Triple Point: Using Theory as a Link Between\\nExperiments.J. Phys. Chem. Lett.2016, 7, 2210−2215.(583) Morawietz, T.; Marsalek, O.; Pattenaude, S. R.; Streacker, L.\\nM.; Ben-Amotz, D.; Markland, T. E. The Interplay of Structure and\\nDynamics in the Raman Spectrum of Liquid Water Over the FullFrequency and Temperature Range.J. Phys. Chem. Lett.2018, 9, 851−\\n857.\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9868(584) Ko, H.-Y.; Zhang, L.; Santra, B.; Wang, H.; E, W.; DiStasio, R.\\nA., Jr.; Car, R. Isotope Effects in Liquid Water via Deep Potential\\nMolecular Dynamics.Mol. Phys. 2019, 117, 3269−3281.(585) Sauceda, H. E.; Chmiela, S.; Poltavsky, I.; Mu ̈ller, K.-R.;\\nTkatchenko, A. Molecular Force Fields With Gradient-Domain\\nMachine Learning: Construction and Application to Dynamics ofSmall Molecules With Coupled Cluster Forces.J. Chem. Phys.2019,\\n150, 114102.\\n(586) Bisbo, M. K.; Hammer, B. Efficient Global Structure\\nOptimization With a Machine-Learned Surrogate Model.Phys. Rev.\\nLett. 2020, 124, 086102.Lett. 2020, 124, 086102.\\n(587) Meldgaard, S. A.; Kolsbjerg, E. L.; Hammer, B. Machine\\nLearning Enhanced Global Optimization by Clustering Local\\nEnvironments to Enable Bundled Atomic Energies. J. Chem. Phys.\\n2018, 149, 134104.2018, 149, 134104.\\n(588) Deringer, V. L.; Pickard, C. J.; Proserpio, D. M. Hierarchically\\nStructured Allotropes of Phosphorus From Data-Driven Exploration.\\nAngew. Chem., Int. Ed.2020, 59, 15880.Angew. Chem., Int. Ed.2020, 59, 15880.\\n(589) Chiavazzo, E.; Covino, R.; Coifman, R. R.; Gear, C. W.;\\nGeorgiou, A. S.; Hummer, G.; Kevrekidis, I. G. Intrinsic Map\\nDynamics Exploration for Uncharted Effective Free-Energy Land-scapes.Proc. Natl. Acad. Sci. U. S. A.2017, 114, No. E5494.\\n(590) Rogal, J.; Schneider, E.; Tuckerman, M. E. Neural-Network-\\nBased Path Collective Variables for Enhanced Sampling of Phase\\nTransformations. Phys. Rev. Lett.2019, 123, 245701.(591) Bonati, L.; Rizzi, V.; Parrinello, M. Data-Driven Collective\\nVariables for Enhanced Sampling. J. Phys. Chem. Lett. 2020, 11,\\n2998−3004.\\n(592) Mones, L.; Bernstein, N.; Csányi, G. Exploration, Sampling,and Reconstruction of Free Energy Surfaces With Gaussian Process\\nRegression.J. Chem. Theory Comput.2016, 12, 5100−5110.\\n(593) Debnath, J.; Parrinello, M. Gaussian Mixture-Based EnhancedSampling for Statics and Dynamics. J. Phys. Chem. Lett. 2020, 11,\\n5076−5080.\\n(594) Schneider, E.; Dai, L.; Topper, R. Q.; Drechsel-Grau, C.;\\nTuckerman, M. E. Stochastic Neural Network Approach for LearningHigh-Dimensional Free Energy Surfaces.Phys. Rev. Lett. 2017, 119,\\n150601.\\n(595) Bonati, L.; Zhang, Y.-Y.; Parrinello, M. Neural Networks-\\nBased Variationally Enhanced Sampling.Proc. Natl. Acad. Sci. U. S. A.\\n2019, 116, 17641−17647.2019, 116, 17641−17647.\\n(596) Vanhaelen, Q.; Lin, Y.-C.; Zhavoronkov, A. The Advent of\\nGenerative Chemistry. ACS Med. Chem. Lett.2020, 11, 1496−1505.\\n(597) Sanchez-Lengeling, B.; Aspuru-Guzik, A. Inverse MolecularDesign Using Machine Learning: Generative Models for Matter\\nEngineering. Science 2018, 361, 360−365.\\n(598) Gupta, A.; Müller, A. T.; Huisman, B. J. H.; Fuchs, J. A.;\\nSchneider, P.; Schneider, G. Generative Recurrent Networks for DeNovo Drug Design.Mol. Inf. 2018, 37, 1700111.\\n(599) Segler, M. H. S.; Kogej, T.; Tyrchan, C.; Waller, M. P.\\nGenerating Focused Molecule Libraries for Drug Discovery With\\nRecurrent Neural Networks.ACS Cent. Sci.2018, 4, 120−131.(600) Merk, D.; Friedrich, L.; Grisoni, F.; Schneider, G. De Novo\\nDesign of Bioactive Small Molecules by Artificial Intelligence.Mol. Inf.\\n2018, 37, 1700153.\\n(601) Liu, Q.; Allamanis, M.; Brockschmidt, M.; Gaunt, A.Constrained Graph Variational Autoencoders for Molecule Design.\\nAdv. Neural Inf. Process. Syst.2018, 31, 7795−7804.\\n(602) Jin, W.; Barzilay, R.; Jaakkola, T. Junction Tree VariationalAutoencoder for Molecular Graph Generation.Proceedings of the 35th\\nInternational Conference on Machine Learning2018, 2323−2332.\\n(603) Dai, H.; Tian, Y.; Dai, B.; Skiena, S.; Song, L. Syntax-DirectedVariational Autoencoder for Structured Data. arXiv , 2018,\\n1802.08786. https://arxiv.org/abs/1802.08786.\\n(604) Kusner, M. J.; Paige, B.; Hernández-Lobato, J. M. Grammar\\nVariational Autoencoder. Proceedings of the 34th InternationalConference on Machine Learning2017, 1945−1954.\\n(605) Jørgensen, P. B.; Mesta, M.; Shil, S.; García Lastra, J. M.;\\nJacobsen, K. W.; Thygesen, K. S.; Schmidt, M. N. Machine Learning-\\nBased Screening of Complex Molecules for Polymer Solar Cells.J.Chem. Phys. 2018, 148, 241735.Jacobsen, K. W.; Thygesen, K. S.; Schmidt, M. N. Machine Learning-\\nBased Screening of Complex Molecules for Polymer Solar Cells.J.\\nChem. Phys. 2018, 148, 241735.\\n(606) Jin, W.; Yang, K.; Barzilay, R.; Jaakkola, T. LearningMultimodal Graph-to-Graph Translation for Molecule Optimization.\\narXiv, 2019, 1812.01070, ver. 3.https://arxiv.org/abs/1812.01070.\\n(607) Gómez-Bombarelli, R.; Wei, J. N.; Duvenaud, D.; Hernández-Lobato, J. M.; Sánchez-Lengeling, B.; Sheberla, D.; Aguilera-\\nIparraguirre, J.; Hirzel, T. D.; Adams, R. P.; Aspuru-Guzik, A.\\nAutomatic Chemical Design Using a Data-Driven Continuous\\nRepresentation of Molecules.ACS Cent. Sci.2018, 4, 268−276.(608) Polykovskiy, D.; Zhebrak, A.; Vetrov, D.; Ivanenkov, Y.;\\nAladinskiy, V.; Mamoshina, P.; Bozdaganyan, M.; Aliper, A.;\\nZhavoronkov, A.; Kadurin, A. Entangled Conditional Adversarial\\nAutoencoder for De Novo Drug Discovery.Mol. Pharmaceutics2018,15, 4398−4405.\\n(609) Blaschke, T.; Olivecrona, M.; Engkvist, O.; Bajorath, J.; Chen,\\nH. Application of Generative Autoencoder in De Novo Molecular\\nDesign.Mol. Inf. 2018, 37, 1700123.\\n(610) Kadurin, A.; Nikolenko, S.; Khrabrov, K.; Aliper, A.;Zhavoronkov, A. druGAN: An Advanced Generative Adversarial\\nAutoencoder Model for De Novo Generation of New Molecules With\\nDesired Molecular Properties in Silico.Mol. Pharmaceutics 2017, 14,\\n3098−3104.3098−3104.\\n(611) Yu, L.; Zhang, W.; Wang, J.; Yu, Y. SeqGAN: Sequence\\nGenerative Adversarial Nets with Policy Gradient.Proceedings of the\\nThirty-First AAAI Conference on Artificial Intelligence. 2017, 2852−\\n2858.2858.\\n(612) Guimaraes, G. L.; Sanchez-Lengeling, B.; Farias, P. L. C.;\\nAspuru-Guzik, A. Objective-Reinforced Generative Adversarial Net-\\nworks (ORGAN) for Sequence Generation Models. arXiv, 2017,\\n1705.10843. https://arxiv.org/abs/1705.10843.1705.10843. https://arxiv.org/abs/1705.10843.\\n(613) De Cao, N.; Kipf, T. MolGAN: An Implicit Generative Model\\nfor Small Molecular Graphs.arXiv, 2018, 1805.11973.https://arxiv.\\norg/abs/1805.11973.org/abs/1805.11973.\\n(614) Maziarka,Ł .; Pocha, A.; Kaczmarczyk, J.; Rataj, K.; Danel, T.;\\nWarchoł, M. Mol-CycleGAN: A Generative Model for Molecular\\nOptimization. J. Cheminf. 2020, 12,2 .Optimization. J. Cheminf. 2020, 12,2 .\\n(615) You, J.; Liu, B.; Ying, R.; Pande, V.; Leskovec, J. Graph\\nConvolutional Policy Network for Goal-Directed Molecular Graph\\nGeneration. Adv. Neural Inf. Process. Syst.2018, 31, 6410−6421.(616) Popova, M.; Isayev, O.; Tropsha, A. Deep Reinforcement\\nLearning for De Novo Drug Design.Sci. Adv.2018, 4, No. eaap7885.\\n(617) Zhavoronkov, A.; et al. Deep Learning Enables Rapid\\nIdentification of Potent DDR1 Kinase Inhibitors. Nat. Biotechnol.2019, 37, 1038−1040.\\n(618) Li, Y.; Zhang, L.; Liu, Z. Multi-Objective De Novo Drug\\nDesign With Conditional Graph Generative Model.J. Cheminf.2018,\\n10, 33.\\n(619) Li, Y.; Vinyals, O.; Dyer, C.; Pascanu, R.; Battaglia, P.Learning Deep Generative Models of Graphs. arXiv , 2018,\\n1803.03324. https://arxiv.org/abs/1803.03324.\\n(620) Mansimov, E.; Mahmood, O.; Kang, S.; Cho, K. Molecular\\nGeometry Prediction Using a Deep Generative Graph NeuralNetwork.Sci. Rep. 2019, 9, 20381.\\n(621) Gebauer, N. W. A.; Gastegger, M.; Schütt, K. T. Generating\\nEquilibrium Molecules With Deep Neural Networks. arXiv, 2018,\\n1810.11347. https://arxiv.org/abs/1810.11347.1810.11347. https://arxiv.org/abs/1810.11347.\\n(622) Gebauer, N.; Gastegger, M.; Schütt, K. Symmetry-Adapted\\nGeneration of 3d Point Sets for the Targeted Discovery of Molecules.\\nAdv. Neural Inf. Process. Syst.2019, 32, 7564−7576.(623) Caccin, M.; Li, Z.; Kermode, J. R.; De Vita, A. A Framework\\nfor Machine-Learning-Augmented Multiscale Atomistic Simulations\\non Parallel Supercomputers.Int. J. Quantum Chem.2015, 115, 1129−\\n1139.1139.\\n(624) Gkeka, P.; et al. Machine Learning Force Fields and Coarse-\\nGrained Variables in Molecular Dynamics: Application to Materials\\nand Biological Systems.J. Chem. Theory Comput. 2020, 16, 4757−\\n4775.\\nChemical Reviews pubs.acs.org/CR Review4775.\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9869(625) Saunders, M. G.; Voth, G. A. Coarse-Graining Methods for\\nComputational Biology. Annu. Rev. Biophys.2013, 42,7 3−93.\\n(626) John, S. T.; Csányi, G. Many-Body Coarse-Grained\\nInteractions Using Gaussian Approximation Potentials. J. Phys.Chem. B 2017, 121, 10934−10949.\\n(627) Zhang, L.; Han, J.; Wang, H.; Car, R.; E, W. DeePCG:\\nConstructing Coarse-Grained Models via Deep Neural Networks.J.\\nChem. Phys. 2018, 149, 034101.Chem. Phys. 2018, 149, 034101.\\n(628) Cesari, A.; Gil-Ley, A.; Bussi, G. Combining Simulations and\\nSolution Experiments as a Paradigm for RNA Force Field Refinement.\\nJ. Chem. Theory Comput.2016, 12, 6192−6200.J. Chem. Theory Comput.2016, 12, 6192−6200.\\n(629) Goh, G. B.; Hodas, N. O.; Vishnu, A. Deep Learning for\\nComputational Chemistry. J. Comput. Chem.2017, 38, 1291−1307.\\n(630) Goh, G. B.; Vishnu, A.; Siegel, C.; Hodas, N. Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for\\nTransferable Chemical Property Prediction. Proceedings of the ACM\\nSIGKDD International Conference on Knowledge Discovery and Data\\nMining. 2018, 302−310.Mining. 2018, 302−310.\\n(631) Curtarolo, S.; Hart, G. L.; Nardelli, M. B.; Mingo, N.; Sanvito,\\nS.; Levy, O. The High-Throughput Highway to Computational\\nMaterials Design. Nat. Mater. 2013, 12, 191−201.Materials Design. Nat. Mater. 2013, 12, 191−201.\\n(632) Gómez-Bombarelli, R.; Aguilera-Iparraguirre, J.; Hirzel, T. D.;\\nDuvenaud, D.; Maclaurin, D.; Blood-Forsythe, M. A.; Chae, H. S.;Einzinger, M.; Ha, D. G.; Wu, T.; et al. Design of Efficient Molecular\\nOrganic Light-Emitting Diodes by a High-Throughput Virtual\\nScreening and Experimental Approach. Nat. Mater. 2016, 15,\\n1120−1127.1120−1127.\\n(633) Kolb, B.; Lentz, L. C.; Kolpak, A. M. Discovering Charge\\nDensity Functionals and Structure-Property Relationships With\\nPROPhet: A General Framework for Coupling Machine Learning\\nand First-Principles Methods.Sci. Rep. 2017, 7, 1192.(634) von Lilienfeld, O. A.; Müller, K.-R.; Tkatchenko, A. Exploring\\nChemical Compound Space With Quantum-Based Machine Learning.\\nNat. Rev. Chem.2020, 4, 347−358.\\n(635) Kuhn, C.; Beratan, D. N. Inverse Strategies for MolecularDesign. J. Phys. Chem.1996, 100, 10595−10599.\\n(636) Isayev, O.; Oses, C.; Toher, C.; Gossett, E.; Curtarolo, S.;\\nTropsha, A. Universal Fragment Descriptors for Predicting Properties\\nof Inorganic Crystals.Nat. Commun. 2017, 8, 15679.(637) Zhang, L.; Mao, H.; Liu, Q.; Gani, R. Chemical Product\\nDesign - Recent Advances and Perspectives.Curr. Opin. Chem. Eng.\\n2020, 27,2 2−34.\\n(638) Park, C. W.; Wolverton, C. Developing an Improved CrystalGraph Convolutional Neural Network Framework for Accelerated\\nMaterials Discovery. Phys. Rev. Mater.2020, 4, 063801.\\n(639) Xie, T.; Grossman, J. C. Crystal Graph Convolutional Neural\\nNetworks for an Accurate and Interpretable Prediction of MaterialProperties. Phys. Rev. Lett.2018, 120, 145301.\\n(640) Dewar, M. J.; Storch, D. M. Comparative Tests of Theoretical\\nProcedures for Studying Chemical Reactions.J. Am. Chem. Soc.1985,\\n107, 3898−3902.107, 3898−3902.\\n(641) Nam, J.; Kim, J. Linking the Neural Machine Translation and\\nthe Prediction of Organic Chemistry Reactions. arXiv, 2016,\\n1612.09529. https://arxiv.org/abs/1612.09529.1612.09529. https://arxiv.org/abs/1612.09529.\\n(642) Segler, M. H.; Preuss, M.; Waller, M. P. Planning Chemical\\nSyntheses With Deep Neural Networks and Symbolic AI. Nature\\n2018, 555, 604−610.2018, 555, 604−610.\\n(643) Segler, M.; Preuss, M.; Waller, M. P. Towards“Alphachem”:\\nChemical Synthesis Planning With Tree Search and Deep Neural\\nNetwork Policies.5th International Conference on Learning Representa-tions, ICLR 2017Workshop Track Proceedings, 2019.\\n(644) Warr, W. A. A Short Review of Chemical Reaction Database\\nSystems, Computer-Aided Synthesis Design, Reaction Prediction and\\nSynthetic Feasibility. Mol. Inf. 2014, 33, 469−476.(645) Schwaller, P.; Laino, T.; Gaudin, T.; Bolgar, P.; Hunter, C. A.;\\nBekas, C.; Lee, A. A. Molecula rT r a n s f o r m e r :AM o d e lf o r\\nUncertainty-Calibrated Chemical Reaction Prediction. ACS Cent.\\nSci. 2019, 5, 1572−1583.Sci. 2019, 5, 1572−1583.\\n(646) Corey, E. J.; Wipke, W. T. Computer-Assisted Design of\\nComplex Organic Syntheses.Science 1969, 166, 178−192.Sci. 2019, 5, 1572−1583.\\n(646) Corey, E. J.; Wipke, W. T. Computer-Assisted Design of\\nComplex Organic Syntheses.Science 1969, 166, 178−192.\\n(647) Coley, C. W.; Green, W. H.; Jensen, K. F. Machine Learningin Computer-Aided Synthesis Planning. Acc. Chem. Res. 2018, 51,\\n1281−1289.\\n(648) Cook, A.; Johnson, A. P.; Law, J.; Mirzazadeh, M.; Ravitz, O.;\\nSimon, A. Computer-Aided Synthesis Design: 40 Years On. WileyInterdiscip. Rev.: Comput. Mol. Sci.2012, 2,7 9−107.\\n(649) Liu, B.; Ramsundar, B.; Kawthekar, P.; Shi, J.; Gomes, J.; Luu\\nNguyen, Q.; Ho, S.; Sloane, J.; Wender, P.; Pande, V. RetrosyntheticReaction Prediction Using Neural Sequence-to-Sequence Models.\\nACS Cent. Sci.2017, 3, 1103−1113.\\n(650) Coley, C. W.; Rogers, L.; Green, W. H.; Jensen, K. F.\\nSCScore: Synthetic Complexity Learned From a Reaction Corpus.J.Chem. Inf. Model.2018, 58, 252−261.\\n(651) Klucznik, T.; Mikulak-Klucznik, B.; McCormack, M. P.; Lima,\\nH.; Szymkuć, S.; Bhowmick, M.; Molga, K.; Zhou, Y.; Rickershauser,\\nL.; Gajewska, E. P.; et al. Efficient Syntheses of Diverse, MedicinallyRelevant Targets Planned by Computer and Executed in the\\nLaboratory. Chem. 2018, 4, 522−532.\\n(652) Coley, C. W.; Eyke, N. S.; Jensen, K. F. Autonomous\\nDiscovery in the Chemical Sciences Part I: Progress.Angew. Chem.,\\nInt. Ed. 2020, 59, 22858−22893.Int. Ed. 2020, 59, 22858−22893.\\n(653) Coley, C. W.; Eyke, N. S.; Jensen, K. F. Autonomous\\nDiscovery in the Chemical Sciences Part II: Outlook.Angew. Chem.,\\nInt. Ed. 2020, 59, 23414−23436.Int. Ed. 2020, 59, 23414−23436.\\n(654) Lindström, B.; Pettersson, L. J. A Brief History of Catalysis.\\nCATTECH 2003, 7, 130−138.\\n(655) Cui, X.; Li, W.; Ryabchuk, P.; Junge, K.; Beller, M. BridgingHomogeneous and Heterogeneous Catalysis by Heterogeneous\\nSingle-Metal-Site Catalysts. Nat. Catal. 2018, 1, 385−397.\\n(656) Suen, N.-T.; Hung, S.-F.; Quan, Q.; Zhang, N.; Xu, Y.-J.;\\nChen, H. M. Electrocatalysis for the Oxygen Evolution Reaction:Recent Development and Future Perspectives.Chem. Soc. Rev.2017,\\n46, 337−365.\\n(657) Benson, E. E.; Kubiak, C. P.; Sathrum, A. J.; Smieja, J. M.\\nElectrocatalytic and Homogeneous Approaches to Conversion of CO2to Liquid Fuels.Chem. Soc. Rev.2009, 38,8 9−99.\\n(658) Steinfeld, A. Solar Thermochemical Production of Hydro-\\ngena Review. Sol. Energy 2005, 78, 603−615.\\n(659) Lee, J. H.; Seo, Y.; Park, Y. D.; Anthony, J. E.; Kwak, D. H.;Lim, J. A.; Ko, S.; Jang, H. W.; Cho, K.; Lee, W. H. Effect of\\nCrystallization Modes in TIPS-pentacene/insulating Polymer Blends\\non the Gas Sensing Properties of Organic Field-Effect Transistors.Sci.\\nRep. 2019, 9, 21.Rep. 2019, 9, 21.\\n(660) Zeradjanin, A. R.; Grote, J.-P.; Polymeros, G.; Mayrhofer, K. J.\\nA Critical Review on Hydrogen Evolution Electrocatalysis: Re-\\nExploring the Volcano-Relationship.Electroanalysis 2016, 28, 2256−\\n2269.2269.\\n(661) Wenderich, K.; Mul, G. Methods, Mechanism, and\\nApplications of Photodeposition in Photocatalysis: A Review.Chem.\\nRev. 2016, 116, 14587−14619.\\n(662) Hou, W.; Cronin, S. B. A Review of Surface PlasmonResonance-Enhanced Photocatalysis. Adv. Funct. Mater. 2013, 23,\\n1612−1619.\\n(663) Julliard, M.; Chanon, M. Photoelectron-Transfer Catalysis: Its\\nConnections With Thermal and Electrochemical Analogs.Chem. Rev.\\n1983, 83, 425−506.1983, 83, 425−506.\\n(664) Kavarnos, G. J.; Turro, N. J. Photosensitization by Reversible\\nElectron Transfer: Theories, Experimental Evidence, and Examples.\\nChem. Rev. 1986, 86, 401−449.Chem. Rev. 1986, 86, 401−449.\\n(665) Bogaerts, A.; Tu, X.; Whitehead, J. C.; Centi, G.; Lefferts, L.;\\nGuaitella, O.; Azzolina-Jury, F.; Kim, H.-H.; Murphy, A. B.;\\nSchneider, W. F.; et al. The 2020 Plasma Catalysis Roadmap. J.Phys. D: Appl. Phys.2020, 53, 443001.\\n(666) Van Durme, J.; Dewulf, J.; Leys, C.; Van Langenhove, H.\\nCombining Non-Thermal Plasma With Heterogeneous Catalysis in\\nWaste Gas Treatment: A Review.Appl. Catal., B2008, 78, 324−333.Chemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9870(667) Campos-Gonzalez-Angulo, J. A.; Ribeiro, R. F.; Yuen-Zhou, J.\\nResonant Catalysis of Thermally Activated Chemical Reactions With\\nVibrational Polaritons.Nat. Commun. 2019, 10, 4685.\\n(668) Ma, Z.; Zaera, F.Encyclopedia of Inorganic and BioinorganicChemistry; Wiley Online Library, 2014; pp 1−16.\\n(669) Anastas, P. T.; Crabtree, R. H.Handbook of Green Chemistry;\\nVol. 2: Green Catalysis, Heterogeneous Catalysis; Wiley-VCH:\\nWeinheim, Germany, 2013.Weinheim, Germany, 2013.\\n(670) Trost, B. M. On Inventing Reactions for Atom Economy.Acc.\\nChem. Res. 2002, 35, 695−705.\\n(671) Sheldon, R. A.; Arends, I.; Hanefeld, U.Green Chemistry and\\nCatalysis; Wiley-VCH: Weinheim, Germany, 2007.Catalysis; Wiley-VCH: Weinheim, Germany, 2007.\\n(672) Hammer, B.; Nørskov, J. K. Theoretical Surface Science and\\nCatalysis-Calculations and Concepts.Adv. Catal. 2000, 45,7 1−129.\\n(673) Medford, A. J.; Vojvodic, A.; Hummelshøj, J. S.; Voss, J.;Abild-Pedersen, F.; Studt, F.; Bligaard, T.; Nilsson, A.; Nørskov, J. K.\\nFrom the Sabatier Principle to a Predictive Theory of Transition-\\nMetal Heterogeneous Catalysis.J. Catal. 2015, 328,3 6−42.(674) Calle-Vallejo, F.; Loffreda, D.; Koper, M. T.; Sautet, P.\\nIntroducing Structural Sensitivity Into Adsorption-Energy Scaling\\nRelations by Means of Coordination Numbers.Nat. Chem. 2015, 7,\\n403−410.403−410.\\n(675) Kitchin, J. R. Machine Learning in Catalysis.Nat. Catal.2018,\\n1, 230−232.\\n(676) Toyao, T.; Maeno, Z.; Takakusagi, S.; Kamachi, T.; Takigawa,\\nI.; Shimizu, K. I. Machine Learning for Catalysis Informatics: RecentApplications and Prospects.ACS Catal. 2020, 10, 2260−2297.\\n(677) Grajciar, L.; Heard, C. J.; Bondarenko, A. A.; Polynski, M. V.;\\nMeeprasert, J.; Pidko, E. A.; Nachtigall, P. Towards OperandoComputational Modeling in Heterogeneous Catalysis.Chem. Soc. Rev.\\n2018, 47, 8307−8348.\\n(678) Goldsmith, B. R.; Esterhuizen, J.; Liu, J. X.; Bartel, C. J.;\\nSutton, C. Machine Learning for Heterogeneous Catalyst Design andDiscovery. AIChE J. 2018, 64, 2311−2323.\\n(679) McCullough, K.; Williams, T.; Mingle, K.; Jamshidi, P.;\\nLauterbach, J. High-Throughput Experimentation Meets Artificial\\nIntelligence: A New Pathway to Catalyst Discovery. Phys. Chem.Chem. Phys. 2020, 22, 11174−11196.\\n(680) Wexler, R. B.; Qiu, T.; Rappe, A. M. Automatic Prediction of\\nSurface Phase Diagrams Using Ab Initio Grand Canonical Monte\\nCarlo. J. Phys. Chem. C2019, 123, 2321−2328.Carlo. J. Phys. Chem. C2019, 123, 2321−2328.\\n(681) Ulissi, Z. W.; Singh, A. R.; Tsai, C.; Nørskov, J. K. Automated\\nDiscovery and Construction of Surface Phase Diagrams Using\\nMachine Learning. J. Phys. Chem. Lett.2016, 7, 3931−3935.(682) Roling, L. T.; Li, L.; Abild-Pedersen, F. Configurational\\nEnergies of Nanoparticles Based on Metal-Metal Coordination. J.\\nPhys. Chem. C2017, 121, 23002−23010.\\n(683) Roling, L. T.; Choksi, T. S.; Abild-Pedersen, F. ACoordination-Based Model for Transition Metal Alloy Nanoparticles.\\nNanoscale 2019, 11, 4438−4452.\\n(684) Yan, Z.; Taylor, M. G.; Mascareno, A.; Mpourmpakis, G. Size-,\\nShape-, and Composition-Dependent Model for Metal NanoparticleStability Prediction. Nano Lett. 2018, 18, 2696−2704.\\n(685) Dean, J.; Taylor, M. G.; Mpourmpakis, G. Unfolding\\nAdsorption on Metal Nanoparticles: Connecting Stability With\\nCatalysis.Sci. Adv. 2019, 5, No. eaax5101.Catalysis.Sci. Adv. 2019, 5, No. eaax5101.\\n(686) Núñez, M.; Lansford, J. L.; Vlachos, D. G. Optimization of the\\nFacet Structure of Transition-Metal Catalysts Applied to the Oxygen\\nReduction Reaction. Nat. Chem. 2019, 11, 449−456.Reduction Reaction. Nat. Chem. 2019, 11, 449−456.\\n(687) van der Maaten, L. Accelerating T-SNE Using Tree-Based\\nAlgorithms. J. Mach. Learn. Res.2014, 15, 3221−3245.\\n(688) Zhong, M.; et al. Accelerated Discovery of CO 2 Electro-catalysts Using Active Machine Learning. Nature 2020, 581, 178−\\n183.\\n(689) Artrith, N.; Kolpak, A. M. Understanding the Composition\\nand Activity of Electrocatalytic Nanoalloys in Aqueous Solvents: ACombination of DFT and Accurate Neural Network Potentials.Nano\\nLett. 2014, 14, 2670−2676.\\n(690) Nandy, A.; Zhu, J.; Janet, J. P.; Duan, C.; Getman, R. B.;\\nKulik, H. J. Machine Learning Accelerates the Discovery of DesignLett. 2014, 14, 2670−2676.\\n(690) Nandy, A.; Zhu, J.; Janet, J. P.; Duan, C.; Getman, R. B.;\\nKulik, H. J. Machine Learning Accelerates the Discovery of Design\\nRules and Exceptions in Stable Metal-Oxo Intermediate Formation.ACS Catal. 2019, 9, 8243−8255.\\n(691) Wexler, R. B.; Martirez, J. M. P.; Rappe, A. M. Chemical\\nPressure-Driven Enhancement of the Hydrogen Evolving Activity of\\nNi2P From Nonmetal Surface Doping Interpreted via MachineLearning. J. Am. Chem. Soc.2018, 140, 4678−4683.\\n(692) O’Connor, N. J.; Jonayat, A. S.; Janik, M. J.; Senftle, T. P.\\nInteraction Trends Between Single Metal Atoms and Oxide Supports\\nIdentified With Density Functional Theory and Statistical Learning.Nat. Catal. 2018, 1, 531−539.\\n(693) Griego, C. D.; Zhao, L.; Saravanan, K.; Keith, J. A. Machine\\nLearning Corrected Alchemical Perturbation Density Functional\\nTheory for Catalysis Applications.AIChE J. 2020, 66, No. 17041.(694) Meyer, B.; Sawatlon, B.; Heinen, S.; von Lilienfeld, O. A.;\\nCorminboeuf, C. Machine Learning Meets Volcano Plots: Computa-\\ntional Discovery of Cross-Coupling Catalysts. Chem. Sci. 2018, 9,\\n7069−7077.7069−7077.\\n(695) Boes, J. R.; Mamun, O.; Winther, K.; Bligaard, T. Graph\\nTheory Approach to High-Throughput Surface Adsorption Structure\\nGeneration. J. Phys. Chem. A2019, 123, 2281−2285.Generation. J. Phys. Chem. A2019, 123, 2281−2285.\\n(696) Ulissi, Z. W.; Medford, A. J.; Bligaard, T.; Nørskov, J. K. To\\nAddress Surface Reaction Network Complexity Using Scaling\\nRelations Machine Learning and DFT Calculations.Nat. Commun.2017, 8, 14621.\\n(697) Bort, W.; Baskin, I. I.; Gimadiev, T.; Mukanov, A.; Nugmanov,\\nR.; Sidorov, P.; Marcou, G.; Horvath, D.; Klimchuk, O.; Madzhidov,\\nT.; Varnek, A. Discovery of Novel Chemical Reactions by DeepGenerative Recurrent Neural Network.Sci. Rep. 2021, 11, 3178.\\n(698) Kayala, M. A.; Baldi, P. ReactionPredictor: Prediction of\\nComplex Chemical Reactions at the Mechanistic Level Using\\nMachine Learning. J. Chem. Inf. Model.2012, 52, 2526−2540.(699) Wei, J. N.; Duvenaud, D.; Aspuru-Guzik, A. Neural Networks\\nfor the Prediction of Organic Chemistry Reactions.ACS Cent. Sci.\\n2016, 2, 725−732.\\n(700) Schwaller, P.; Probst, D.; Vaucher, A. C.; Nair, V. H.;Kreutter, D.; Laino, T.; Reymond, J.-L. Mapping the Space of\\nChemical Reactions Using Attention-Based Neural Networks.Nat.\\nMach. Intell. 2021, 3, 144−152.\\n(701) Blondal, K.; Jelic, J.; Mazeau, E.; Studt, F.; West, R. H.;Goldsmith, C. F. Computer-Generated Kinetics for Coupled\\nHeterogeneous/Homogeneous Systems: A Case Study in Catalytic\\nCombustion of Methane on Platinum.Ind. Eng. Chem. Res.2019, 58,\\n17682−17691.17682−17691.\\n(702) Rangarajan, S.; Maravelias, C. T.; Mavrikakis, M. Sequential-\\nOptimization-Based Framework for Robust Modeling and Design of\\nHeterogeneous Catalytic Systems. J. Phys. Chem. C 2017, 121,\\n25847−25863.25847−25863.\\n(703) Banerjee, S.; Sreenithya, A.; Sunoj, R. B. Machine Learning for\\nPredicting Product Distributions in Catalytic Regioselective Reac-\\ntions. Phys. Chem. Chem. Phys.2018, 20, 18311−18318.(704) Reid, J. P.; Sigman, M. S. Holistic Prediction of\\nEnantioselectivity in Asymmetric Catalysis.Nature 2019, 571, 343−\\n348.\\n(705) Zahrt, A. F.; Henle, J. J.; Rose, B. T.; Wang, Y.; Darrow, W. T.;Denmark, S. E. Prediction of Higher-Selectivity Catalysts by\\nComputer-Driven Workflow and Machine Learning. Science 2019,\\n363, No. eaau5631.\\n(706) Ravasco, J. M.; Coelho, J. A. Predictive Multivariate Modelsfor Bioorthogonal Inverse-Electron Demand Diels-Alder Reactions.J.\\nAm. Chem. Soc.2020, 142, 4235−4241.\\n(707) Davies, I. W. The Digitization of Organic Synthesis.Nature\\n2019, 570, 175−181.\\n(708) Li, J.; Eastgate, M. D. Making Better Decisions DuringSynthetic Route Design: Leveraging Prediction to Achieve Greenness-\\nby-Design. React. Chem. Eng.2019, 4, 1595−1607.\\n(709) Schneider, G.; Fechner, U. Computer-Based De Novo Design\\nof Drug-Like Molecules.Nat. Rev. Drug Discovery2005, 4, 649−663.Chemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9871(710) Reker, D.; Schneider, G. Active-Learning Strategies in\\nComputer-Assisted Drug Discovery.Drug Discovery Today2015, 20,\\n458−465.\\n(711) Wainberg, M.; Merico, D.; Delong, A.; Frey, B. J. DeepLearning in Biomedicine.Nat. Biotechnol. 2018, 36, 829−838.\\n(712) Khaket, T. P.; Aggarwal, H.; Dhanda, S.; Singh, J.Industrial\\nEnzymes: Trends, Scope and Relevance; Nova Science Publishers, Inc.:\\nHauppauge, NY, 2014; pp 110−143.Hauppauge, NY, 2014; pp 110−143.\\n(713) Blaschke, T.; Aru ́s-Pous, J.; Chen, H.; Margreitter, C.;\\nTyrchan, C.; Engkvist, O.; Papadopoulos, K.; Patronov, A.\\nREINVENT 2.0: An AI Tool for De Novo Drug Design.J. Chem.\\nInf. Model. 2020, 60, 5918−5922.Inf. Model. 2020, 60, 5918−5922.\\n(714) Jiménez-Luna, J.; Grisoni, F.; Schneider, G. Drug Discovery\\nWith Explainable Artificial Intelligence. Nat. Mach. Intell. 2020, 2,\\n573−584.\\n(715) Chen, H.; Engkvist, O.; Wang, Y.; Olivecrona, M.; Blaschke,T. The Rise of Deep Learning in Drug Discovery. Drug Discovery\\nToday 2018, 23, 1241−1250.\\n(716) Piroozmand, F.; Mohammadipanah, F.; Sajedi, H. Spectrum of\\nDeep Learning Algorithms in Drug Discovery.Chem. Biol. Drug Des.\\n2020, 96, 886−901.2020, 96, 886−901.\\n(717) Fjell, C. D.; Hiss, J. A.; Hancock, R. E.; Schneider, G.\\nDesigning Antimicrobial Peptides: Form Follows Function.Nat. Rev.\\nDrug Discovery 2012, 11,3 7−51.\\n(718) Batra, R.; Chan, H.; Kamath, G.; Ramprasad, R.; Cherukara,M. J.; Sankaranarayanan, S. K. Screening of Therapeutic Agents for\\nCOVID-19 Using Machine Learning and Ensemble Docking Studies.\\nJ. Phys. Chem. Lett.2020, 11, 7058−7065.\\n(719) Haghighatlari, M.; Vis hwakarma, G.; Altarawy, D.;Subramanian, R.; Kota, B. U.; Sonpal, A.; Setlur, S.; Hachmann, J.\\nChem ML: A Machine Learning and Informatics Program Package for\\nthe Analysis, Mining, and Modeling of Chemical and Materials Data.Wiley Interdiscip. Rev.: Comput. Mol. Sci.2020, 10, No. 1458.\\n(720) Noé, F.; Tkatchenko, A.; Müller, K.-R.; Clementi, C. Machine\\nLearning for Molecular Simulation.Annu. Rev. Phys. Chem.2020, 71,\\n361−390.361−390.\\n(721) von Lilienfeld, O. A. Quantum Machine Learning in Chemical\\nCompound Space. Angew. Chem., Int. Ed.2018, 57, 4164−4169.\\n(722) von Lilienfeld, O. A.; Burke, K. Retrospective on a Decade ofMachine Learning for Chemical Discovery.Nat. Commun. 2020, 11,\\n4895.\\n(723) Strieth-Kalthoff, F.; Sandfort, F.; Segler, M. H.; Glorius, F.\\nMachine Learning the Ropes: Principles, Applications and Directionsin Synthetic Chemistry.Chem. Soc. Rev.2020, 49, 6154−6168.\\n(724) Schütt, K. T.; Chmiela, S.; von Lilienfeld, O. A.; Tkatchenko,\\nA.; Tsuda, K.; Müller, K.-R.Machine Learning Meets Quantum Physics;Springer Lecture Notes in Physics, Springer: Cham, Switzerland,\\n2020; Vol. 968.\\n(725) Schnake, T.; Eberle, O.; Lederer, J.; Nakajima, S.; Schütt, K.\\nT.; Müller, K.-R.; Montavon, G. XAI for Graphs: Explaining GraphNeural Network Predictions by Identifying Relevant Walks. arXiv,\\n2020, 2006.03589, ver. 1.https://arxiv.org/abs/2006.03589v1.\\nChemical Reviews pubs.acs.org/CR Review\\nhttps://doi.org/10.1021/acs.chemrev.1c00107\\nChem. Rev. 2021, 121, 9816−9872\\n9872\\\".Your output should only and very strictly use the following template:\\n# {Title}\\n## {Subtitle01}\\n- {Emoji01} Bulletpoint01\\n- {Emoji02} Bulletpoint02\\n## {Subtitle02}\\n- {Emoji03} Bulletpoint03\\n- {Emoji04} Bulletpoint04\\n\\nSummarize the giving topic to generate a mind map (as many subtitles as possible, with a minimum of three subtitles, any node in the mindmap should not exceed 10-12 words) structure markdown.\\n Do not include anything in the response, that is not the part of mindmap.\\n  Importantly your output must use language \\\"English\\\"\"\\n\n"
     ]
    }
   ],
   "source": [
    "print(prompt.replace('\\n', '\\\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 49 prefix-match hit, remaining 13717 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   41052.59 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms / 13717 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   333 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   57494.41 ms / 14050 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Mindmap Structure\n",
      "\n",
      "# Introduction\n",
      "- Background: Computational chemistry and machine learning (ML) are two fields that have been growing rapidly in recent years.\n",
      "  - Advantages of combining them:\n",
      "    * Improved accuracy for chemical systems modeling.\n",
      "\n",
      "### CompChem Methods Overview\n",
      "\n",
      "\n",
      "#### Models & Levels Of Abstraction \n",
      "* Extract information from data using simple models, e.g., ideal gas equation\n",
      "- Ideal Gas Equation: PV = nRT\n",
      "\n",
      "## Wavefunction Theory (WF) and CorrelatedWavefunctions \n",
      "\n",
      "| **Method** |  Description |\n",
      "|:-----------:|---------------|\n",
      "1. Hartree-Fock   || HF method for electronic structure calculations.\n",
      "2.Hartmann Fokker    ||\n",
      "3.Coulombic Interaction|| CI methods to account electron-electron interactions.\n",
      "\n",
      "## Hierarchies of Methods\n",
      "\n",
      "### Wavefunction Theory (WF) Hierarchy\n",
      "| **Method** |  Description |\n",
      "|:-----------:|---------------|\n",
      "1. Hartree-Fock   || HF method for electronic structure calculations.\n",
      "2.Hartmann Fokker    ||\n",
      "3.Coulombic Interaction|| CI methods to account electron-electron interactions.\n",
      "\n",
      "## Correlated Wavefunctions\n",
      "\n",
      "### Full Conﬁguration Interactions (FCI) Hierarchy\n",
      "| **Method** |  Description |\n",
      "|:-----------:|---------------|\n",
      "1. Hartree-Fock   || HF method for electronic structure calculations.\n",
      "2.Hartmann Fokker    ||\n",
      "3.Coulombic Interaction|| CI methods to account electron-electron interactions.\n",
      "\n",
      "## Machine Learning Tutorial & Intersections\n",
      "\n",
      "### Introduction \n",
      "- Background: Computational chemistry and machine learning (ML) are two fields that have been growing rapidly in recent years.\n"
     ]
    }
   ],
   "source": [
    "response = llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {'role':'system',\n",
    "         'content': 'You are a helpful research assistant for generating mindmaps in MarkDown format from scientific research papers.'},\n",
    "        {'role':'user',\n",
    "        'content': prompt}\n",
    "    ],\n",
    "    temperature=0.8,\n",
    "    top_k=500,\n",
    "    top_p=3.0,\n",
    "    presence_penalty = 1.0,\n",
    "    frequency_penalty = 1.1,\n",
    "    repeat_penalty=5.0,\n",
    ")\n",
    "mindmap_data = response['choices'][0]['message']['content']\n",
    "print(mindmap_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction_mindmap.svg\n"
     ]
    }
   ],
   "source": [
    "svg = generate_mindmap_svg(mindmap_data)\n",
    "try:\n",
    "    print(svg)\n",
    "except:\n",
    "    svg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
